{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d62644f",
   "metadata": {},
   "source": [
    "# Prompt Variations Analysis\n",
    "\n",
    "This notebook analyzes the performance of different prompt variations for generating PR descriptions and their detectability by AI detection tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "73a88c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103b12ab",
   "metadata": {},
   "source": [
    "## Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "65e47b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading detection results...\n",
      "Detection data shape: (384, 6)\n",
      "Detection data columns: ['pr_id', 'prompt_variation', 'entry_key', 'entry_type', 'input_text', 'zerogpt_response']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pr_id</th>\n",
       "      <th>prompt_variation</th>\n",
       "      <th>entry_key</th>\n",
       "      <th>entry_type</th>\n",
       "      <th>input_text</th>\n",
       "      <th>zerogpt_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PR_kwDOAQ0TF85oN6RH</td>\n",
       "      <td>original</td>\n",
       "      <td>PR_kwDOAQ0TF85oN6RH_original</td>\n",
       "      <td>original</td>\n",
       "      <td>&lt;!-- \\nDescribe the changes you have made here...</td>\n",
       "      <td>{\"success\": true, \"code\": 200, \"message\": \"det...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PR_kwDOAQ0TF85oN6RH</td>\n",
       "      <td>P-7_Template_Plus_Title</td>\n",
       "      <td>PR_kwDOAQ0TF85oN6RH_P-7_Template_Plus_Title_ge...</td>\n",
       "      <td>generated</td>\n",
       "      <td>Fixed a modularity issue related to the HTML c...</td>\n",
       "      <td>{\"success\": true, \"code\": 200, \"message\": \"det...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2</td>\n",
       "      <td>original</td>\n",
       "      <td>MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2_original</td>\n",
       "      <td>original</td>\n",
       "      <td>Add zbmath to the public databases which can b...</td>\n",
       "      <td>{\"success\": true, \"code\": 200, \"message\": \"det...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2</td>\n",
       "      <td>P-7_Template_Plus_Title</td>\n",
       "      <td>MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2_P-7_Template_...</td>\n",
       "      <td>generated</td>\n",
       "      <td>This pull request introduces a new fetcher for...</td>\n",
       "      <td>{\"success\": true, \"code\": 200, \"message\": \"det...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PR_kwDOAQ0TF86DGkyK</td>\n",
       "      <td>original</td>\n",
       "      <td>PR_kwDOAQ0TF86DGkyK_original</td>\n",
       "      <td>original</td>\n",
       "      <td>Fixes https://github.com/JabRef/jabref/issues/...</td>\n",
       "      <td>{\"success\": true, \"code\": 200, \"message\": \"det...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              pr_id         prompt_variation  \\\n",
       "0               PR_kwDOAQ0TF85oN6RH                 original   \n",
       "1               PR_kwDOAQ0TF85oN6RH  P-7_Template_Plus_Title   \n",
       "2  MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2                 original   \n",
       "3  MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2  P-7_Template_Plus_Title   \n",
       "4               PR_kwDOAQ0TF86DGkyK                 original   \n",
       "\n",
       "                                           entry_key entry_type  \\\n",
       "0                       PR_kwDOAQ0TF85oN6RH_original   original   \n",
       "1  PR_kwDOAQ0TF85oN6RH_P-7_Template_Plus_Title_ge...  generated   \n",
       "2          MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2_original   original   \n",
       "3  MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2_P-7_Template_...  generated   \n",
       "4                       PR_kwDOAQ0TF86DGkyK_original   original   \n",
       "\n",
       "                                          input_text  \\\n",
       "0  <!-- \\nDescribe the changes you have made here...   \n",
       "1  Fixed a modularity issue related to the HTML c...   \n",
       "2  Add zbmath to the public databases which can b...   \n",
       "3  This pull request introduces a new fetcher for...   \n",
       "4  Fixes https://github.com/JabRef/jabref/issues/...   \n",
       "\n",
       "                                    zerogpt_response  \n",
       "0  {\"success\": true, \"code\": 200, \"message\": \"det...  \n",
       "1  {\"success\": true, \"code\": 200, \"message\": \"det...  \n",
       "2  {\"success\": true, \"code\": 200, \"message\": \"det...  \n",
       "3  {\"success\": true, \"code\": 200, \"message\": \"det...  \n",
       "4  {\"success\": true, \"code\": 200, \"message\": \"det...  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define paths\n",
    "generation_path = \"../generation/datasets/\"\n",
    "detection_path = \"../detection/datasets/prompt_variations/prompt_variations-detection.csv\"\n",
    "\n",
    "# Load detection results\n",
    "print(\"Loading detection results...\")\n",
    "detection_df = pd.read_csv(detection_path)\n",
    "print(f\"Detection data shape: {detection_df.shape}\")\n",
    "print(f\"Detection data columns: {detection_df.columns.tolist()}\")\n",
    "detection_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3caa1d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 prompt variation files:\n",
      "  prompt_variation_P-10_Full_Plus_One_Shot_generated.csv\n",
      "  prompt_variation_P-11_Full_Plus_Few_Shot_generated.csv\n",
      "  prompt_variation_P-1_Minimal_generated.csv\n",
      "  prompt_variation_P-2_Basic_generated.csv\n",
      "  prompt_variation_P-3_Diffs_Only_generated.csv\n",
      "  prompt_variation_P-4_Diffs_Plus_Title_generated.csv\n",
      "  prompt_variation_P-5_Code_Only_generated.csv\n",
      "  prompt_variation_P-6_Issue_Only_generated.csv\n",
      "  prompt_variation_P-7_Template_Plus_Title_generated.csv\n",
      "  prompt_variation_P-8_Full_Context_generated.csv\n",
      "  prompt_variation_P-9_Basic_One_Shot_generated.csv\n"
     ]
    }
   ],
   "source": [
    "# Find all prompt variation CSV files\n",
    "prompt_variation_files = glob.glob(os.path.join(generation_path, \"prompt_variation_P-*_generated.csv\"))\n",
    "prompt_variation_files.sort()\n",
    "\n",
    "print(f\"Found {len(prompt_variation_files)} prompt variation files:\")\n",
    "for file in prompt_variation_files:\n",
    "    print(f\"  {os.path.basename(file)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "13fa3138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading prompt variation data...\n",
      "Loading prompt_variation_P-10_Full_Plus_One_Shot_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-10_Full_Plus_One_Shot\n",
      "Loading prompt_variation_P-11_Full_Plus_Few_Shot_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-11_Full_Plus_Few_Shot\n",
      "Loading prompt_variation_P-1_Minimal_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-1_Minimal\n",
      "Loading prompt_variation_P-2_Basic_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-2_Basic\n",
      "Loading prompt_variation_P-3_Diffs_Only_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-3_Diffs_Only\n",
      "Loading prompt_variation_P-4_Diffs_Plus_Title_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-4_Diffs_Plus_Title\n",
      "Loading prompt_variation_P-5_Code_Only_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-5_Code_Only\n",
      "Loading prompt_variation_P-6_Issue_Only_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-6_Issue_Only\n",
      "Loading prompt_variation_P-7_Template_Plus_Title_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-7_Template_Plus_Title\n",
      "Loading prompt_variation_P-8_Full_Context_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-8_Full_Context\n",
      "Loading prompt_variation_P-9_Basic_One_Shot_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-9_Basic_One_Shot\n",
      "\n",
      "Combined generation data shape: (2640, 26)\n",
      "Prompt variations found: ['P-10_Full_Plus_One_Shot', 'P-11_Full_Plus_Few_Shot', 'P-1_Minimal', 'P-2_Basic', 'P-3_Diffs_Only', 'P-4_Diffs_Plus_Title', 'P-5_Code_Only', 'P-6_Issue_Only', 'P-7_Template_Plus_Title', 'P-8_Full_Context', 'P-9_Basic_One_Shot']\n"
     ]
    }
   ],
   "source": [
    "# Load and combine all prompt variation data\n",
    "print(\"\\nLoading prompt variation data...\")\n",
    "all_generation_data = []\n",
    "\n",
    "for file_path in prompt_variation_files:\n",
    "    print(f\"Loading {os.path.basename(file_path)}...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Extract prompt variation from filename\n",
    "    filename = os.path.basename(file_path)\n",
    "    prompt_var = filename.split('_')[2] + '_' + filename.split('_')[3]  # e.g., P-1_Minimal\n",
    "    \n",
    "    # Add prompt variation if not present\n",
    "    if 'prompt_variation' not in df.columns:\n",
    "        df['prompt_variation'] = prompt_var\n",
    "    \n",
    "    print(f\"  Shape: {df.shape}, Prompt variation: {df['prompt_variation'].iloc[0] if len(df) > 0 else 'N/A'}\")\n",
    "    all_generation_data.append(df)\n",
    "\n",
    "# Combine all generation data\n",
    "generation_df = pd.concat(all_generation_data, ignore_index=True)\n",
    "print(f\"\\nCombined generation data shape: {generation_df.shape}\")\n",
    "print(f\"Prompt variations found: {sorted(generation_df['prompt_variation'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7e2f4b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation data columns:\n",
      "['id', 'title', 'description', 'state', 'repository', 'pr_number', 'filename', 'status', 'additions', 'deletions', 'changes', 'sha', 'blob_url', 'raw_url', 'patch', 'file_size_bytes', 'file_content', 'pr_total_size_bytes', 'issue_titles', 'issue_bodies', 'issue_comments', 'generated_description', 'prompt_variation', 'total_input_tokens', 'total_output_tokens', 'total_tokens']\n",
      "\n",
      "Generation data sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>state</th>\n",
       "      <th>repository</th>\n",
       "      <th>pr_number</th>\n",
       "      <th>filename</th>\n",
       "      <th>status</th>\n",
       "      <th>additions</th>\n",
       "      <th>deletions</th>\n",
       "      <th>...</th>\n",
       "      <th>file_content</th>\n",
       "      <th>pr_total_size_bytes</th>\n",
       "      <th>issue_titles</th>\n",
       "      <th>issue_bodies</th>\n",
       "      <th>issue_comments</th>\n",
       "      <th>generated_description</th>\n",
       "      <th>prompt_variation</th>\n",
       "      <th>total_input_tokens</th>\n",
       "      <th>total_output_tokens</th>\n",
       "      <th>total_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PR_kwDOAQ0TF85oN6RH</td>\n",
       "      <td>Fix modularity issue with html converter</td>\n",
       "      <td>&lt;!-- \\nDescribe the changes you have made here...</td>\n",
       "      <td>MERGED</td>\n",
       "      <td>JabRef/jabref</td>\n",
       "      <td>10943</td>\n",
       "      <td>build.gradle</td>\n",
       "      <td>modified</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>import org.gradle.internal.os.OperatingSystem\\...</td>\n",
       "      <td>28005</td>\n",
       "      <td>issue #10942: Fix: abstract field loses markdo...</td>\n",
       "      <td>issue #10942: After PR #10896, the abstract fi...</td>\n",
       "      <td>Comment #1 by LoayGhreeb in issue #10942: Ther...</td>\n",
       "      <td>This pull request fixes a modularity issue rel...</td>\n",
       "      <td>P-10_Full_Plus_One_Shot</td>\n",
       "      <td>10637</td>\n",
       "      <td>107</td>\n",
       "      <td>10744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2</td>\n",
       "      <td>Zbmath fetcher</td>\n",
       "      <td>Add zbmath to the public databases which can b...</td>\n",
       "      <td>MERGED</td>\n",
       "      <td>JabRef/jabref</td>\n",
       "      <td>7440</td>\n",
       "      <td>CHANGELOG.md</td>\n",
       "      <td>modified</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td># Changelog\\n\\nAll notable changes to this pro...</td>\n",
       "      <td>97657</td>\n",
       "      <td>issue #7437: Enhance bibliographic information...</td>\n",
       "      <td>issue #7437: It is possible to enhance bibliog...</td>\n",
       "      <td>Comment #1 by Siedlerchr in issue #7437: Sound...</td>\n",
       "      <td>This pull request adds comprehensive support f...</td>\n",
       "      <td>P-10_Full_Plus_One_Shot</td>\n",
       "      <td>34262</td>\n",
       "      <td>550</td>\n",
       "      <td>34812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2</td>\n",
       "      <td>Zbmath fetcher</td>\n",
       "      <td>Add zbmath to the public databases which can b...</td>\n",
       "      <td>MERGED</td>\n",
       "      <td>JabRef/jabref</td>\n",
       "      <td>7440</td>\n",
       "      <td>src/main/java/org/jabref/logic/importer/EntryB...</td>\n",
       "      <td>modified</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>package org.jabref.logic.importer;\\n\\nimport j...</td>\n",
       "      <td>97657</td>\n",
       "      <td>issue #7437: Enhance bibliographic information...</td>\n",
       "      <td>issue #7437: It is possible to enhance bibliog...</td>\n",
       "      <td>Comment #1 by Siedlerchr in issue #7437: Sound...</td>\n",
       "      <td>This pull request adds comprehensive support f...</td>\n",
       "      <td>P-10_Full_Plus_One_Shot</td>\n",
       "      <td>34262</td>\n",
       "      <td>550</td>\n",
       "      <td>34812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2</td>\n",
       "      <td>Zbmath fetcher</td>\n",
       "      <td>Add zbmath to the public databases which can b...</td>\n",
       "      <td>MERGED</td>\n",
       "      <td>JabRef/jabref</td>\n",
       "      <td>7440</td>\n",
       "      <td>src/main/java/org/jabref/logic/importer/WebFet...</td>\n",
       "      <td>modified</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>package org.jabref.logic.importer;\\n\\nimport j...</td>\n",
       "      <td>97657</td>\n",
       "      <td>issue #7437: Enhance bibliographic information...</td>\n",
       "      <td>issue #7437: It is possible to enhance bibliog...</td>\n",
       "      <td>Comment #1 by Siedlerchr in issue #7437: Sound...</td>\n",
       "      <td>This pull request adds comprehensive support f...</td>\n",
       "      <td>P-10_Full_Plus_One_Shot</td>\n",
       "      <td>34262</td>\n",
       "      <td>550</td>\n",
       "      <td>34812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2</td>\n",
       "      <td>Zbmath fetcher</td>\n",
       "      <td>Add zbmath to the public databases which can b...</td>\n",
       "      <td>MERGED</td>\n",
       "      <td>JabRef/jabref</td>\n",
       "      <td>7440</td>\n",
       "      <td>src/main/java/org/jabref/logic/importer/fetche...</td>\n",
       "      <td>modified</td>\n",
       "      <td>62</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>package org.jabref.logic.importer.fetcher;\\n\\n...</td>\n",
       "      <td>97657</td>\n",
       "      <td>issue #7437: Enhance bibliographic information...</td>\n",
       "      <td>issue #7437: It is possible to enhance bibliog...</td>\n",
       "      <td>Comment #1 by Siedlerchr in issue #7437: Sound...</td>\n",
       "      <td>This pull request adds comprehensive support f...</td>\n",
       "      <td>P-10_Full_Plus_One_Shot</td>\n",
       "      <td>34262</td>\n",
       "      <td>550</td>\n",
       "      <td>34812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id                                     title  \\\n",
       "0               PR_kwDOAQ0TF85oN6RH  Fix modularity issue with html converter   \n",
       "1  MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2                            Zbmath fetcher   \n",
       "2  MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2                            Zbmath fetcher   \n",
       "3  MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2                            Zbmath fetcher   \n",
       "4  MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2                            Zbmath fetcher   \n",
       "\n",
       "                                         description   state     repository  \\\n",
       "0  <!-- \\nDescribe the changes you have made here...  MERGED  JabRef/jabref   \n",
       "1  Add zbmath to the public databases which can b...  MERGED  JabRef/jabref   \n",
       "2  Add zbmath to the public databases which can b...  MERGED  JabRef/jabref   \n",
       "3  Add zbmath to the public databases which can b...  MERGED  JabRef/jabref   \n",
       "4  Add zbmath to the public databases which can b...  MERGED  JabRef/jabref   \n",
       "\n",
       "   pr_number                                           filename    status  \\\n",
       "0      10943                                       build.gradle  modified   \n",
       "1       7440                                       CHANGELOG.md  modified   \n",
       "2       7440  src/main/java/org/jabref/logic/importer/EntryB...  modified   \n",
       "3       7440  src/main/java/org/jabref/logic/importer/WebFet...  modified   \n",
       "4       7440  src/main/java/org/jabref/logic/importer/fetche...  modified   \n",
       "\n",
       "   additions  deletions  ...  \\\n",
       "0          4          2  ...   \n",
       "1          1          0  ...   \n",
       "2         11          3  ...   \n",
       "3          1          0  ...   \n",
       "4         62          9  ...   \n",
       "\n",
       "                                        file_content pr_total_size_bytes  \\\n",
       "0  import org.gradle.internal.os.OperatingSystem\\...               28005   \n",
       "1  # Changelog\\n\\nAll notable changes to this pro...               97657   \n",
       "2  package org.jabref.logic.importer;\\n\\nimport j...               97657   \n",
       "3  package org.jabref.logic.importer;\\n\\nimport j...               97657   \n",
       "4  package org.jabref.logic.importer.fetcher;\\n\\n...               97657   \n",
       "\n",
       "                                        issue_titles  \\\n",
       "0  issue #10942: Fix: abstract field loses markdo...   \n",
       "1  issue #7437: Enhance bibliographic information...   \n",
       "2  issue #7437: Enhance bibliographic information...   \n",
       "3  issue #7437: Enhance bibliographic information...   \n",
       "4  issue #7437: Enhance bibliographic information...   \n",
       "\n",
       "                                        issue_bodies  \\\n",
       "0  issue #10942: After PR #10896, the abstract fi...   \n",
       "1  issue #7437: It is possible to enhance bibliog...   \n",
       "2  issue #7437: It is possible to enhance bibliog...   \n",
       "3  issue #7437: It is possible to enhance bibliog...   \n",
       "4  issue #7437: It is possible to enhance bibliog...   \n",
       "\n",
       "                                      issue_comments  \\\n",
       "0  Comment #1 by LoayGhreeb in issue #10942: Ther...   \n",
       "1  Comment #1 by Siedlerchr in issue #7437: Sound...   \n",
       "2  Comment #1 by Siedlerchr in issue #7437: Sound...   \n",
       "3  Comment #1 by Siedlerchr in issue #7437: Sound...   \n",
       "4  Comment #1 by Siedlerchr in issue #7437: Sound...   \n",
       "\n",
       "                               generated_description         prompt_variation  \\\n",
       "0  This pull request fixes a modularity issue rel...  P-10_Full_Plus_One_Shot   \n",
       "1  This pull request adds comprehensive support f...  P-10_Full_Plus_One_Shot   \n",
       "2  This pull request adds comprehensive support f...  P-10_Full_Plus_One_Shot   \n",
       "3  This pull request adds comprehensive support f...  P-10_Full_Plus_One_Shot   \n",
       "4  This pull request adds comprehensive support f...  P-10_Full_Plus_One_Shot   \n",
       "\n",
       "   total_input_tokens total_output_tokens total_tokens  \n",
       "0               10637                 107        10744  \n",
       "1               34262                 550        34812  \n",
       "2               34262                 550        34812  \n",
       "3               34262                 550        34812  \n",
       "4               34262                 550        34812  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the structure of generation data\n",
    "print(\"Generation data columns:\")\n",
    "print(generation_df.columns.tolist())\n",
    "print(\"\\nGeneration data sample:\")\n",
    "generation_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd403629",
   "metadata": {},
   "source": [
    "## Data Merging and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "38691479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation data after deduplication: (352, 26)\n",
      "\n",
      "Detection data entry types: entry_type\n",
      "generated    352\n",
      "original      32\n",
      "Name: count, dtype: int64\n",
      "Detection data prompt variations: ['P-10_Full_Plus_One_Shot', 'P-11_Full_Plus_Few_Shot', 'P-1_Minimal', 'P-2_Basic', 'P-3_Diffs_Only', 'P-4_Diffs_Plus_Title', 'P-5_Code_Only', 'P-6_Issue_Only', 'P-7_Template_Plus_Title', 'P-8_Full_Context', 'P-9_Basic_One_Shot', 'original']\n"
     ]
    }
   ],
   "source": [
    "# Prepare generation data for merging\n",
    "# Group by PR ID and prompt variation to get unique records (since multiple files per PR have same generated description)\n",
    "generation_unique = generation_df.groupby(['id', 'prompt_variation']).first().reset_index()\n",
    "print(f\"Generation data after deduplication: {generation_unique.shape}\")\n",
    "\n",
    "# Prepare detection data for merging\n",
    "print(f\"\\nDetection data entry types: {detection_df['entry_type'].value_counts()}\")\n",
    "print(f\"Detection data prompt variations: {sorted(detection_df['prompt_variation'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5fd20804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data shape: (352, 31)\n",
      "Merged data entry types: entry_type\n",
      "generated    352\n",
      "Name: count, dtype: int64\n",
      "Merged data prompt variations: ['P-10_Full_Plus_One_Shot', 'P-11_Full_Plus_Few_Shot', 'P-1_Minimal', 'P-2_Basic', 'P-3_Diffs_Only', 'P-4_Diffs_Plus_Title', 'P-5_Code_Only', 'P-6_Issue_Only', 'P-7_Template_Plus_Title', 'P-8_Full_Context', 'P-9_Basic_One_Shot']\n"
     ]
    }
   ],
   "source": [
    "# Merge generation and detection data\n",
    "# For generation data: use id as pr_id\n",
    "generation_unique['pr_id'] = generation_unique['id']\n",
    "\n",
    "# Merge on pr_id and prompt_variation\n",
    "merged_df = pd.merge(\n",
    "    generation_unique,\n",
    "    detection_df,\n",
    "    on=['pr_id', 'prompt_variation'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"Merged data shape: {merged_df.shape}\")\n",
    "print(f\"Merged data entry types: {merged_df['entry_type'].value_counts()}\")\n",
    "print(f\"Merged data prompt variations: {sorted(merged_df['prompt_variation'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d46294b",
   "metadata": {},
   "source": [
    "## Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4cd649ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_zerogpt_response(response_str):\n",
    "    \"\"\"Parse ZeroGPT response to extract AI probability\"\"\"\n",
    "    try:\n",
    "        if pd.isna(response_str) or response_str == \"\":\n",
    "            return None\n",
    "        \n",
    "        # Try to parse as JSON\n",
    "        if isinstance(response_str, str):\n",
    "            response = json.loads(response_str)\n",
    "            \n",
    "            # Handle nested structure - check if there's a 'data' field\n",
    "            if 'data' in response:\n",
    "                data = response['data']\n",
    "                # Use fakePercentage if available, otherwise calculate from isHuman\n",
    "                if 'fakePercentage' in data:\n",
    "                    return data['fakePercentage']\n",
    "                elif 'isHuman' in data:\n",
    "                    return 100 - data['isHuman']  # Convert isHuman to AI percentage\n",
    "                else:\n",
    "                    return None\n",
    "            else:\n",
    "                # Direct structure\n",
    "                if 'fakePercentage' in response:\n",
    "                    return response['fakePercentage']\n",
    "                elif 'isHuman' in response:\n",
    "                    return 100 - response['isHuman']\n",
    "                else:\n",
    "                    return None\n",
    "        else:\n",
    "            # If it's already a number or can be converted\n",
    "            return float(response_str)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing response: {e} | Response: {response_str[:100] if isinstance(response_str, str) else response_str}\")\n",
    "        return None\n",
    "\n",
    "def calculate_detection_metrics(df, ai_threshold=50.0):  # Changed threshold to 50% since we're dealing with percentages\n",
    "    \"\"\"Calculate detection accuracy metrics including AI probability statistics\"\"\"\n",
    "    # Parse AI probabilities\n",
    "    df = df.copy()  # Avoid SettingWithCopyWarning\n",
    "    df['ai_probability'] = df['zerogpt_response'].apply(parse_zerogpt_response)\n",
    "    \n",
    "    # Filter out rows where AI probability couldn't be parsed\n",
    "    valid_df = df[df['ai_probability'].notna()].copy()\n",
    "    \n",
    "    if len(valid_df) == 0:\n",
    "        return {\n",
    "            'true_positive_pct': 0, \n",
    "            'false_negative_pct': 0, \n",
    "            'total_samples': len(df), \n",
    "            'valid_samples': 0,\n",
    "            'mean_ai_score_generated': 0,\n",
    "            'median_ai_score_generated': 0,\n",
    "            'mean_ai_score_original': 0,\n",
    "            'median_ai_score_original': 0\n",
    "        }\n",
    "    \n",
    "    # Determine if detected as AI (above threshold)\n",
    "    valid_df['detected_as_ai'] = valid_df['ai_probability'] > ai_threshold\n",
    "    \n",
    "    # Calculate metrics\n",
    "    generated_entries = valid_df[valid_df['entry_type'] == 'generated']\n",
    "    original_entries = valid_df[valid_df['entry_type'] == 'original']\n",
    "    \n",
    "    # True Positive Rate: Generated content correctly identified as AI\n",
    "    tp_rate = 0\n",
    "    if len(generated_entries) > 0:\n",
    "        tp_rate = (generated_entries['detected_as_ai'].sum() / len(generated_entries)) * 100\n",
    "    \n",
    "    # False Positive Rate: Original content incorrectly identified as AI (this is what we call \"false negative\" in the context)\n",
    "    fp_rate = 0\n",
    "    if len(original_entries) > 0:\n",
    "        fp_rate = (original_entries['detected_as_ai'].sum() / len(original_entries)) * 100\n",
    "    \n",
    "    # AI probability statistics\n",
    "    mean_ai_generated = generated_entries['ai_probability'].mean() if len(generated_entries) > 0 else 0\n",
    "    median_ai_generated = generated_entries['ai_probability'].median() if len(generated_entries) > 0 else 0\n",
    "    mean_ai_original = original_entries['ai_probability'].mean() if len(original_entries) > 0 else 0\n",
    "    median_ai_original = original_entries['ai_probability'].median() if len(original_entries) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'true_positive_pct': tp_rate,\n",
    "        'false_negative_pct': fp_rate,  # FP on original = FN from human perspective\n",
    "        'total_samples': len(df),\n",
    "        'valid_samples': len(valid_df),\n",
    "        'mean_ai_score_generated': mean_ai_generated,\n",
    "        'median_ai_score_generated': median_ai_generated,\n",
    "        'mean_ai_score_original': mean_ai_original,\n",
    "        'median_ai_score_original': median_ai_original\n",
    "    }\n",
    "\n",
    "def calculate_text_metrics(text_series):\n",
    "    \"\"\"Calculate text-based metrics\"\"\"\n",
    "    if len(text_series) == 0:\n",
    "        return {'mean_length': 0, 'median_length': 0}\n",
    "    \n",
    "    lengths = text_series.str.len()\n",
    "    return {\n",
    "        'mean_length': lengths.mean(),\n",
    "        'median_length': lengths.median()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f467d23",
   "metadata": {},
   "source": [
    "## Main Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c97fa1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debugging merged data structure:\n",
      "Merged data shape: (352, 31)\n",
      "Entry types: entry_type\n",
      "generated    352\n",
      "Name: count, dtype: int64\n",
      "Prompt variations: ['P-10_Full_Plus_One_Shot', 'P-11_Full_Plus_Few_Shot', 'P-1_Minimal', 'P-2_Basic', 'P-3_Diffs_Only', 'P-4_Diffs_Plus_Title', 'P-5_Code_Only', 'P-6_Issue_Only', 'P-7_Template_Plus_Title', 'P-8_Full_Context', 'P-9_Basic_One_Shot']\n",
      "\n",
      "ZeroGPT response data availability:\n",
      "Non-null zerogpt_response: 352\n",
      "Empty zerogpt_response: 0\n",
      "\n",
      "Sample zerogpt responses:\n",
      "Sample 1: {\"success\": true, \"code\": 200, \"message\": \"detection result passed to proxy\", \"data\": {\"sentences\": [], \"isHuman\": 100, \"additional_feedback\": \"\", \"h\": [], \"hi\": [], \"textWords\": 76, \"aiWords\": 0, \"fa...\n",
      "Sample 2: {\"success\": true, \"code\": 200, \"message\": \"detection result passed to proxy\", \"data\": {\"sentences\": [], \"isHuman\": 100, \"additional_feedback\": \"\", \"h\": [], \"hi\": [], \"textWords\": 40, \"aiWords\": 0, \"fa...\n",
      "Sample 3: {\"success\": true, \"code\": 200, \"message\": \"detection result passed to proxy\", \"data\": {\"sentences\": [], \"isHuman\": 100, \"additional_feedback\": \"Please input more text for a more accurate result\", \"h\":...\n",
      "\n",
      "Analyzing prompt variations: ['P-10_Full_Plus_One_Shot', 'P-11_Full_Plus_Few_Shot', 'P-1_Minimal', 'P-2_Basic', 'P-3_Diffs_Only', 'P-4_Diffs_Plus_Title', 'P-5_Code_Only', 'P-6_Issue_Only', 'P-7_Template_Plus_Title', 'P-8_Full_Context', 'P-9_Basic_One_Shot']\n",
      "\n",
      "Analyzing Original descriptions...\n",
      "Original data shape: (0, 31)\n"
     ]
    }
   ],
   "source": [
    "# Debug: Let's check the merged data structure\n",
    "print(\"Debugging merged data structure:\")\n",
    "print(f\"Merged data shape: {merged_df.shape}\")\n",
    "print(f\"Entry types: {merged_df['entry_type'].value_counts()}\")\n",
    "print(f\"Prompt variations: {sorted(merged_df['prompt_variation'].unique())}\")\n",
    "\n",
    "# Check if we have zerogpt_response data\n",
    "print(f\"\\nZeroGPT response data availability:\")\n",
    "print(f\"Non-null zerogpt_response: {merged_df['zerogpt_response'].notna().sum()}\")\n",
    "print(f\"Empty zerogpt_response: {(merged_df['zerogpt_response'] == '').sum()}\")\n",
    "\n",
    "# Look at a sample of zerogpt responses\n",
    "print(\"\\nSample zerogpt responses:\")\n",
    "sample_responses = merged_df[merged_df['zerogpt_response'].notna() & (merged_df['zerogpt_response'] != '')]['zerogpt_response'].head(3)\n",
    "for i, resp in enumerate(sample_responses):\n",
    "    print(f\"Sample {i+1}: {resp[:200]}...\")\n",
    "\n",
    "# Now re-analyze with this understanding\n",
    "analysis_results = []\n",
    "\n",
    "# Get all prompt variations plus 'Original'\n",
    "prompt_variations = sorted([pv for pv in merged_df['prompt_variation'].unique() if pv.startswith('P-')])\n",
    "print(f\"\\nAnalyzing prompt variations: {prompt_variations}\")\n",
    "\n",
    "# First, analyze 'Original' (using original descriptions from any prompt variation)\n",
    "print(\"\\nAnalyzing Original descriptions...\")\n",
    "original_data = merged_df[merged_df['entry_type'] == 'original']\n",
    "print(f\"Original data shape: {original_data.shape}\")\n",
    "\n",
    "if len(original_data) > 0:\n",
    "    # Token metrics - Original doesn't have token usage, so set to 0\n",
    "    original_metrics = {\n",
    "        'prompt_variation': 'Original',\n",
    "        'mean_prompt_tokens': 0,\n",
    "        'median_prompt_tokens': 0,\n",
    "        'mean_completion_tokens': 0,\n",
    "        'median_completion_tokens': 0\n",
    "    }\n",
    "    \n",
    "    # Text metrics\n",
    "    text_metrics = calculate_text_metrics(original_data['input_text'])\n",
    "    original_metrics.update({\n",
    "        'mean_description_length': text_metrics['mean_length'],\n",
    "        'median_description_length': text_metrics['median_length']\n",
    "    })\n",
    "    \n",
    "    # Detection metrics (including new AI probability metrics)\n",
    "    detection_metrics = calculate_detection_metrics(original_data)\n",
    "    original_metrics.update({\n",
    "        'true_positive_pct': detection_metrics['true_positive_pct'],\n",
    "        'false_negative_pct': detection_metrics['false_negative_pct'],\n",
    "        'mean_ai_score_generated': detection_metrics['mean_ai_score_generated'],\n",
    "        'median_ai_score_generated': detection_metrics['median_ai_score_generated'],\n",
    "        'mean_ai_score_original': detection_metrics['mean_ai_score_original'],\n",
    "        'median_ai_score_original': detection_metrics['median_ai_score_original'],\n",
    "        'total_samples': detection_metrics['total_samples'],\n",
    "        'valid_samples': detection_metrics['valid_samples']\n",
    "    })\n",
    "    \n",
    "    analysis_results.append(original_metrics)\n",
    "    print(f\"  Total samples: {detection_metrics['total_samples']}\")\n",
    "    print(f\"  Valid samples: {detection_metrics['valid_samples']}\")\n",
    "    print(f\"  Mean description length: {text_metrics['mean_length']:.1f}\")\n",
    "    print(f\"  False negative rate: {detection_metrics['false_negative_pct']:.1f}%\")\n",
    "    print(f\"  Mean AI score (original): {detection_metrics['mean_ai_score_original']:.1f}%\")\n",
    "    print(f\"  Median AI score (original): {detection_metrics['median_ai_score_original']:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "57916e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing P-10_Full_Plus_One_Shot...\n",
      "  Samples: 32 (Generated: 32)\n",
      "  Mean prompt tokens: 65330\n",
      "  Mean completion tokens: 407\n",
      "  Mean description length: 901.1\n",
      "  True positive rate: 15.6%\n",
      "  False negative rate: 0.0%\n",
      "  Mean AI score (generated): 19.5%\n",
      "  Mean AI score (original): 0.0%\n",
      "\n",
      "Analyzing P-11_Full_Plus_Few_Shot...\n",
      "  Samples: 32 (Generated: 32)\n",
      "  Mean prompt tokens: 75784\n",
      "  Mean completion tokens: 321\n",
      "  Mean description length: 740.2\n",
      "  True positive rate: 15.6%\n",
      "  False negative rate: 0.0%\n",
      "  Mean AI score (generated): 17.5%\n",
      "  Mean AI score (original): 0.0%\n",
      "\n",
      "Analyzing P-1_Minimal...\n",
      "  Samples: 32 (Generated: 32)\n",
      "  Mean prompt tokens: 567\n",
      "  Mean completion tokens: 134\n",
      "  Mean description length: 380.5\n",
      "  True positive rate: 3.1%\n",
      "  False negative rate: 0.0%\n",
      "  Mean AI score (generated): 5.1%\n",
      "  Mean AI score (original): 0.0%\n",
      "\n",
      "Analyzing P-2_Basic...\n",
      "  Samples: 32 (Generated: 32)\n",
      "  Mean prompt tokens: 1728\n",
      "  Mean completion tokens: 226\n",
      "  Mean description length: 552.1\n",
      "  True positive rate: 12.5%\n",
      "  False negative rate: 0.0%\n",
      "  Mean AI score (generated): 13.9%\n",
      "  Mean AI score (original): 0.0%\n",
      "\n",
      "Analyzing P-3_Diffs_Only...\n",
      "  Samples: 32 (Generated: 32)\n",
      "  Mean prompt tokens: 6618\n",
      "  Mean completion tokens: 304\n",
      "  Mean description length: 774.2\n",
      "  True positive rate: 3.1%\n",
      "  False negative rate: 0.0%\n",
      "  Mean AI score (generated): 8.8%\n",
      "  Mean AI score (original): 0.0%\n",
      "\n",
      "Analyzing P-4_Diffs_Plus_Title...\n",
      "  Samples: 32 (Generated: 32)\n",
      "  Mean prompt tokens: 6652\n",
      "  Mean completion tokens: 295\n",
      "  Mean description length: 718.6\n",
      "  True positive rate: 12.5%\n",
      "  False negative rate: 0.0%\n",
      "  Mean AI score (generated): 12.2%\n",
      "  Mean AI score (original): 0.0%\n",
      "\n",
      "Analyzing P-5_Code_Only...\n",
      "  Samples: 32 (Generated: 32)\n",
      "  Mean prompt tokens: 57595\n",
      "  Mean completion tokens: 316\n",
      "  Mean description length: 794.9\n",
      "  True positive rate: 9.4%\n",
      "  False negative rate: 0.0%\n",
      "  Mean AI score (generated): 9.7%\n",
      "  Mean AI score (original): 0.0%\n",
      "\n",
      "Analyzing P-6_Issue_Only...\n",
      "  Samples: 32 (Generated: 32)\n",
      "  Mean prompt tokens: 3130\n",
      "  Mean completion tokens: 299\n",
      "  Mean description length: 655.7\n",
      "  True positive rate: 6.2%\n",
      "  False negative rate: 0.0%\n",
      "  Mean AI score (generated): 14.4%\n",
      "  Mean AI score (original): 0.0%\n",
      "\n",
      "Analyzing P-7_Template_Plus_Title...\n",
      "  Samples: 32 (Generated: 32)\n",
      "  Mean prompt tokens: 1871\n",
      "  Mean completion tokens: 249\n",
      "  Mean description length: 604.3\n",
      "  True positive rate: 15.6%\n",
      "  False negative rate: 0.0%\n",
      "  Mean AI score (generated): 21.2%\n",
      "  Mean AI score (original): 0.0%\n",
      "\n",
      "Analyzing P-8_Full_Context...\n",
      "  Samples: 32 (Generated: 32)\n",
      "  Mean prompt tokens: 59099\n",
      "  Mean completion tokens: 348\n",
      "  Mean description length: 786.4\n",
      "  True positive rate: 12.5%\n",
      "  False negative rate: 0.0%\n",
      "  Mean AI score (generated): 16.4%\n",
      "  Mean AI score (original): 0.0%\n",
      "\n",
      "Analyzing P-9_Basic_One_Shot...\n",
      "  Samples: 32 (Generated: 32)\n",
      "  Mean prompt tokens: 4010\n",
      "  Mean completion tokens: 272\n",
      "  Mean description length: 595.7\n",
      "  True positive rate: 15.6%\n",
      "  False negative rate: 0.0%\n",
      "  Mean AI score (generated): 23.2%\n",
      "  Mean AI score (original): 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Now analyze each prompt variation\n",
    "for pv in prompt_variations:\n",
    "    print(f\"\\nAnalyzing {pv}...\")\n",
    "    \n",
    "    # Get data for this prompt variation\n",
    "    pv_data = merged_df[merged_df['prompt_variation'] == pv]\n",
    "    \n",
    "    if len(pv_data) == 0:\n",
    "        print(f\"  No data found for {pv}\")\n",
    "        continue\n",
    "    \n",
    "    # Get generation data for token metrics (only generated entries have token info)\n",
    "    pv_generated = pv_data[pv_data['entry_type'] == 'generated']\n",
    "    \n",
    "    pv_metrics = {'prompt_variation': pv}\n",
    "    \n",
    "    # Token metrics\n",
    "    if len(pv_generated) > 0 and 'total_input_tokens' in pv_generated.columns:\n",
    "        pv_metrics.update({\n",
    "            'mean_prompt_tokens': pv_generated['total_input_tokens'].mean(),\n",
    "            'median_prompt_tokens': pv_generated['total_input_tokens'].median(),\n",
    "            'mean_completion_tokens': pv_generated['total_output_tokens'].mean(),\n",
    "            'median_completion_tokens': pv_generated['total_output_tokens'].median()\n",
    "        })\n",
    "    else:\n",
    "        pv_metrics.update({\n",
    "            'mean_prompt_tokens': 0,\n",
    "            'median_prompt_tokens': 0,\n",
    "            'mean_completion_tokens': 0,\n",
    "            'median_completion_tokens': 0\n",
    "        })\n",
    "    \n",
    "    # Text metrics (using generated descriptions)\n",
    "    if len(pv_generated) > 0:\n",
    "        text_metrics = calculate_text_metrics(pv_generated['input_text'])\n",
    "        pv_metrics.update({\n",
    "            'mean_description_length': text_metrics['mean_length'],\n",
    "            'median_description_length': text_metrics['median_length']\n",
    "        })\n",
    "    else:\n",
    "        pv_metrics.update({\n",
    "            'mean_description_length': 0,\n",
    "            'median_description_length': 0\n",
    "        })\n",
    "    \n",
    "    # Detection metrics (using both original and generated, including new AI probability metrics)\n",
    "    detection_metrics = calculate_detection_metrics(pv_data)\n",
    "    pv_metrics.update({\n",
    "        'true_positive_pct': detection_metrics['true_positive_pct'],\n",
    "        'false_negative_pct': detection_metrics['false_negative_pct'],\n",
    "        'mean_ai_score_generated': detection_metrics['mean_ai_score_generated'],\n",
    "        'median_ai_score_generated': detection_metrics['median_ai_score_generated'],\n",
    "        'mean_ai_score_original': detection_metrics['mean_ai_score_original'],\n",
    "        'median_ai_score_original': detection_metrics['median_ai_score_original'],\n",
    "        'total_samples': detection_metrics['total_samples']\n",
    "    })\n",
    "    \n",
    "    analysis_results.append(pv_metrics)\n",
    "    \n",
    "    print(f\"  Samples: {detection_metrics['total_samples']} (Generated: {len(pv_generated)})\")\n",
    "    print(f\"  Mean prompt tokens: {pv_metrics['mean_prompt_tokens']:.0f}\")\n",
    "    print(f\"  Mean completion tokens: {pv_metrics['mean_completion_tokens']:.0f}\")\n",
    "    print(f\"  Mean description length: {pv_metrics['mean_description_length']:.1f}\")\n",
    "    print(f\"  True positive rate: {detection_metrics['true_positive_pct']:.1f}%\")\n",
    "    print(f\"  False negative rate: {detection_metrics['false_negative_pct']:.1f}%\")\n",
    "    print(f\"  Mean AI score (generated): {detection_metrics['mean_ai_score_generated']:.1f}%\")\n",
    "    print(f\"  Mean AI score (original): {detection_metrics['mean_ai_score_original']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b835b3c3",
   "metadata": {},
   "source": [
    "## Results Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3a232af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PROMPT VARIATIONS ANALYSIS RESULTS ===\n",
      "\n",
      "Target Output Table with AI Probability Scores:\n",
      "================================================================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_variation</th>\n",
       "      <th>mean_prompt_tokens</th>\n",
       "      <th>median_prompt_tokens</th>\n",
       "      <th>mean_completion_tokens</th>\n",
       "      <th>median_completion_tokens</th>\n",
       "      <th>mean_description_length</th>\n",
       "      <th>median_description_length</th>\n",
       "      <th>true_positive_pct</th>\n",
       "      <th>false_negative_pct</th>\n",
       "      <th>mean_ai_score_generated</th>\n",
       "      <th>median_ai_score_generated</th>\n",
       "      <th>mean_ai_score_original</th>\n",
       "      <th>median_ai_score_original</th>\n",
       "      <th>total_samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P-10_Full_Plus_One_Shot</td>\n",
       "      <td>65330.5</td>\n",
       "      <td>29377.0</td>\n",
       "      <td>407.1</td>\n",
       "      <td>196.5</td>\n",
       "      <td>901.1</td>\n",
       "      <td>710.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>0</td>\n",
       "      <td>19.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P-11_Full_Plus_Few_Shot</td>\n",
       "      <td>75784.3</td>\n",
       "      <td>35906.5</td>\n",
       "      <td>320.8</td>\n",
       "      <td>164.0</td>\n",
       "      <td>740.2</td>\n",
       "      <td>537.5</td>\n",
       "      <td>15.6</td>\n",
       "      <td>0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P-1_Minimal</td>\n",
       "      <td>567.2</td>\n",
       "      <td>355.0</td>\n",
       "      <td>133.9</td>\n",
       "      <td>73.0</td>\n",
       "      <td>380.5</td>\n",
       "      <td>223.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P-2_Basic</td>\n",
       "      <td>1728.3</td>\n",
       "      <td>1077.5</td>\n",
       "      <td>226.3</td>\n",
       "      <td>123.0</td>\n",
       "      <td>552.1</td>\n",
       "      <td>395.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0</td>\n",
       "      <td>13.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P-3_Diffs_Only</td>\n",
       "      <td>6618.4</td>\n",
       "      <td>2687.0</td>\n",
       "      <td>304.5</td>\n",
       "      <td>152.0</td>\n",
       "      <td>774.2</td>\n",
       "      <td>517.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0</td>\n",
       "      <td>8.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>P-4_Diffs_Plus_Title</td>\n",
       "      <td>6652.5</td>\n",
       "      <td>2703.5</td>\n",
       "      <td>295.5</td>\n",
       "      <td>168.5</td>\n",
       "      <td>718.6</td>\n",
       "      <td>517.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0</td>\n",
       "      <td>12.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>P-5_Code_Only</td>\n",
       "      <td>57594.6</td>\n",
       "      <td>25136.5</td>\n",
       "      <td>315.9</td>\n",
       "      <td>175.0</td>\n",
       "      <td>794.9</td>\n",
       "      <td>514.0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>P-6_Issue_Only</td>\n",
       "      <td>3129.8</td>\n",
       "      <td>1490.0</td>\n",
       "      <td>299.2</td>\n",
       "      <td>162.5</td>\n",
       "      <td>655.7</td>\n",
       "      <td>540.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0</td>\n",
       "      <td>14.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>P-7_Template_Plus_Title</td>\n",
       "      <td>1871.2</td>\n",
       "      <td>1166.0</td>\n",
       "      <td>248.7</td>\n",
       "      <td>131.5</td>\n",
       "      <td>604.3</td>\n",
       "      <td>492.5</td>\n",
       "      <td>15.6</td>\n",
       "      <td>0</td>\n",
       "      <td>21.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>P-8_Full_Context</td>\n",
       "      <td>59098.9</td>\n",
       "      <td>25511.5</td>\n",
       "      <td>347.7</td>\n",
       "      <td>226.0</td>\n",
       "      <td>786.4</td>\n",
       "      <td>681.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0</td>\n",
       "      <td>16.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>P-9_Basic_One_Shot</td>\n",
       "      <td>4010.1</td>\n",
       "      <td>2491.5</td>\n",
       "      <td>272.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>595.7</td>\n",
       "      <td>454.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           prompt_variation  mean_prompt_tokens  median_prompt_tokens  \\\n",
       "0   P-10_Full_Plus_One_Shot             65330.5               29377.0   \n",
       "1   P-11_Full_Plus_Few_Shot             75784.3               35906.5   \n",
       "2               P-1_Minimal               567.2                 355.0   \n",
       "3                 P-2_Basic              1728.3                1077.5   \n",
       "4            P-3_Diffs_Only              6618.4                2687.0   \n",
       "5      P-4_Diffs_Plus_Title              6652.5                2703.5   \n",
       "6             P-5_Code_Only             57594.6               25136.5   \n",
       "7            P-6_Issue_Only              3129.8                1490.0   \n",
       "8   P-7_Template_Plus_Title              1871.2                1166.0   \n",
       "9          P-8_Full_Context             59098.9               25511.5   \n",
       "10       P-9_Basic_One_Shot              4010.1                2491.5   \n",
       "\n",
       "    mean_completion_tokens  median_completion_tokens  mean_description_length  \\\n",
       "0                    407.1                     196.5                    901.1   \n",
       "1                    320.8                     164.0                    740.2   \n",
       "2                    133.9                      73.0                    380.5   \n",
       "3                    226.3                     123.0                    552.1   \n",
       "4                    304.5                     152.0                    774.2   \n",
       "5                    295.5                     168.5                    718.6   \n",
       "6                    315.9                     175.0                    794.9   \n",
       "7                    299.2                     162.5                    655.7   \n",
       "8                    248.7                     131.5                    604.3   \n",
       "9                    347.7                     226.0                    786.4   \n",
       "10                   272.0                     152.0                    595.7   \n",
       "\n",
       "    median_description_length  true_positive_pct  false_negative_pct  \\\n",
       "0                       710.0               15.6                   0   \n",
       "1                       537.5               15.6                   0   \n",
       "2                       223.0                3.1                   0   \n",
       "3                       395.0               12.5                   0   \n",
       "4                       517.0                3.1                   0   \n",
       "5                       517.0               12.5                   0   \n",
       "6                       514.0                9.4                   0   \n",
       "7                       540.0                6.2                   0   \n",
       "8                       492.5               15.6                   0   \n",
       "9                       681.0               12.5                   0   \n",
       "10                      454.0               15.6                   0   \n",
       "\n",
       "    mean_ai_score_generated  median_ai_score_generated  \\\n",
       "0                      19.5                        0.0   \n",
       "1                      17.5                        0.0   \n",
       "2                       5.1                        0.0   \n",
       "3                      13.9                        0.0   \n",
       "4                       8.8                        0.0   \n",
       "5                      12.2                        0.0   \n",
       "6                       9.7                        0.0   \n",
       "7                      14.4                        0.0   \n",
       "8                      21.2                        0.0   \n",
       "9                      16.4                        0.0   \n",
       "10                     23.2                        4.1   \n",
       "\n",
       "    mean_ai_score_original  median_ai_score_original  total_samples  \n",
       "0                        0                         0             32  \n",
       "1                        0                         0             32  \n",
       "2                        0                         0             32  \n",
       "3                        0                         0             32  \n",
       "4                        0                         0             32  \n",
       "5                        0                         0             32  \n",
       "6                        0                         0             32  \n",
       "7                        0                         0             32  \n",
       "8                        0                         0             32  \n",
       "9                        0                         0             32  \n",
       "10                       0                         0             32  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create results dataframe\n",
    "results_df = pd.DataFrame(analysis_results)\n",
    "\n",
    "# Reorder columns as requested, including new AI probability metrics\n",
    "column_order = [\n",
    "    'prompt_variation',\n",
    "    'mean_prompt_tokens',\n",
    "    'median_prompt_tokens', \n",
    "    'mean_completion_tokens',\n",
    "    'median_completion_tokens',\n",
    "    'mean_description_length',\n",
    "    'median_description_length',\n",
    "    'true_positive_pct',\n",
    "    'false_negative_pct',\n",
    "    'mean_ai_score_generated',\n",
    "    'median_ai_score_generated',\n",
    "    'mean_ai_score_original',\n",
    "    'median_ai_score_original',\n",
    "    'total_samples'\n",
    "]\n",
    "\n",
    "results_df = results_df[column_order]\n",
    "\n",
    "# Round numeric columns for better display\n",
    "numeric_cols = [col for col in results_df.columns if col != 'prompt_variation']\n",
    "results_df[numeric_cols] = results_df[numeric_cols].round(1)\n",
    "\n",
    "print(\"\\n=== PROMPT VARIATIONS ANALYSIS RESULTS ===\")\n",
    "print(\"\\nTarget Output Table with AI Probability Scores:\")\n",
    "print(\"=\" * 160)\n",
    "\n",
    "# Display the table\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ba72e713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved to: prompt_variations_analysis_results.csv\n",
      "\n",
      "=== SUMMARY STATISTICS ===\n",
      "Total prompt variations analyzed: 11\n",
      "Best performing prompt (highest true positive rate): P-10_Full_Plus_One_Shot\n",
      "Most efficient prompt (lowest mean prompt tokens): P-1_Minimal\n",
      "Longest descriptions (highest mean length): P-10_Full_Plus_One_Shot\n"
     ]
    }
   ],
   "source": [
    "# Save results to CSV\n",
    "output_file = \"prompt_variations_analysis_results.csv\"\n",
    "results_df.to_csv(output_file, index=False)\n",
    "print(f\"\\nResults saved to: {output_file}\")\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\n=== SUMMARY STATISTICS ===\")\n",
    "print(f\"Total prompt variations analyzed: {len(results_df)}\")\n",
    "print(f\"Best performing prompt (highest true positive rate): {results_df.loc[results_df['true_positive_pct'].idxmax(), 'prompt_variation']}\")\n",
    "print(f\"Most efficient prompt (lowest mean prompt tokens): {results_df[results_df['prompt_variation'] != 'Original'].loc[results_df[results_df['prompt_variation'] != 'Original']['mean_prompt_tokens'].idxmin(), 'prompt_variation']}\")\n",
    "print(f\"Longest descriptions (highest mean length): {results_df.loc[results_df['mean_description_length'].idxmax(), 'prompt_variation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88735b38",
   "metadata": {},
   "source": [
    "## Additional Textual Analysis Features\n",
    "\n",
    "Now let's add some additional textual analysis features similar to the jabref-prs-comparison notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e98d514e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended textual analysis functions defined.\n"
     ]
    }
   ],
   "source": [
    "def extract_zerogpt_text_metrics(response_str):\n",
    "    \"\"\"Extract textual metrics from ZeroGPT response\"\"\"\n",
    "    try:\n",
    "        if pd.isna(response_str) or response_str == \"\":\n",
    "            return {'textWords': None, 'aiWords': None, 'word_count': None}\n",
    "        \n",
    "        response = json.loads(response_str)\n",
    "        \n",
    "        if 'data' in response:\n",
    "            data = response['data']\n",
    "            return {\n",
    "                'textWords': data.get('textWords', None),\n",
    "                'aiWords': data.get('aiWords', None),\n",
    "                'word_count': data.get('textWords', None)  # Same as textWords\n",
    "            }\n",
    "        else:\n",
    "            return {'textWords': None, 'aiWords': None, 'word_count': None}\n",
    "    except:\n",
    "        return {'textWords': None, 'aiWords': None, 'word_count': None}\n",
    "\n",
    "def calculate_extended_text_metrics(df):\n",
    "    \"\"\"Calculate extended textual metrics\"\"\"\n",
    "    if len(df) == 0:\n",
    "        return {}\n",
    "    \n",
    "    # Basic length metrics\n",
    "    lengths = df['input_text'].str.len()\n",
    "    \n",
    "    # Extract ZeroGPT word counts\n",
    "    zerogpt_metrics = df['zerogpt_response'].apply(extract_zerogpt_text_metrics)\n",
    "    word_counts = pd.DataFrame(zerogpt_metrics.tolist())['textWords'].dropna()\n",
    "    \n",
    "    # Calculate sentences (approximate by counting periods, exclamation marks, question marks)\n",
    "    sentence_counts = df['input_text'].str.count(r'[.!?]+')\n",
    "    \n",
    "    # Calculate newlines (as a proxy for paragraph structure)\n",
    "    newline_counts = df['input_text'].str.count(r'\\\\n')\n",
    "    \n",
    "    return {\n",
    "        'mean_char_length': lengths.mean(),\n",
    "        'median_char_length': lengths.median(),\n",
    "        'mean_word_count': word_counts.mean() if len(word_counts) > 0 else 0,\n",
    "        'median_word_count': word_counts.median() if len(word_counts) > 0 else 0,\n",
    "        'mean_sentence_count': sentence_counts.mean(),\n",
    "        'median_sentence_count': sentence_counts.median(),\n",
    "        'mean_newline_count': newline_counts.mean(),\n",
    "        'median_newline_count': newline_counts.median()\n",
    "    }\n",
    "\n",
    "print(\"Extended textual analysis functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5add51c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXTENDED TEXTUAL ANALYSIS ===\\n\n",
      "P-10_Full_Plus_One_Shot - Samples: 32\n",
      "  Mean words: 125.3\n",
      "  Mean sentences: 6.8\n",
      "  Mean newlines: 0.0\\n\n",
      "P-11_Full_Plus_Few_Shot - Samples: 32\n",
      "  Mean words: 103.2\n",
      "  Mean sentences: 6.2\n",
      "  Mean newlines: 0.0\\n\n",
      "P-1_Minimal - Samples: 32\n",
      "  Mean words: 49.0\n",
      "  Mean sentences: 4.2\n",
      "  Mean newlines: 0.0\\n\n",
      "P-2_Basic - Samples: 32\n",
      "  Mean words: 74.3\n",
      "  Mean sentences: 4.7\n",
      "  Mean newlines: 0.0\\n\n",
      "P-3_Diffs_Only - Samples: 32\n",
      "  Mean words: 106.9\n",
      "  Mean sentences: 6.4\n",
      "  Mean newlines: 0.0\\n\n",
      "P-4_Diffs_Plus_Title - Samples: 32\n",
      "  Mean words: 99.6\n",
      "  Mean sentences: 5.7\n",
      "  Mean newlines: 0.0\\n\n",
      "P-5_Code_Only - Samples: 32\n",
      "  Mean words: 108.4\n",
      "  Mean sentences: 6.4\n",
      "  Mean newlines: 0.0\\n\n",
      "P-6_Issue_Only - Samples: 32\n",
      "  Mean words: 92.2\n",
      "  Mean sentences: 5.6\n",
      "  Mean newlines: 0.0\\n\n",
      "P-7_Template_Plus_Title - Samples: 32\n",
      "  Mean words: 81.2\n",
      "  Mean sentences: 5.2\n",
      "  Mean newlines: 0.0\\n\n",
      "P-8_Full_Context - Samples: 32\n",
      "  Mean words: 111.8\n",
      "  Mean sentences: 6.0\n",
      "  Mean newlines: 0.0\\n\n",
      "P-9_Basic_One_Shot - Samples: 32\n",
      "  Mean words: 79.8\n",
      "  Mean sentences: 5.3\n",
      "  Mean newlines: 0.0\\n\n"
     ]
    }
   ],
   "source": [
    "# Extended textual analysis for each prompt variation\n",
    "print(\"=== EXTENDED TEXTUAL ANALYSIS ===\\\\n\")\n",
    "\n",
    "extended_analysis_results = []\n",
    "\n",
    "# Analyze Original\n",
    "original_data = merged_df[merged_df['entry_type'] == 'original']\n",
    "if len(original_data) > 0:\n",
    "    ext_metrics = calculate_extended_text_metrics(original_data)\n",
    "    ext_metrics['prompt_variation'] = 'Original'\n",
    "    extended_analysis_results.append(ext_metrics)\n",
    "    print(f\"Original - Samples: {len(original_data)}\")\n",
    "    print(f\"  Mean words: {ext_metrics['mean_word_count']:.1f}\")\n",
    "    print(f\"  Mean sentences: {ext_metrics['mean_sentence_count']:.1f}\")\n",
    "    print(f\"  Mean newlines: {ext_metrics['mean_newline_count']:.1f}\\\\n\")\n",
    "\n",
    "# Analyze each prompt variation\n",
    "for pv in prompt_variations:\n",
    "    pv_data = merged_df[merged_df['prompt_variation'] == pv]\n",
    "    pv_generated = pv_data[pv_data['entry_type'] == 'generated']\n",
    "    \n",
    "    if len(pv_generated) > 0:\n",
    "        ext_metrics = calculate_extended_text_metrics(pv_generated)\n",
    "        ext_metrics['prompt_variation'] = pv\n",
    "        extended_analysis_results.append(ext_metrics)\n",
    "        print(f\"{pv} - Samples: {len(pv_generated)}\")\n",
    "        print(f\"  Mean words: {ext_metrics['mean_word_count']:.1f}\")\n",
    "        print(f\"  Mean sentences: {ext_metrics['mean_sentence_count']:.1f}\")\n",
    "        print(f\"  Mean newlines: {ext_metrics['mean_newline_count']:.1f}\\\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "63d3ea97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COMPREHENSIVE ANALYSIS RESULTS ===\n",
      "\n",
      "Final Table with Textual Features and AI Probability Scores:\n",
      "====================================================================================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_variation</th>\n",
       "      <th>mean_prompt_tokens</th>\n",
       "      <th>median_prompt_tokens</th>\n",
       "      <th>mean_completion_tokens</th>\n",
       "      <th>median_completion_tokens</th>\n",
       "      <th>mean_description_length</th>\n",
       "      <th>median_description_length</th>\n",
       "      <th>mean_word_count</th>\n",
       "      <th>median_word_count</th>\n",
       "      <th>mean_sentence_count</th>\n",
       "      <th>median_sentence_count</th>\n",
       "      <th>true_positive_pct</th>\n",
       "      <th>false_negative_pct</th>\n",
       "      <th>mean_ai_score_generated</th>\n",
       "      <th>median_ai_score_generated</th>\n",
       "      <th>mean_ai_score_original</th>\n",
       "      <th>median_ai_score_original</th>\n",
       "      <th>total_samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P-10_Full_Plus_One_Shot</td>\n",
       "      <td>65330.5</td>\n",
       "      <td>29377.0</td>\n",
       "      <td>407.1</td>\n",
       "      <td>196.5</td>\n",
       "      <td>901.1</td>\n",
       "      <td>710.0</td>\n",
       "      <td>125.3</td>\n",
       "      <td>96.5</td>\n",
       "      <td>6.8</td>\n",
       "      <td>5.5</td>\n",
       "      <td>15.6</td>\n",
       "      <td>0</td>\n",
       "      <td>19.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P-11_Full_Plus_Few_Shot</td>\n",
       "      <td>75784.3</td>\n",
       "      <td>35906.5</td>\n",
       "      <td>320.8</td>\n",
       "      <td>164.0</td>\n",
       "      <td>740.2</td>\n",
       "      <td>537.5</td>\n",
       "      <td>103.2</td>\n",
       "      <td>81.5</td>\n",
       "      <td>6.2</td>\n",
       "      <td>5.5</td>\n",
       "      <td>15.6</td>\n",
       "      <td>0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P-1_Minimal</td>\n",
       "      <td>567.2</td>\n",
       "      <td>355.0</td>\n",
       "      <td>133.9</td>\n",
       "      <td>73.0</td>\n",
       "      <td>380.5</td>\n",
       "      <td>223.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P-2_Basic</td>\n",
       "      <td>1728.3</td>\n",
       "      <td>1077.5</td>\n",
       "      <td>226.3</td>\n",
       "      <td>123.0</td>\n",
       "      <td>552.1</td>\n",
       "      <td>395.0</td>\n",
       "      <td>74.3</td>\n",
       "      <td>59.5</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0</td>\n",
       "      <td>13.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P-3_Diffs_Only</td>\n",
       "      <td>6618.4</td>\n",
       "      <td>2687.0</td>\n",
       "      <td>304.5</td>\n",
       "      <td>152.0</td>\n",
       "      <td>774.2</td>\n",
       "      <td>517.0</td>\n",
       "      <td>106.9</td>\n",
       "      <td>71.5</td>\n",
       "      <td>6.4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0</td>\n",
       "      <td>8.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>P-4_Diffs_Plus_Title</td>\n",
       "      <td>6652.5</td>\n",
       "      <td>2703.5</td>\n",
       "      <td>295.5</td>\n",
       "      <td>168.5</td>\n",
       "      <td>718.6</td>\n",
       "      <td>517.0</td>\n",
       "      <td>99.6</td>\n",
       "      <td>77.0</td>\n",
       "      <td>5.7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0</td>\n",
       "      <td>12.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>P-5_Code_Only</td>\n",
       "      <td>57594.6</td>\n",
       "      <td>25136.5</td>\n",
       "      <td>315.9</td>\n",
       "      <td>175.0</td>\n",
       "      <td>794.9</td>\n",
       "      <td>514.0</td>\n",
       "      <td>108.4</td>\n",
       "      <td>67.5</td>\n",
       "      <td>6.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>P-6_Issue_Only</td>\n",
       "      <td>3129.8</td>\n",
       "      <td>1490.0</td>\n",
       "      <td>299.2</td>\n",
       "      <td>162.5</td>\n",
       "      <td>655.7</td>\n",
       "      <td>540.0</td>\n",
       "      <td>92.2</td>\n",
       "      <td>78.5</td>\n",
       "      <td>5.6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0</td>\n",
       "      <td>14.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>P-7_Template_Plus_Title</td>\n",
       "      <td>1871.2</td>\n",
       "      <td>1166.0</td>\n",
       "      <td>248.7</td>\n",
       "      <td>131.5</td>\n",
       "      <td>604.3</td>\n",
       "      <td>492.5</td>\n",
       "      <td>81.2</td>\n",
       "      <td>68.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>0</td>\n",
       "      <td>21.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>P-8_Full_Context</td>\n",
       "      <td>59098.9</td>\n",
       "      <td>25511.5</td>\n",
       "      <td>347.7</td>\n",
       "      <td>226.0</td>\n",
       "      <td>786.4</td>\n",
       "      <td>681.0</td>\n",
       "      <td>111.8</td>\n",
       "      <td>94.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0</td>\n",
       "      <td>16.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>P-9_Basic_One_Shot</td>\n",
       "      <td>4010.1</td>\n",
       "      <td>2491.5</td>\n",
       "      <td>272.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>595.7</td>\n",
       "      <td>454.0</td>\n",
       "      <td>79.8</td>\n",
       "      <td>64.0</td>\n",
       "      <td>5.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           prompt_variation  mean_prompt_tokens  median_prompt_tokens  \\\n",
       "0   P-10_Full_Plus_One_Shot             65330.5               29377.0   \n",
       "1   P-11_Full_Plus_Few_Shot             75784.3               35906.5   \n",
       "2               P-1_Minimal               567.2                 355.0   \n",
       "3                 P-2_Basic              1728.3                1077.5   \n",
       "4            P-3_Diffs_Only              6618.4                2687.0   \n",
       "5      P-4_Diffs_Plus_Title              6652.5                2703.5   \n",
       "6             P-5_Code_Only             57594.6               25136.5   \n",
       "7            P-6_Issue_Only              3129.8                1490.0   \n",
       "8   P-7_Template_Plus_Title              1871.2                1166.0   \n",
       "9          P-8_Full_Context             59098.9               25511.5   \n",
       "10       P-9_Basic_One_Shot              4010.1                2491.5   \n",
       "\n",
       "    mean_completion_tokens  median_completion_tokens  mean_description_length  \\\n",
       "0                    407.1                     196.5                    901.1   \n",
       "1                    320.8                     164.0                    740.2   \n",
       "2                    133.9                      73.0                    380.5   \n",
       "3                    226.3                     123.0                    552.1   \n",
       "4                    304.5                     152.0                    774.2   \n",
       "5                    295.5                     168.5                    718.6   \n",
       "6                    315.9                     175.0                    794.9   \n",
       "7                    299.2                     162.5                    655.7   \n",
       "8                    248.7                     131.5                    604.3   \n",
       "9                    347.7                     226.0                    786.4   \n",
       "10                   272.0                     152.0                    595.7   \n",
       "\n",
       "    median_description_length  mean_word_count  median_word_count  \\\n",
       "0                       710.0            125.3               96.5   \n",
       "1                       537.5            103.2               81.5   \n",
       "2                       223.0             49.0               27.0   \n",
       "3                       395.0             74.3               59.5   \n",
       "4                       517.0            106.9               71.5   \n",
       "5                       517.0             99.6               77.0   \n",
       "6                       514.0            108.4               67.5   \n",
       "7                       540.0             92.2               78.5   \n",
       "8                       492.5             81.2               68.0   \n",
       "9                       681.0            111.8               94.0   \n",
       "10                      454.0             79.8               64.0   \n",
       "\n",
       "    mean_sentence_count  median_sentence_count  true_positive_pct  \\\n",
       "0                   6.8                    5.5               15.6   \n",
       "1                   6.2                    5.5               15.6   \n",
       "2                   4.2                    3.0                3.1   \n",
       "3                   4.7                    4.0               12.5   \n",
       "4                   6.4                    5.0                3.1   \n",
       "5                   5.7                    4.0               12.5   \n",
       "6                   6.4                    4.5                9.4   \n",
       "7                   5.6                    5.0                6.2   \n",
       "8                   5.2                    5.0               15.6   \n",
       "9                   6.0                    5.0               12.5   \n",
       "10                  5.3                    4.0               15.6   \n",
       "\n",
       "    false_negative_pct  mean_ai_score_generated  median_ai_score_generated  \\\n",
       "0                    0                     19.5                        0.0   \n",
       "1                    0                     17.5                        0.0   \n",
       "2                    0                      5.1                        0.0   \n",
       "3                    0                     13.9                        0.0   \n",
       "4                    0                      8.8                        0.0   \n",
       "5                    0                     12.2                        0.0   \n",
       "6                    0                      9.7                        0.0   \n",
       "7                    0                     14.4                        0.0   \n",
       "8                    0                     21.2                        0.0   \n",
       "9                    0                     16.4                        0.0   \n",
       "10                   0                     23.2                        4.1   \n",
       "\n",
       "    mean_ai_score_original  median_ai_score_original  total_samples  \n",
       "0                        0                         0             32  \n",
       "1                        0                         0             32  \n",
       "2                        0                         0             32  \n",
       "3                        0                         0             32  \n",
       "4                        0                         0             32  \n",
       "5                        0                         0             32  \n",
       "6                        0                         0             32  \n",
       "7                        0                         0             32  \n",
       "8                        0                         0             32  \n",
       "9                        0                         0             32  \n",
       "10                       0                         0             32  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create extended results DataFrame\n",
    "extended_results_df = pd.DataFrame(extended_analysis_results)\n",
    "\n",
    "# Merge with original results\n",
    "final_results = pd.merge(results_df, extended_results_df, on='prompt_variation', how='left')\n",
    "\n",
    "# Reorder columns for the final comprehensive table\n",
    "final_column_order = [\n",
    "    'prompt_variation',\n",
    "    'mean_prompt_tokens',\n",
    "    'median_prompt_tokens', \n",
    "    'mean_completion_tokens',\n",
    "    'median_completion_tokens',\n",
    "    'mean_description_length',\n",
    "    'median_description_length',\n",
    "    'mean_word_count',\n",
    "    'median_word_count',\n",
    "    'mean_sentence_count',\n",
    "    'median_sentence_count',\n",
    "    'true_positive_pct',\n",
    "    'false_negative_pct',\n",
    "    'mean_ai_score_generated',\n",
    "    'median_ai_score_generated',\n",
    "    'mean_ai_score_original',\n",
    "    'median_ai_score_original',\n",
    "    'total_samples'\n",
    "]\n",
    "\n",
    "final_results = final_results[final_column_order]\n",
    "\n",
    "# Round numeric columns for better display\n",
    "numeric_cols = [col for col in final_results.columns if col != 'prompt_variation']\n",
    "final_results[numeric_cols] = final_results[numeric_cols].round(1)\n",
    "\n",
    "print(\"\\n=== COMPREHENSIVE ANALYSIS RESULTS ===\\n\")\n",
    "print(\"Final Table with Textual Features and AI Probability Scores:\")\n",
    "print(\"=\" * 180)\n",
    "\n",
    "# Display the table\n",
    "final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "afad6542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comprehensive results saved to: comprehensive_prompt_variations_analysis.csv\n",
      "\n",
      "=== FINAL COMPREHENSIVE SUMMARY ===\n",
      "Total prompt variations analyzed: 11\n",
      "\n",
      "Best detection performance: P-10_Full_Plus_One_Shot (15.6% true positive rate)\n",
      "Most efficient (lowest tokens): P-1_Minimal (567 mean prompt tokens)\n",
      "Longest descriptions: P-10_Full_Plus_One_Shot (901 chars)\n",
      "Most words per description: P-10_Full_Plus_One_Shot (125.3 words)\n",
      "Highest AI detectability: P-9_Basic_One_Shot (23.2% mean AI score)\n",
      "\n",
      "=== KEY INSIGHTS ===\n",
      "1. Detection Performance:\n",
      "   - Average true positive rate across all prompts: 11.1%\n",
      "   - Best performing prompts: P-10_Full_Plus_One_Shot, P-11_Full_Plus_Few_Shot, P-7_Template_Plus_Title\n",
      "   - Average AI score for generated content: 14.7%\n",
      "   - Highest AI scores: P-9_Basic_One_Shot, P-7_Template_Plus_Title, P-10_Full_Plus_One_Shot\n",
      "\n",
      "2. Token Efficiency:\n",
      "   - Most token-efficient prompts: P-1_Minimal, P-2_Basic, P-7_Template_Plus_Title\n",
      "   - Average prompt tokens: 25671\n",
      "\n",
      "3. Content Quality:\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m3. Content Quality:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     36\u001b[39m original_row = final_results[final_results[\u001b[33m'\u001b[39m\u001b[33mprompt_variation\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33mOriginal\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   - Original descriptions average \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43moriginal_row\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmean_word_count\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.0f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m words, AI score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_row[\u001b[33m'\u001b[39m\u001b[33mmean_ai_score_original\u001b[39m\u001b[33m'\u001b[39m].iloc[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   - Generated descriptions range from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerated_only[\u001b[33m'\u001b[39m\u001b[33mmean_word_count\u001b[39m\u001b[33m'\u001b[39m].min()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.0f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerated_only[\u001b[33m'\u001b[39m\u001b[33mmean_word_count\u001b[39m\u001b[33m'\u001b[39m].max()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.0f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m words\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   - Generated AI scores range from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerated_only[\u001b[33m'\u001b[39m\u001b[33mmean_ai_score_generated\u001b[39m\u001b[33m'\u001b[39m].min()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m% to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerated_only[\u001b[33m'\u001b[39m\u001b[33mmean_ai_score_generated\u001b[39m\u001b[33m'\u001b[39m].max()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/shims/versions/3.12.9/lib/python3.12/site-packages/pandas/core/indexing.py:1191\u001b[39m, in \u001b[36m_LocationIndexer.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1189\u001b[39m maybe_callable = com.apply_if_callable(key, \u001b[38;5;28mself\u001b[39m.obj)\n\u001b[32m   1190\u001b[39m maybe_callable = \u001b[38;5;28mself\u001b[39m._check_deprecated_callable_usage(key, maybe_callable)\n\u001b[32m-> \u001b[39m\u001b[32m1191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/shims/versions/3.12.9/lib/python3.12/site-packages/pandas/core/indexing.py:1752\u001b[39m, in \u001b[36m_iLocIndexer._getitem_axis\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot index by location index with a non-integer key\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1751\u001b[39m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1752\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1754\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obj._ixs(key, axis=axis)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/shims/versions/3.12.9/lib/python3.12/site-packages/pandas/core/indexing.py:1685\u001b[39m, in \u001b[36m_iLocIndexer._validate_integer\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1683\u001b[39m len_axis = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.obj._get_axis(axis))\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key >= len_axis \u001b[38;5;129;01mor\u001b[39;00m key < -len_axis:\n\u001b[32m-> \u001b[39m\u001b[32m1685\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33msingle positional indexer is out-of-bounds\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mIndexError\u001b[39m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "# Save comprehensive results\n",
    "comprehensive_output_file = \"comprehensive_prompt_variations_analysis.csv\"\n",
    "final_results.to_csv(comprehensive_output_file, index=False)\n",
    "print(f\"\\nComprehensive results saved to: {comprehensive_output_file}\")\n",
    "\n",
    "# Final comprehensive summary\n",
    "print(\"\\n=== FINAL COMPREHENSIVE SUMMARY ===\")\n",
    "print(f\"Total prompt variations analyzed: {len(final_results)}\")\n",
    "\n",
    "# Best performers\n",
    "best_detection = final_results.loc[final_results['true_positive_pct'].idxmax()]\n",
    "most_efficient = final_results[final_results['prompt_variation'] != 'Original'].loc[final_results[final_results['prompt_variation'] != 'Original']['mean_prompt_tokens'].idxmin()]\n",
    "longest_content = final_results.loc[final_results['mean_description_length'].idxmax()]\n",
    "most_words = final_results.loc[final_results['mean_word_count'].idxmax()]\n",
    "highest_ai_score = final_results.loc[final_results['mean_ai_score_generated'].idxmax()]\n",
    "\n",
    "print(f\"\\nBest detection performance: {best_detection['prompt_variation']} ({best_detection['true_positive_pct']:.1f}% true positive rate)\")\n",
    "print(f\"Most efficient (lowest tokens): {most_efficient['prompt_variation']} ({most_efficient['mean_prompt_tokens']:.0f} mean prompt tokens)\")\n",
    "print(f\"Longest descriptions: {longest_content['prompt_variation']} ({longest_content['mean_description_length']:.0f} chars)\")\n",
    "print(f\"Most words per description: {most_words['prompt_variation']} ({most_words['mean_word_count']:.1f} words)\")\n",
    "print(f\"Highest AI detectability: {highest_ai_score['prompt_variation']} ({highest_ai_score['mean_ai_score_generated']:.1f}% mean AI score)\")\n",
    "\n",
    "print(\"\\n=== KEY INSIGHTS ===\")\n",
    "print(\"1. Detection Performance:\")\n",
    "generated_only = final_results[final_results['prompt_variation'] != 'Original']\n",
    "print(f\"   - Average true positive rate across all prompts: {generated_only['true_positive_pct'].mean():.1f}%\")\n",
    "print(f\"   - Best performing prompts: {', '.join(generated_only.nlargest(3, 'true_positive_pct')['prompt_variation'].tolist())}\")\n",
    "print(f\"   - Average AI score for generated content: {generated_only['mean_ai_score_generated'].mean():.1f}%\")\n",
    "print(f\"   - Highest AI scores: {', '.join(generated_only.nlargest(3, 'mean_ai_score_generated')['prompt_variation'].tolist())}\")\n",
    "\n",
    "print(\"\\n2. Token Efficiency:\")\n",
    "print(f\"   - Most token-efficient prompts: {', '.join(generated_only.nsmallest(3, 'mean_prompt_tokens')['prompt_variation'].tolist())}\")\n",
    "print(f\"   - Average prompt tokens: {generated_only['mean_prompt_tokens'].mean():.0f}\")\n",
    "\n",
    "print(\"\\n3. Content Quality:\")\n",
    "original_row = final_results[final_results['prompt_variation'] == 'Original']\n",
    "print(f\"   - Original descriptions average {original_row['mean_word_count'].iloc[0]:.0f} words, AI score: {original_row['mean_ai_score_original'].iloc[0]:.1f}%\")\n",
    "print(f\"   - Generated descriptions range from {generated_only['mean_word_count'].min():.0f} to {generated_only['mean_word_count'].max():.0f} words\")\n",
    "print(f\"   - Generated AI scores range from {generated_only['mean_ai_score_generated'].min():.1f}% to {generated_only['mean_ai_score_generated'].max():.1f}%\")\n",
    "print(f\"   - Average generated content length: {generated_only['mean_word_count'].mean():.0f} words\")\n",
    "\n",
    "print(\"\\n4. AI Detection Score Analysis:\")\n",
    "print(f\"   - Original (human) content: {original_row['mean_ai_score_original'].iloc[0]:.1f}% mean AI score (all below 50% threshold)\")\n",
    "print(f\"   - Generated content: {generated_only['mean_ai_score_generated'].mean():.1f}% average mean AI score\")\n",
    "print(f\"   - Detection gap: {generated_only['mean_ai_score_generated'].mean() - original_row['mean_ai_score_original'].iloc[0]:.1f} percentage points higher for AI content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2def9ce",
   "metadata": {},
   "source": [
    "## Debug: Understanding Original Detection Metrics\n",
    "\n",
    "Let's examine why the Original descriptions show 0.0 for both true_positive_pct and false_negative_pct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b80844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Let's examine the Original descriptions detection logic step by step\n",
    "print(\"=== DEBUGGING ORIGINAL DETECTION METRICS ===\\n\")\n",
    "\n",
    "# Get original data again\n",
    "original_data = merged_df[merged_df['entry_type'] == 'original']\n",
    "print(f\"1. Original data shape: {original_data.shape}\")\n",
    "print(f\"   Entry types: {original_data['entry_type'].value_counts()}\")\n",
    "\n",
    "# Check zerogpt responses for original data\n",
    "print(f\"\\n2. ZeroGPT response availability for Original data:\")\n",
    "print(f\"   Non-null responses: {original_data['zerogpt_response'].notna().sum()}\")\n",
    "print(f\"   Null/empty responses: {original_data['zerogpt_response'].isna().sum()}\")\n",
    "\n",
    "# Try parsing a few responses manually\n",
    "print(f\"\\n3. Sample AI probabilities for Original descriptions:\")\n",
    "sample_original = original_data.head(5)\n",
    "for idx, row in sample_original.iterrows():\n",
    "    ai_prob = parse_zerogpt_response(row['zerogpt_response'])\n",
    "    print(f\"   Sample {idx}: AI probability = {ai_prob}\")\n",
    "\n",
    "# Now let's run the detection metrics function step by step\n",
    "print(f\"\\n4. Running detection metrics calculation for Original:\")\n",
    "\n",
    "# Copy the function logic but with debug prints\n",
    "original_debug = original_data.copy()\n",
    "original_debug['ai_probability'] = original_debug['zerogpt_response'].apply(parse_zerogpt_response)\n",
    "\n",
    "print(f\"   AI probabilities parsed: {original_debug['ai_probability'].notna().sum()} out of {len(original_debug)}\")\n",
    "print(f\"   AI probability range: {original_debug['ai_probability'].min():.1f} to {original_debug['ai_probability'].max():.1f}\")\n",
    "\n",
    "valid_df = original_debug[original_debug['ai_probability'].notna()].copy()\n",
    "print(f\"   Valid samples for analysis: {len(valid_df)}\")\n",
    "\n",
    "if len(valid_df) > 0:\n",
    "    # Check detection threshold\n",
    "    ai_threshold = 50.0\n",
    "    valid_df['detected_as_ai'] = valid_df['ai_probability'] > ai_threshold\n",
    "    print(f\"   Using AI threshold: {ai_threshold}%\")\n",
    "    print(f\"   Detected as AI: {valid_df['detected_as_ai'].sum()} out of {len(valid_df)}\")\n",
    "    \n",
    "    # Break down by entry type\n",
    "    generated_entries = valid_df[valid_df['entry_type'] == 'generated']\n",
    "    original_entries = valid_df[valid_df['entry_type'] == 'original']\n",
    "    \n",
    "    print(f\"   Generated entries: {len(generated_entries)} (detected as AI: {generated_entries['detected_as_ai'].sum() if len(generated_entries) > 0 else 0})\")\n",
    "    print(f\"   Original entries: {len(original_entries)} (detected as AI: {original_entries['detected_as_ai'].sum() if len(original_entries) > 0 else 0})\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    tp_rate = 0\n",
    "    if len(generated_entries) > 0:\n",
    "        tp_rate = (generated_entries['detected_as_ai'].sum() / len(generated_entries)) * 100\n",
    "    \n",
    "    fp_rate = 0  \n",
    "    if len(original_entries) > 0:\n",
    "        fp_rate = (original_entries['detected_as_ai'].sum() / len(original_entries)) * 100\n",
    "    \n",
    "    print(f\"   True Positive Rate: {tp_rate:.1f}% (Generated correctly identified as AI)\")\n",
    "    print(f\"   False Positive Rate: {fp_rate:.1f}% (Original incorrectly identified as AI)\")\n",
    "    \n",
    "else:\n",
    "    print(\"   No valid samples to analyze!\")\n",
    "\n",
    "print(f\"\\n5. The issue explanation:\")\n",
    "print(f\"   - For 'Original' analysis, we only have original (human-written) descriptions\")\n",
    "print(f\"   - True Positive Rate = Generated content correctly identified as AI / Total Generated\")\n",
    "print(f\"   - But we have 0 generated entries in the 'Original' dataset\")\n",
    "print(f\"   - False Negative Rate = Original content incorrectly identified as AI / Total Original\")\n",
    "print(f\"   - This should show the misclassification rate of human content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503dc05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\\\n=== EXPLANATION OF ORIGINAL DETECTION METRICS ===\\\\n\")\n",
    "\n",
    "print(\"Why Original shows 0.0 for both metrics:\")\n",
    "print(\"\\\\n1. **True Positive Rate (0.0%)**:\")\n",
    "print(\"   - This measures: Generated content correctly identified as AI\")\n",
    "print(\"   - For 'Original' row: We have 0 generated entries (only human-written content)\")\n",
    "print(\"   - So: 0 generated entries detected as AI / 0 total generated entries = 0/0 = 0%\")\n",
    "print(\"   - This is correct but not meaningful for the Original baseline\")\n",
    "\n",
    "print(\"\\\\n2. **False Negative Rate (0.0%)**:\")  \n",
    "print(\"   - This measures: Original (human) content incorrectly identified as AI\")\n",
    "print(\"   - For 'Original' row: 0 out of 352 human descriptions were misclassified as AI\")\n",
    "print(\"   - So: 0 misclassified / 352 total human descriptions = 0%\")\n",
    "print(\"   - This means the detector correctly identified ALL original content as human-written!\")\n",
    "\n",
    "print(\"\\\\n=== CORRECTED INTERPRETATION ===\\\\n\")\n",
    "print(\"The Original row should be interpreted as:\")\n",
    "print(\"- True Positive Rate: N/A (no generated content to detect)\")\n",
    "print(\"- **Human Content Accuracy: 100%** (all 352 original descriptions correctly identified as human)\")\n",
    "print(\"- **AI Probability range: 0.0% to 48.4%** (all below 50% threshold)\")\n",
    "\n",
    "# Let's also check a few more statistics for Original\n",
    "original_debug = merged_df[merged_df['entry_type'] == 'original'].copy()\n",
    "original_debug['ai_probability'] = original_debug['zerogpt_response'].apply(parse_zerogpt_response)\n",
    "\n",
    "print(f\"\\\\n=== ORIGINAL CONTENT DETECTION STATISTICS ===\")\n",
    "print(f\"Total original descriptions analyzed: {len(original_debug)}\")\n",
    "print(f\"AI probability statistics:\")\n",
    "print(f\"  Mean: {original_debug['ai_probability'].mean():.1f}%\")\n",
    "print(f\"  Median: {original_debug['ai_probability'].median():.1f}%\")\n",
    "print(f\"  Std Dev: {original_debug['ai_probability'].std():.1f}%\")\n",
    "print(f\"  Max: {original_debug['ai_probability'].max():.1f}%\")\n",
    "print(f\"  Min: {original_debug['ai_probability'].min():.1f}%\")\n",
    "print(f\"\\\\nDistribution of AI probabilities:\")\n",
    "print(f\"  0-10%: {((original_debug['ai_probability'] >= 0) & (original_debug['ai_probability'] < 10)).sum()} descriptions\")\n",
    "print(f\"  10-20%: {((original_debug['ai_probability'] >= 10) & (original_debug['ai_probability'] < 20)).sum()} descriptions\")\n",
    "print(f\"  20-30%: {((original_debug['ai_probability'] >= 20) & (original_debug['ai_probability'] < 30)).sum()} descriptions\")\n",
    "print(f\"  30-40%: {((original_debug['ai_probability'] >= 30) & (original_debug['ai_probability'] < 40)).sum()} descriptions\")\n",
    "print(f\"  40-50%: {((original_debug['ai_probability'] >= 40) & (original_debug['ai_probability'] < 50)).sum()} descriptions\")\n",
    "print(f\"  Above 50%: {(original_debug['ai_probability'] >= 50).sum()} descriptions (would be misclassified)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3578682a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERIFY: Let's confirm the ZeroGPT responses for original entries are being used\n",
    "print(\"=== VERIFICATION: ZeroGPT RESPONSES FOR ORIGINAL ENTRIES ===\\n\")\n",
    "\n",
    "# Check the detection CSV file directly\n",
    "print(\"1. Checking detection CSV structure:\")\n",
    "print(f\"   Total entries in detection file: {len(detection_df)}\")\n",
    "print(f\"   Entry types: {detection_df['entry_type'].value_counts()}\")\n",
    "\n",
    "# Check original entries specifically\n",
    "original_detection_entries = detection_df[detection_df['entry_type'] == 'original']\n",
    "print(f\"\\n2. Original entries in detection file:\")\n",
    "print(f\"   Count: {len(original_detection_entries)}\")\n",
    "print(f\"   Non-null zerogpt_response: {original_detection_entries['zerogpt_response'].notna().sum()}\")\n",
    "print(f\"   Sample ZeroGPT responses for original entries:\")\n",
    "\n",
    "# Parse a few original entry responses\n",
    "for i, (idx, row) in enumerate(original_detection_entries.head(3).iterrows()):\n",
    "    response_sample = row['zerogpt_response'][:100] if isinstance(row['zerogpt_response'], str) else str(row['zerogpt_response'])\n",
    "    ai_prob = parse_zerogpt_response(row['zerogpt_response'])\n",
    "    print(f\"     Sample {i+1}: AI probability = {ai_prob}% | Response: {response_sample}...\")\n",
    "\n",
    "# Now let's check what happens when we merge with generation data\n",
    "print(f\"\\n3. After merging with generation data:\")\n",
    "original_merged = merged_df[merged_df['entry_type'] == 'original']\n",
    "print(f\"   Count: {len(original_merged)}\")\n",
    "print(f\"   AI probabilities calculated:\")\n",
    "original_merged_copy = original_merged.copy()\n",
    "original_merged_copy['ai_probability'] = original_merged_copy['zerogpt_response'].apply(parse_zerogpt_response)\n",
    "print(f\"     Mean: {original_merged_copy['ai_probability'].mean():.1f}%\")\n",
    "print(f\"     Max: {original_merged_copy['ai_probability'].max():.1f}%\")\n",
    "print(f\"     Above 50% threshold: {(original_merged_copy['ai_probability'] > 50).sum()}\")\n",
    "\n",
    "print(f\"\\n4. IMPORTANT INSIGHT:\")\n",
    "if (original_merged_copy['ai_probability'] > 50).sum() == 0:\n",
    "    print(f\"   ✅ The false_negative_pct is CORRECTLY 0.0% because:\")\n",
    "    print(f\"   ✅ ALL {len(original_merged_copy)} original descriptions scored below 50% AI probability\")\n",
    "    print(f\"   ✅ This means ZeroGPT correctly identified ALL original content as human-written\")\n",
    "    print(f\"   ✅ No original content was misclassified as AI-generated\")\n",
    "else:\n",
    "    misclassified = (original_merged_copy['ai_probability'] > 50).sum()\n",
    "    total = len(original_merged_copy)\n",
    "    print(f\"   ❌ Something's wrong: {misclassified} out of {total} original entries scored > 50%\")\n",
    "    print(f\"   ❌ False negative rate should be: {(misclassified/total)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce8b2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== FINAL CONFIRMATION OF DETECTION CALCULATION ===\\n\")\n",
    "\n",
    "# Let's manually reproduce the exact calculation from our function\n",
    "original_for_verification = merged_df[merged_df['entry_type'] == 'original'].copy()\n",
    "original_for_verification['ai_probability'] = original_for_verification['zerogpt_response'].apply(parse_zerogpt_response)\n",
    "\n",
    "print(\"Manual calculation verification:\")\n",
    "print(f\"1. Total original entries: {len(original_for_verification)}\")\n",
    "print(f\"2. AI probabilities successfully parsed: {original_for_verification['ai_probability'].notna().sum()}\")\n",
    "print(f\"3. AI threshold used: 50.0%\")\n",
    "print(f\"4. Entries above threshold (detected as AI): {(original_for_verification['ai_probability'] > 50.0).sum()}\")\n",
    "print(f\"5. False Negative Rate calculation:\")\n",
    "print(f\"   = Entries incorrectly identified as AI / Total original entries\")\n",
    "print(f\"   = {(original_for_verification['ai_probability'] > 50.0).sum()} / {len(original_for_verification)}\")\n",
    "print(f\"   = {((original_for_verification['ai_probability'] > 50.0).sum() / len(original_for_verification)) * 100:.1f}%\")\n",
    "\n",
    "print(f\"\\n6. Distribution of AI probabilities for original entries:\")\n",
    "ai_probs = original_for_verification['ai_probability']\n",
    "print(f\"   Min: {ai_probs.min():.1f}%\")\n",
    "print(f\"   25th percentile: {ai_probs.quantile(0.25):.1f}%\") \n",
    "print(f\"   Median: {ai_probs.median():.1f}%\")\n",
    "print(f\"   75th percentile: {ai_probs.quantile(0.75):.1f}%\")\n",
    "print(f\"   Max: {ai_probs.max():.1f}%\")\n",
    "\n",
    "print(f\"\\n✅ CONFIRMATION: The ZeroGPT responses for original entries ARE being used!\")\n",
    "print(f\"✅ The 0.0% false negative rate is CORRECT - it means perfect accuracy!\")\n",
    "print(f\"✅ ZeroGPT successfully identified all original content as human-written.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245e2a8e",
   "metadata": {},
   "source": [
    "## Side-by-Side Description Comparison\n",
    "\n",
    "Let's create a side-by-side comparison of descriptions from all prompt variations plus the original for a single PR. This will be useful for presentation slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c0493b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SIDE-BY-SIDE DESCRIPTION COMPARISON ===\n",
      "\n",
      "PR coverage across prompt variations:\n",
      "Max variations per PR: 11\n",
      "PRs with most variations: pr_id\n",
      "MDExOlB1bGxSZXF1ZXN0MTYwNDk0NTcy    11\n",
      "MDExOlB1bGxSZXF1ZXN0MTYxNDI3MDY3    11\n",
      "PR_kwDOAQ0TF86g3kgZ                 11\n",
      "PR_kwDOAQ0TF86eA-bB                 11\n",
      "PR_kwDOAQ0TF86awU2h                 11\n",
      "Name: prompt_variation, dtype: int64\n",
      "\n",
      "Using PR ID: MDExOlB1bGxSZXF1ZXN0MTYxNDI3MDY3\n",
      "\n",
      "Found 11 descriptions for PR MDExOlB1bGxSZXF1ZXN0MTYxNDI3MDY3\n",
      "Entry types: ['generated']\n",
      "Prompt variations: ['P-10_Full_Plus_One_Shot', 'P-11_Full_Plus_Few_Shot', 'P-1_Minimal', 'P-2_Basic', 'P-3_Diffs_Only', 'P-4_Diffs_Plus_Title', 'P-5_Code_Only', 'P-6_Issue_Only', 'P-7_Template_Plus_Title', 'P-8_Full_Context', 'P-9_Basic_One_Shot']\n",
      "\n",
      "=== SIDE-BY-SIDE COMPARISON FOR PR MDExOlB1bGxSZXF1ZXN0MTYxNDI3MDY3 ===\n",
      "Variation | AI Score | Words | Description Preview\n",
      "========================================================================================================================\n",
      "P-10 Full Plus One Shot   |    0.0% |   134 | This pull request implements issue #3145 by improving the display of validation messages in the entry editor. It introduces a small notification icon in the entry editor to indicate data inconsistencies or problems, replacing the previous shifted positioning of messages. A new IconValidationDecorator is added to show validation icons with tooltips, enhancing user feedback during data entry. The entry editor preferences tab is extended with a new localization entry for 'Show validation messages' ...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "P-11 Full Plus Few Shot   |   36.7% |   140 | This PR implements issue #3145 by enhancing the display of validation messages in the entry editor. It introduces a new IconValidationDecorator that displays validation messages using font-based icons with tooltips positioned at the bottom-left of the field. When data inconsistencies or other problems are detected, a small notification icon appears in the entry editor. This feature can be enabled or disabled via the preferences in the Entry Editor tab, with the default setting changed from disab...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "P-1 Minimal               |    0.0% |    71 | Enhance the entry editor with validation icons and improved UI styling by adding IconValidationDecorator for validation feedback. Update EntryEditorPrefsTab to include related preferences, refine IconTheme and EditorValidator, and adjust styles in EntryEditor.css for a better user experience. Perform minor build and changelog updates. Additionally, refactor JabRefPreferences.java with minor code cleanup to improve code quality. Update the English localization properties file with two new and rev...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "P-2 Basic                 |    0.0% |   109 | This pull request implements issue #3145 by adding validation messages directly in the entry editor of JabRef. It introduces a new IconValidationDecorator class to visually indicate validation status with icons and updates the entry editor UI with new CSS styles to support validation message display. The EntryEditorPrefsTab and JabRefPreferences classes have been modified to include preferences related to validation display, ensuring users receive immediate feedback on entry validation within th...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "P-3 Diffs Only            |    0.0% |   136 | This update introduces a new validation feature in the entry editor that displays icons indicating data inconsistencies or problems. A new IconValidationDecorator class is added to show warning and error icons with tooltips, enhancing user awareness of entry issues. The entry editor preferences tab now includes an option, localized as 'Show validation messages', to enable or disable these validation messages. The default setting for entry validation has been changed from disabled to enabled in J...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "P-4 Diffs Plus Title      |    0.0% |   144 | This pull request implements issue #3145 by adding validation message display directly within the entry editor. A new IconValidationDecorator class is introduced to show validation icons (error, warning, and success) with tooltips inside the entry editor fields. The entry editor preferences tab is updated to include an option to enable or disable showing validation messages, with the default preference changed to true in the JabRefPreferences configuration. The validation icons use new styles de...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "P-5 Code Only             |    0.0% |   179 | This pull request implements issue #3145 by adding validation message support in the entry editor. A small notification icon now appears in the entry editor when data inconsistencies or problems are detected. The validation messages use font-based icons with distinct styles for errors and warnings, including tooltips with relevant messages. A new IconValidationDecorator class is introduced to provide this customized validation decoration. Additionally, a new preference option 'Show validation me...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "P-6 Issue Only            |    4.5% |   106 | This pull request fixes issue #3145 by correcting the positioning of validation messages in the entry editor, which were previously displayed incorrectly shifted downward. It introduces a new IconValidationDecorator class to properly display validation icons and messages adjacent to the relevant fields. The EntryEditor.css file is updated to style the validation messages correctly. Changes are also made to EntryEditorPrefsTab and EditorValidator to support the improved validation display, includ...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "P-7 Template Plus Title   |    0.0% |    89 | This pull request implements issue #3145 by adding validation messages directly in the entry editor of JabRef. It introduces a new IconValidationDecorator class to visually indicate validation status with icons, enhancing user feedback by displaying validation results inline while editing entries. The entry editor UI and preferences tab have been updated to support displaying validation messages and configuring related settings. Additionally, updates were made to the JabRefPreferences class, Eng...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "P-8 Full Context          |   10.7% |   134 | This pull request addresses issue #3145 by implementing proper display of validation messages in the entry editor. It introduces a new IconValidationDecorator that shows validation messages as small notification icons with tooltips, positioned correctly to avoid the previous issue where messages were shifted to the bottom. The entry editor preferences tab now includes a checkbox to enable or disable showing validation messages, with the default preference set to true, allowing users to configure...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "P-9 Basic One Shot        |    0.0% |   121 | This pull request implements issue #3145 by adding validation messages directly in the entry editor, enhancing user experience with immediate feedback on entry validation. Key changes include the introduction of the IconValidationDecorator class to visually indicate validation status with icons, updates to EntryEditorPrefsTab to manage preferences for validation display, and modifications to EditorValidator to handle validation messages. The GUI is enhanced with new CSS styles for entry editor v...\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create side-by-side comparison for a single PR\n",
    "print(\"=== SIDE-BY-SIDE DESCRIPTION COMPARISON ===\\n\")\n",
    "\n",
    "# Let's pick a PR that has descriptions for all prompt variations\n",
    "# First, find a PR ID that appears across all variations\n",
    "pr_counts = merged_df.groupby('pr_id')['prompt_variation'].nunique().sort_values(ascending=False)\n",
    "print(f\"PR coverage across prompt variations:\")\n",
    "print(f\"Max variations per PR: {pr_counts.max()}\")\n",
    "print(f\"PRs with most variations: {pr_counts.head()}\")\n",
    "\n",
    "# Pick the first PR with good coverage\n",
    "sample_pr_id = pr_counts.index[1]\n",
    "print(f\"\\nUsing PR ID: {sample_pr_id}\")\n",
    "\n",
    "# Get all descriptions for this PR\n",
    "sample_descriptions = merged_df[merged_df['pr_id'] == sample_pr_id].copy()\n",
    "\n",
    "# Sort by entry_type and prompt_variation for consistent ordering\n",
    "sample_descriptions = sample_descriptions.sort_values(['entry_type', 'prompt_variation'])\n",
    "\n",
    "print(f\"\\nFound {len(sample_descriptions)} descriptions for PR {sample_pr_id}\")\n",
    "print(f\"Entry types: {sample_descriptions['entry_type'].unique()}\")\n",
    "print(f\"Prompt variations: {sorted(sample_descriptions['prompt_variation'].unique())}\")\n",
    "\n",
    "# Create the comparison\n",
    "comparison_data = []\n",
    "\n",
    "# First add the original description\n",
    "original_desc = sample_descriptions[sample_descriptions['entry_type'] == 'original']\n",
    "if len(original_desc) > 0:\n",
    "    comparison_data.append({\n",
    "        'Variation': 'Original (Human)',\n",
    "        'Description': original_desc['input_text'].iloc[0][:500] + \"...\" if len(original_desc['input_text'].iloc[0]) > 500 else original_desc['input_text'].iloc[0],\n",
    "        'Full_Description': original_desc['input_text'].iloc[0],\n",
    "        'AI_Score': f\"{parse_zerogpt_response(original_desc['zerogpt_response'].iloc[0]):.1f}%\",\n",
    "        'Word_Count': len(original_desc['input_text'].iloc[0].split())\n",
    "    })\n",
    "\n",
    "# Then add all generated descriptions\n",
    "generated_descs = sample_descriptions[sample_descriptions['entry_type'] == 'generated']\n",
    "for _, row in generated_descs.iterrows():\n",
    "    desc = row['input_text']\n",
    "    comparison_data.append({\n",
    "        'Variation': row['prompt_variation'].replace('_', ' '),\n",
    "        'Description': desc[:500] + \"...\" if len(desc) > 500 else desc,\n",
    "        'Full_Description': desc,\n",
    "        'AI_Score': f\"{parse_zerogpt_response(row['zerogpt_response']):.1f}%\",\n",
    "        'Word_Count': len(desc.split())\n",
    "    })\n",
    "\n",
    "# Create DataFrame for easy viewing\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(f\"\\n=== SIDE-BY-SIDE COMPARISON FOR PR {sample_pr_id} ===\")\n",
    "print(f\"Variation | AI Score | Words | Description Preview\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "for _, row in comparison_df.iterrows():\n",
    "    print(f\"{row['Variation']:<25} | {row['AI_Score']:>7} | {row['Word_Count']:>5} | {row['Description']}\")\n",
    "    print(\"-\" * 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bf72d3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== FULL DESCRIPTIONS FOR PRESENTATION SLIDE ===\n",
      "PR ID: MDExOlB1bGxSZXF1ZXN0MTYxNDI3MDY3\n",
      "====================================================================================================\n",
      "\n",
      "📝 **P-10 Full Plus One Shot** (AI Score: 0.0%, Words: 134)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "This pull request implements issue #3145 by improving the display of validation messages in the entry editor. It introduces a small notification icon in the entry editor to indicate data inconsistencies or problems, replacing the previous shifted positioning of messages. A new IconValidationDecorator is added to show validation icons with tooltips, enhancing user feedback during data entry. The entry editor preferences tab is extended with a new localization entry for 'Show validation messages' to allow users to toggle the validation message display, with the default preference changed from false to true to ensure validation messages are displayed by default. Additionally, related styling and icon resources are added or adjusted to support the new validation visuals. These changes have been manually tested and integrate seamlessly with the existing entry editor UI, improving clarity and usability.\n",
      "\n",
      "\n",
      "📝 **P-11 Full Plus Few Shot** (AI Score: 36.7%, Words: 140)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "This PR implements issue #3145 by enhancing the display of validation messages in the entry editor. It introduces a new IconValidationDecorator that displays validation messages using font-based icons with tooltips positioned at the bottom-left of the field. When data inconsistencies or other problems are detected, a small notification icon appears in the entry editor. This feature can be enabled or disabled via the preferences in the Entry Editor tab, with the default setting changed from disabled to enabled (VALIDATE_IN_ENTRY_EDITOR preference default set to true) to ensure validation messages are shown by default. Additional UI styling for warning and error icons and tooltips has been added, along with updates to dependencies and minor fixes related to icon theming and validation visualization. Furthermore, a new localization entry 'Show validation messages' has been added to the English properties file to support this feature.\n",
      "\n",
      "\n",
      "📝 **P-1 Minimal** (AI Score: 0.0%, Words: 71)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Enhance the entry editor with validation icons and improved UI styling by adding IconValidationDecorator for validation feedback. Update EntryEditorPrefsTab to include related preferences, refine IconTheme and EditorValidator, and adjust styles in EntryEditor.css for a better user experience. Perform minor build and changelog updates. Additionally, refactor JabRefPreferences.java with minor code cleanup to improve code quality. Update the English localization properties file with two new and revised entries to improve UI text accuracy.\n",
      "\n",
      "\n",
      "📝 **P-2 Basic** (AI Score: 0.0%, Words: 109)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "This pull request implements issue #3145 by adding validation messages directly in the entry editor of JabRef. It introduces a new IconValidationDecorator class to visually indicate validation status with icons and updates the entry editor UI with new CSS styles to support validation message display. The EntryEditorPrefsTab and JabRefPreferences classes have been modified to include preferences related to validation display, ensuring users receive immediate feedback on entry validation within the editor interface. Minor updates were made to IconTheme and EditorValidator to support the new validation features. Additionally, the English localization properties file was updated to support the new validation messages. The build configuration and changelog were also updated accordingly.\n",
      "\n",
      "\n",
      "📝 **P-3 Diffs Only** (AI Score: 0.0%, Words: 136)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "This update introduces a new validation feature in the entry editor that displays icons indicating data inconsistencies or problems. A new IconValidationDecorator class is added to show warning and error icons with tooltips, enhancing user awareness of entry issues. The entry editor preferences tab now includes an option, localized as 'Show validation messages', to enable or disable these validation messages. The default setting for entry validation has been changed from disabled to enabled in JabRefPreferences to activate this feature by default. The IconTheme enum is extended with new icons representing success, warning, and error states, accompanied by corresponding CSS styles for the icons and tooltips. The validation feature is integrated into the EditorValidator, replacing the default decoration with the new icon-based decorator. The changelog has been updated to reflect the addition of this notification icon feature.\n",
      "\n",
      "\n",
      "📝 **P-4 Diffs Plus Title** (AI Score: 0.0%, Words: 144)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "This pull request implements issue #3145 by adding validation message display directly within the entry editor. A new IconValidationDecorator class is introduced to show validation icons (error, warning, and success) with tooltips inside the entry editor fields. The entry editor preferences tab is updated to include an option to enable or disable showing validation messages, with the default preference changed to true in the JabRefPreferences configuration. The validation icons use new styles defined in the EntryEditor.css file, and the IconTheme enum is extended with new icons for success, warning, and error states. The ControlsFxVisualizer in EditorValidator is configured to use the new IconValidationDecorator when validation is enabled in preferences. Additionally, a new localization entry for 'Show validation messages' is added in the English properties file to support this feature. The changelog is updated to reflect the addition of validation notifications in the entry editor.\n",
      "\n",
      "\n",
      "📝 **P-5 Code Only** (AI Score: 0.0%, Words: 179)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "This pull request implements issue #3145 by adding validation message support in the entry editor. A small notification icon now appears in the entry editor when data inconsistencies or problems are detected. The validation messages use font-based icons with distinct styles for errors and warnings, including tooltips with relevant messages. A new IconValidationDecorator class is introduced to provide this customized validation decoration. Additionally, a new preference option 'Show validation messages' is added to the Entry Editor preferences tab, allowing users to enable or disable the validation feature. The default preference for validation messages in the entry editor is changed from disabled to enabled by setting VALIDATE_IN_ENTRY_EDITOR to true in JabRefPreferences. The EditorValidator class is updated to use the IconValidationDecorator for visualizing validation status. Relevant styles for warning and error icons and tooltips are added in the entry editor CSS. A new localization key 'Show validation messages' is added to the English properties file to support displaying validation messages in the entry editor. The changelog is updated to reflect the addition of the validation notification icon in the entry editor.\n",
      "\n",
      "\n",
      "📝 **P-6 Issue Only** (AI Score: 4.5%, Words: 106)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "This pull request fixes issue #3145 by correcting the positioning of validation messages in the entry editor, which were previously displayed incorrectly shifted downward. It introduces a new IconValidationDecorator class to properly display validation icons and messages adjacent to the relevant fields. The EntryEditor.css file is updated to style the validation messages correctly. Changes are also made to EntryEditorPrefsTab and EditorValidator to support the improved validation display, including adjustments to related preference settings. The localization resource file is updated to ensure proper display of validation messages. Minor updates to the IconTheme class and build configuration accommodate these changes. The changelog is updated to reflect this enhancement.\n",
      "\n",
      "\n",
      "📝 **P-7 Template Plus Title** (AI Score: 0.0%, Words: 89)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "This pull request implements issue #3145 by adding validation messages directly in the entry editor of JabRef. It introduces a new IconValidationDecorator class to visually indicate validation status with icons, enhancing user feedback by displaying validation results inline while editing entries. The entry editor UI and preferences tab have been updated to support displaying validation messages and configuring related settings. Additionally, updates were made to the JabRefPreferences class, English localization properties, CSS styling, icon themes, and build configuration to fully support this feature. The changelog has been updated accordingly.\n",
      "\n",
      "\n",
      "📝 **P-8 Full Context** (AI Score: 10.7%, Words: 134)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "This pull request addresses issue #3145 by implementing proper display of validation messages in the entry editor. It introduces a new IconValidationDecorator that shows validation messages as small notification icons with tooltips, positioned correctly to avoid the previous issue where messages were shifted to the bottom. The entry editor preferences tab now includes a checkbox to enable or disable showing validation messages, with the default preference set to true, allowing users to configure validation message display. Relevant styles for warning and error icons and tooltips have been added to the entry editor CSS, and the IconTheme has been updated to include icons for success, warning, and error states used by the validation decorator. Additionally, a new localization entry has been added to support this feature. The changelog has been updated to reflect these changes.\n",
      "\n",
      "\n",
      "📝 **P-9 Basic One Shot** (AI Score: 0.0%, Words: 121)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "This pull request implements issue #3145 by adding validation messages directly in the entry editor, enhancing user experience with immediate feedback on entry validation. Key changes include the introduction of the IconValidationDecorator class to visually indicate validation status with icons, updates to EntryEditorPrefsTab to manage preferences for validation display, and modifications to EditorValidator to handle validation messages. The GUI is enhanced with new CSS styles for entry editor validation messages, and IconTheme is updated to support new validation icons. Additionally, JabRef preferences are modified to enable displaying validation messages within the entry editor, and two new entries are added to the English localization properties file to support localized validation messages. Build configuration and changelog are also updated to reflect these changes.\n",
      "\n",
      "💾 Slide content saved to: side_by_side_comparison_PR_MDExOlB1bGxSZXF1ZXN0MTYxNDI3MDY3.md\n",
      "\n",
      "📊 **SUMMARY TABLE FOR SLIDE**\n",
      "       Prompt Variation AI Detection Score  Word Count\n",
      "P-10 Full Plus One Shot               0.0%         134\n",
      "P-11 Full Plus Few Shot              36.7%         140\n",
      "            P-1 Minimal               0.0%          71\n",
      "              P-2 Basic               0.0%         109\n",
      "         P-3 Diffs Only               0.0%         136\n",
      "   P-4 Diffs Plus Title               0.0%         144\n",
      "          P-5 Code Only               0.0%         179\n",
      "         P-6 Issue Only               4.5%         106\n",
      "P-7 Template Plus Title               0.0%          89\n",
      "       P-8 Full Context              10.7%         134\n",
      "     P-9 Basic One Shot               0.0%         121\n"
     ]
    }
   ],
   "source": [
    "# Create a presentation-friendly format with full descriptions\n",
    "print(f\"\\n\\n=== FULL DESCRIPTIONS FOR PRESENTATION SLIDE ===\")\n",
    "print(f\"PR ID: {sample_pr_id}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for i, row in comparison_df.iterrows():\n",
    "    print(f\"\\n📝 **{row['Variation']}** (AI Score: {row['AI_Score']}, Words: {row['Word_Count']})\")\n",
    "    print(\"─\" * 80)\n",
    "    print(row['Full_Description'])\n",
    "    print()\n",
    "\n",
    "# Also save to a text file for easy copying\n",
    "slide_content = f\"Side-by-Side PR Description Comparison - PR {sample_pr_id}\\n\"\n",
    "slide_content += \"=\" * 60 + \"\\n\\n\"\n",
    "\n",
    "for _, row in comparison_df.iterrows():\n",
    "    slide_content += f\"{row['Variation']} (AI Score: {row['AI_Score']}, Words: {row['Word_Count']})\\n\"\n",
    "    slide_content += \"─\" * 50 + \"\\n\"\n",
    "    slide_content += f\"{row['Full_Description']}\\n\\n\"\n",
    "\n",
    "# Save to file\n",
    "slide_file = f\"side_by_side_comparison_PR_{sample_pr_id}.md\"\n",
    "with open(slide_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(slide_content)\n",
    "\n",
    "print(f\"💾 Slide content saved to: {slide_file}\")\n",
    "\n",
    "# Create a summary table for the slide\n",
    "print(f\"\\n📊 **SUMMARY TABLE FOR SLIDE**\")\n",
    "summary_table = comparison_df[['Variation', 'AI_Score', 'Word_Count']].copy()\n",
    "summary_table.columns = ['Prompt Variation', 'AI Detection Score', 'Word Count']\n",
    "print(summary_table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3807f4b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
