{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d62644f",
   "metadata": {},
   "source": [
    "# Prompt Variations Analysis\n",
    "\n",
    "This notebook analyzes the performance of different prompt variations for generating PR descriptions and their detectability by AI detection tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "73a88c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103b12ab",
   "metadata": {},
   "source": [
    "## Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "65e47b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading detection results...\n",
      "Detection data shape: (704, 6)\n",
      "Detection data columns: ['pr_id', 'prompt_variation', 'entry_key', 'entry_type', 'input_text', 'zerogpt_response']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pr_id</th>\n",
       "      <th>prompt_variation</th>\n",
       "      <th>entry_key</th>\n",
       "      <th>entry_type</th>\n",
       "      <th>input_text</th>\n",
       "      <th>zerogpt_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PR_kwDOAQ0TF85oN6RH</td>\n",
       "      <td>P-7_Template_Plus_Title</td>\n",
       "      <td>PR_kwDOAQ0TF85oN6RH_P-7_Template_Plus_Title_or...</td>\n",
       "      <td>original</td>\n",
       "      <td>&lt;!-- \\nDescribe the changes you have made here...</td>\n",
       "      <td>{\"success\": true, \"code\": 200, \"message\": \"det...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PR_kwDOAQ0TF85oN6RH</td>\n",
       "      <td>P-7_Template_Plus_Title</td>\n",
       "      <td>PR_kwDOAQ0TF85oN6RH_P-7_Template_Plus_Title_ge...</td>\n",
       "      <td>generated</td>\n",
       "      <td>Fixed a modularity issue with the HTML convert...</td>\n",
       "      <td>{\"success\": true, \"code\": 200, \"message\": \"det...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2</td>\n",
       "      <td>P-7_Template_Plus_Title</td>\n",
       "      <td>MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2_P-7_Template_...</td>\n",
       "      <td>original</td>\n",
       "      <td>Add zbmath to the public databases which can b...</td>\n",
       "      <td>{\"success\": true, \"code\": 200, \"message\": \"det...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2</td>\n",
       "      <td>P-7_Template_Plus_Title</td>\n",
       "      <td>MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2_P-7_Template_...</td>\n",
       "      <td>generated</td>\n",
       "      <td>Added a new fetcher for ZbMATH to JabRef to en...</td>\n",
       "      <td>{\"success\": true, \"code\": 200, \"message\": \"det...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PR_kwDOAQ0TF86DGkyK</td>\n",
       "      <td>P-7_Template_Plus_Title</td>\n",
       "      <td>PR_kwDOAQ0TF86DGkyK_P-7_Template_Plus_Title_or...</td>\n",
       "      <td>original</td>\n",
       "      <td>Fixes https://github.com/JabRef/jabref/issues/...</td>\n",
       "      <td>{\"success\": true, \"code\": 200, \"message\": \"det...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              pr_id         prompt_variation  \\\n",
       "0               PR_kwDOAQ0TF85oN6RH  P-7_Template_Plus_Title   \n",
       "1               PR_kwDOAQ0TF85oN6RH  P-7_Template_Plus_Title   \n",
       "2  MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2  P-7_Template_Plus_Title   \n",
       "3  MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2  P-7_Template_Plus_Title   \n",
       "4               PR_kwDOAQ0TF86DGkyK  P-7_Template_Plus_Title   \n",
       "\n",
       "                                           entry_key entry_type  \\\n",
       "0  PR_kwDOAQ0TF85oN6RH_P-7_Template_Plus_Title_or...   original   \n",
       "1  PR_kwDOAQ0TF85oN6RH_P-7_Template_Plus_Title_ge...  generated   \n",
       "2  MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2_P-7_Template_...   original   \n",
       "3  MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2_P-7_Template_...  generated   \n",
       "4  PR_kwDOAQ0TF86DGkyK_P-7_Template_Plus_Title_or...   original   \n",
       "\n",
       "                                          input_text  \\\n",
       "0  <!-- \\nDescribe the changes you have made here...   \n",
       "1  Fixed a modularity issue with the HTML convert...   \n",
       "2  Add zbmath to the public databases which can b...   \n",
       "3  Added a new fetcher for ZbMATH to JabRef to en...   \n",
       "4  Fixes https://github.com/JabRef/jabref/issues/...   \n",
       "\n",
       "                                    zerogpt_response  \n",
       "0  {\"success\": true, \"code\": 200, \"message\": \"det...  \n",
       "1  {\"success\": true, \"code\": 200, \"message\": \"det...  \n",
       "2  {\"success\": true, \"code\": 200, \"message\": \"det...  \n",
       "3  {\"success\": true, \"code\": 200, \"message\": \"det...  \n",
       "4  {\"success\": true, \"code\": 200, \"message\": \"det...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define paths\n",
    "generation_path = \"../generation/datasets/\"\n",
    "detection_path = \"../detection/datasets/prompt_variations/prompt_variations-detection.csv\"\n",
    "\n",
    "# Load detection results\n",
    "print(\"Loading detection results...\")\n",
    "detection_df = pd.read_csv(detection_path)\n",
    "print(f\"Detection data shape: {detection_df.shape}\")\n",
    "print(f\"Detection data columns: {detection_df.columns.tolist()}\")\n",
    "detection_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3caa1d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 prompt variation files:\n",
      "  prompt_variation_P-10_Full_Plus_One_Shot_generated.csv\n",
      "  prompt_variation_P-11_Full_Plus_Few_Shot_generated.csv\n",
      "  prompt_variation_P-1_Minimal_generated.csv\n",
      "  prompt_variation_P-2_Basic_generated.csv\n",
      "  prompt_variation_P-3_Diffs_Only_generated.csv\n",
      "  prompt_variation_P-4_Diffs_Plus_Title_generated.csv\n",
      "  prompt_variation_P-5_Code_Only_generated.csv\n",
      "  prompt_variation_P-6_Issue_Only_generated.csv\n",
      "  prompt_variation_P-7_Template_Plus_Title_generated.csv\n",
      "  prompt_variation_P-8_Full_Context_generated.csv\n",
      "  prompt_variation_P-9_Basic_One_Shot_generated.csv\n"
     ]
    }
   ],
   "source": [
    "# Find all prompt variation CSV files\n",
    "prompt_variation_files = glob.glob(os.path.join(generation_path, \"prompt_variation_P-*_generated.csv\"))\n",
    "prompt_variation_files.sort()\n",
    "\n",
    "print(f\"Found {len(prompt_variation_files)} prompt variation files:\")\n",
    "for file in prompt_variation_files:\n",
    "    print(f\"  {os.path.basename(file)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "13fa3138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading prompt variation data...\n",
      "Loading prompt_variation_P-10_Full_Plus_One_Shot_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-10_Full_Plus_One_Shot\n",
      "Loading prompt_variation_P-11_Full_Plus_Few_Shot_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-11_Full_Plus_Few_Shot\n",
      "Loading prompt_variation_P-1_Minimal_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-1_Minimal\n",
      "Loading prompt_variation_P-2_Basic_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-11_Full_Plus_Few_Shot\n",
      "Loading prompt_variation_P-1_Minimal_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-1_Minimal\n",
      "Loading prompt_variation_P-2_Basic_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-2_Basic\n",
      "Loading prompt_variation_P-3_Diffs_Only_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-3_Diffs_Only\n",
      "Loading prompt_variation_P-4_Diffs_Plus_Title_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-2_Basic\n",
      "Loading prompt_variation_P-3_Diffs_Only_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-3_Diffs_Only\n",
      "Loading prompt_variation_P-4_Diffs_Plus_Title_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-4_Diffs_Plus_Title\n",
      "Loading prompt_variation_P-5_Code_Only_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-5_Code_Only\n",
      "Loading prompt_variation_P-6_Issue_Only_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-4_Diffs_Plus_Title\n",
      "Loading prompt_variation_P-5_Code_Only_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-5_Code_Only\n",
      "Loading prompt_variation_P-6_Issue_Only_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-6_Issue_Only\n",
      "Loading prompt_variation_P-7_Template_Plus_Title_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-7_Template_Plus_Title\n",
      "Loading prompt_variation_P-8_Full_Context_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-6_Issue_Only\n",
      "Loading prompt_variation_P-7_Template_Plus_Title_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-7_Template_Plus_Title\n",
      "Loading prompt_variation_P-8_Full_Context_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-8_Full_Context\n",
      "Loading prompt_variation_P-9_Basic_One_Shot_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-9_Basic_One_Shot\n",
      "\n",
      "Combined generation data shape: (2640, 26)\n",
      "Prompt variations found: ['P-10_Full_Plus_One_Shot', 'P-11_Full_Plus_Few_Shot', 'P-1_Minimal', 'P-2_Basic', 'P-3_Diffs_Only', 'P-4_Diffs_Plus_Title', 'P-5_Code_Only', 'P-6_Issue_Only', 'P-7_Template_Plus_Title', 'P-8_Full_Context', 'P-9_Basic_One_Shot']\n",
      "  Shape: (240, 26), Prompt variation: P-8_Full_Context\n",
      "Loading prompt_variation_P-9_Basic_One_Shot_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-9_Basic_One_Shot\n",
      "\n",
      "Combined generation data shape: (2640, 26)\n",
      "Prompt variations found: ['P-10_Full_Plus_One_Shot', 'P-11_Full_Plus_Few_Shot', 'P-1_Minimal', 'P-2_Basic', 'P-3_Diffs_Only', 'P-4_Diffs_Plus_Title', 'P-5_Code_Only', 'P-6_Issue_Only', 'P-7_Template_Plus_Title', 'P-8_Full_Context', 'P-9_Basic_One_Shot']\n"
     ]
    }
   ],
   "source": [
    "# Load and combine all prompt variation data\n",
    "print(\"\\nLoading prompt variation data...\")\n",
    "all_generation_data = []\n",
    "\n",
    "for file_path in prompt_variation_files:\n",
    "    print(f\"Loading {os.path.basename(file_path)}...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Extract prompt variation from filename\n",
    "    filename = os.path.basename(file_path)\n",
    "    prompt_var = filename.split('_')[2] + '_' + filename.split('_')[3]  # e.g., P-1_Minimal\n",
    "    \n",
    "    # Add prompt variation if not present\n",
    "    if 'prompt_variation' not in df.columns:\n",
    "        df['prompt_variation'] = prompt_var\n",
    "    \n",
    "    print(f\"  Shape: {df.shape}, Prompt variation: {df['prompt_variation'].iloc[0] if len(df) > 0 else 'N/A'}\")\n",
    "    all_generation_data.append(df)\n",
    "\n",
    "# Combine all generation data\n",
    "generation_df = pd.concat(all_generation_data, ignore_index=True)\n",
    "print(f\"\\nCombined generation data shape: {generation_df.shape}\")\n",
    "print(f\"Prompt variations found: {sorted(generation_df['prompt_variation'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e2f4b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation data columns:\n",
      "['id', 'title', 'description', 'state', 'repository', 'pr_number', 'filename', 'status', 'additions', 'deletions', 'changes', 'sha', 'blob_url', 'raw_url', 'patch', 'file_size_bytes', 'file_content', 'pr_total_size_bytes', 'issue_titles', 'issue_bodies', 'issue_comments', 'generated_description', 'prompt_variation', 'total_input_tokens', 'total_output_tokens', 'total_tokens']\n",
      "\n",
      "Generation data sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>state</th>\n",
       "      <th>repository</th>\n",
       "      <th>pr_number</th>\n",
       "      <th>filename</th>\n",
       "      <th>status</th>\n",
       "      <th>additions</th>\n",
       "      <th>deletions</th>\n",
       "      <th>...</th>\n",
       "      <th>file_content</th>\n",
       "      <th>pr_total_size_bytes</th>\n",
       "      <th>issue_titles</th>\n",
       "      <th>issue_bodies</th>\n",
       "      <th>issue_comments</th>\n",
       "      <th>generated_description</th>\n",
       "      <th>prompt_variation</th>\n",
       "      <th>total_input_tokens</th>\n",
       "      <th>total_output_tokens</th>\n",
       "      <th>total_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PR_kwDOAQ0TF85oN6RH</td>\n",
       "      <td>Fix modularity issue with html converter</td>\n",
       "      <td>&lt;!-- \\nDescribe the changes you have made here...</td>\n",
       "      <td>MERGED</td>\n",
       "      <td>JabRef/jabref</td>\n",
       "      <td>10943</td>\n",
       "      <td>build.gradle</td>\n",
       "      <td>modified</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>import org.gradle.internal.os.OperatingSystem\\...</td>\n",
       "      <td>28005</td>\n",
       "      <td>issue #10942: Fix: abstract field loses markdo...</td>\n",
       "      <td>issue #10942: After PR #10896, the abstract fi...</td>\n",
       "      <td>Comment #1 by LoayGhreeb in issue #10942: Ther...</td>\n",
       "      <td>This PR fixes a modularity issue related to th...</td>\n",
       "      <td>P-10_Full_Plus_One_Shot</td>\n",
       "      <td>10278</td>\n",
       "      <td>179</td>\n",
       "      <td>10457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2</td>\n",
       "      <td>Zbmath fetcher</td>\n",
       "      <td>Add zbmath to the public databases which can b...</td>\n",
       "      <td>MERGED</td>\n",
       "      <td>JabRef/jabref</td>\n",
       "      <td>7440</td>\n",
       "      <td>CHANGELOG.md</td>\n",
       "      <td>modified</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td># Changelog\\n\\nAll notable changes to this pro...</td>\n",
       "      <td>97657</td>\n",
       "      <td>issue #7437: Enhance bibliographic information...</td>\n",
       "      <td>issue #7437: It is possible to enhance bibliog...</td>\n",
       "      <td>Comment #1 by Siedlerchr in issue #7437: Sound...</td>\n",
       "      <td>This PR adds support for fetching bibliographi...</td>\n",
       "      <td>P-10_Full_Plus_One_Shot</td>\n",
       "      <td>33439</td>\n",
       "      <td>662</td>\n",
       "      <td>34101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2</td>\n",
       "      <td>Zbmath fetcher</td>\n",
       "      <td>Add zbmath to the public databases which can b...</td>\n",
       "      <td>MERGED</td>\n",
       "      <td>JabRef/jabref</td>\n",
       "      <td>7440</td>\n",
       "      <td>src/main/java/org/jabref/logic/importer/EntryB...</td>\n",
       "      <td>modified</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>package org.jabref.logic.importer;\\n\\nimport j...</td>\n",
       "      <td>97657</td>\n",
       "      <td>issue #7437: Enhance bibliographic information...</td>\n",
       "      <td>issue #7437: It is possible to enhance bibliog...</td>\n",
       "      <td>Comment #1 by Siedlerchr in issue #7437: Sound...</td>\n",
       "      <td>This PR adds support for fetching bibliographi...</td>\n",
       "      <td>P-10_Full_Plus_One_Shot</td>\n",
       "      <td>33439</td>\n",
       "      <td>662</td>\n",
       "      <td>34101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2</td>\n",
       "      <td>Zbmath fetcher</td>\n",
       "      <td>Add zbmath to the public databases which can b...</td>\n",
       "      <td>MERGED</td>\n",
       "      <td>JabRef/jabref</td>\n",
       "      <td>7440</td>\n",
       "      <td>src/main/java/org/jabref/logic/importer/WebFet...</td>\n",
       "      <td>modified</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>package org.jabref.logic.importer;\\n\\nimport j...</td>\n",
       "      <td>97657</td>\n",
       "      <td>issue #7437: Enhance bibliographic information...</td>\n",
       "      <td>issue #7437: It is possible to enhance bibliog...</td>\n",
       "      <td>Comment #1 by Siedlerchr in issue #7437: Sound...</td>\n",
       "      <td>This PR adds support for fetching bibliographi...</td>\n",
       "      <td>P-10_Full_Plus_One_Shot</td>\n",
       "      <td>33439</td>\n",
       "      <td>662</td>\n",
       "      <td>34101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2</td>\n",
       "      <td>Zbmath fetcher</td>\n",
       "      <td>Add zbmath to the public databases which can b...</td>\n",
       "      <td>MERGED</td>\n",
       "      <td>JabRef/jabref</td>\n",
       "      <td>7440</td>\n",
       "      <td>src/main/java/org/jabref/logic/importer/fetche...</td>\n",
       "      <td>modified</td>\n",
       "      <td>62</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>package org.jabref.logic.importer.fetcher;\\n\\n...</td>\n",
       "      <td>97657</td>\n",
       "      <td>issue #7437: Enhance bibliographic information...</td>\n",
       "      <td>issue #7437: It is possible to enhance bibliog...</td>\n",
       "      <td>Comment #1 by Siedlerchr in issue #7437: Sound...</td>\n",
       "      <td>This PR adds support for fetching bibliographi...</td>\n",
       "      <td>P-10_Full_Plus_One_Shot</td>\n",
       "      <td>33439</td>\n",
       "      <td>662</td>\n",
       "      <td>34101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id                                     title  \\\n",
       "0               PR_kwDOAQ0TF85oN6RH  Fix modularity issue with html converter   \n",
       "1  MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2                            Zbmath fetcher   \n",
       "2  MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2                            Zbmath fetcher   \n",
       "3  MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2                            Zbmath fetcher   \n",
       "4  MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2                            Zbmath fetcher   \n",
       "\n",
       "                                         description   state     repository  \\\n",
       "0  <!-- \\nDescribe the changes you have made here...  MERGED  JabRef/jabref   \n",
       "1  Add zbmath to the public databases which can b...  MERGED  JabRef/jabref   \n",
       "2  Add zbmath to the public databases which can b...  MERGED  JabRef/jabref   \n",
       "3  Add zbmath to the public databases which can b...  MERGED  JabRef/jabref   \n",
       "4  Add zbmath to the public databases which can b...  MERGED  JabRef/jabref   \n",
       "\n",
       "   pr_number                                           filename    status  \\\n",
       "0      10943                                       build.gradle  modified   \n",
       "1       7440                                       CHANGELOG.md  modified   \n",
       "2       7440  src/main/java/org/jabref/logic/importer/EntryB...  modified   \n",
       "3       7440  src/main/java/org/jabref/logic/importer/WebFet...  modified   \n",
       "4       7440  src/main/java/org/jabref/logic/importer/fetche...  modified   \n",
       "\n",
       "   additions  deletions  ...  \\\n",
       "0          4          2  ...   \n",
       "1          1          0  ...   \n",
       "2         11          3  ...   \n",
       "3          1          0  ...   \n",
       "4         62          9  ...   \n",
       "\n",
       "                                        file_content pr_total_size_bytes  \\\n",
       "0  import org.gradle.internal.os.OperatingSystem\\...               28005   \n",
       "1  # Changelog\\n\\nAll notable changes to this pro...               97657   \n",
       "2  package org.jabref.logic.importer;\\n\\nimport j...               97657   \n",
       "3  package org.jabref.logic.importer;\\n\\nimport j...               97657   \n",
       "4  package org.jabref.logic.importer.fetcher;\\n\\n...               97657   \n",
       "\n",
       "                                        issue_titles  \\\n",
       "0  issue #10942: Fix: abstract field loses markdo...   \n",
       "1  issue #7437: Enhance bibliographic information...   \n",
       "2  issue #7437: Enhance bibliographic information...   \n",
       "3  issue #7437: Enhance bibliographic information...   \n",
       "4  issue #7437: Enhance bibliographic information...   \n",
       "\n",
       "                                        issue_bodies  \\\n",
       "0  issue #10942: After PR #10896, the abstract fi...   \n",
       "1  issue #7437: It is possible to enhance bibliog...   \n",
       "2  issue #7437: It is possible to enhance bibliog...   \n",
       "3  issue #7437: It is possible to enhance bibliog...   \n",
       "4  issue #7437: It is possible to enhance bibliog...   \n",
       "\n",
       "                                      issue_comments  \\\n",
       "0  Comment #1 by LoayGhreeb in issue #10942: Ther...   \n",
       "1  Comment #1 by Siedlerchr in issue #7437: Sound...   \n",
       "2  Comment #1 by Siedlerchr in issue #7437: Sound...   \n",
       "3  Comment #1 by Siedlerchr in issue #7437: Sound...   \n",
       "4  Comment #1 by Siedlerchr in issue #7437: Sound...   \n",
       "\n",
       "                               generated_description         prompt_variation  \\\n",
       "0  This PR fixes a modularity issue related to th...  P-10_Full_Plus_One_Shot   \n",
       "1  This PR adds support for fetching bibliographi...  P-10_Full_Plus_One_Shot   \n",
       "2  This PR adds support for fetching bibliographi...  P-10_Full_Plus_One_Shot   \n",
       "3  This PR adds support for fetching bibliographi...  P-10_Full_Plus_One_Shot   \n",
       "4  This PR adds support for fetching bibliographi...  P-10_Full_Plus_One_Shot   \n",
       "\n",
       "   total_input_tokens total_output_tokens total_tokens  \n",
       "0               10278                 179        10457  \n",
       "1               33439                 662        34101  \n",
       "2               33439                 662        34101  \n",
       "3               33439                 662        34101  \n",
       "4               33439                 662        34101  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the structure of generation data\n",
    "print(\"Generation data columns:\")\n",
    "print(generation_df.columns.tolist())\n",
    "print(\"\\nGeneration data sample:\")\n",
    "generation_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd403629",
   "metadata": {},
   "source": [
    "## Data Merging and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "38691479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation data after deduplication: (352, 26)\n",
      "\n",
      "Detection data entry types: entry_type\n",
      "original     352\n",
      "generated    352\n",
      "Name: count, dtype: int64\n",
      "Detection data prompt variations: ['P-10_Full_Plus_One_Shot', 'P-11_Full_Plus_Few_Shot', 'P-1_Minimal', 'P-2_Basic', 'P-3_Diffs_Only', 'P-4_Diffs_Plus_Title', 'P-5_Code_Only', 'P-6_Issue_Only', 'P-7_Template_Plus_Title', 'P-8_Full_Context', 'P-9_Basic_One_Shot']\n"
     ]
    }
   ],
   "source": [
    "# Prepare generation data for merging\n",
    "# Group by PR ID and prompt variation to get unique records (since multiple files per PR have same generated description)\n",
    "generation_unique = generation_df.groupby(['id', 'prompt_variation']).first().reset_index()\n",
    "print(f\"Generation data after deduplication: {generation_unique.shape}\")\n",
    "\n",
    "# Prepare detection data for merging\n",
    "print(f\"\\nDetection data entry types: {detection_df['entry_type'].value_counts()}\")\n",
    "print(f\"Detection data prompt variations: {sorted(detection_df['prompt_variation'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5fd20804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data shape: (704, 31)\n",
      "Merged data entry types: entry_type\n",
      "original     352\n",
      "generated    352\n",
      "Name: count, dtype: int64\n",
      "Merged data prompt variations: ['P-10_Full_Plus_One_Shot', 'P-11_Full_Plus_Few_Shot', 'P-1_Minimal', 'P-2_Basic', 'P-3_Diffs_Only', 'P-4_Diffs_Plus_Title', 'P-5_Code_Only', 'P-6_Issue_Only', 'P-7_Template_Plus_Title', 'P-8_Full_Context', 'P-9_Basic_One_Shot']\n"
     ]
    }
   ],
   "source": [
    "# Merge generation and detection data\n",
    "# For generation data: use id as pr_id\n",
    "generation_unique['pr_id'] = generation_unique['id']\n",
    "\n",
    "# Merge on pr_id and prompt_variation\n",
    "merged_df = pd.merge(\n",
    "    generation_unique,\n",
    "    detection_df,\n",
    "    on=['pr_id', 'prompt_variation'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"Merged data shape: {merged_df.shape}\")\n",
    "print(f\"Merged data entry types: {merged_df['entry_type'].value_counts()}\")\n",
    "print(f\"Merged data prompt variations: {sorted(merged_df['prompt_variation'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d46294b",
   "metadata": {},
   "source": [
    "## Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4cd649ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_zerogpt_response(response_str):\n",
    "    \"\"\"Parse ZeroGPT response to extract AI probability\"\"\"\n",
    "    try:\n",
    "        if pd.isna(response_str) or response_str == \"\":\n",
    "            return None\n",
    "        \n",
    "        # Try to parse as JSON\n",
    "        if isinstance(response_str, str):\n",
    "            response = json.loads(response_str)\n",
    "            \n",
    "            # Handle nested structure - check if there's a 'data' field\n",
    "            if 'data' in response:\n",
    "                data = response['data']\n",
    "                # Use fakePercentage if available, otherwise calculate from isHuman\n",
    "                if 'fakePercentage' in data:\n",
    "                    return data['fakePercentage']\n",
    "                elif 'isHuman' in data:\n",
    "                    return 100 - data['isHuman']  # Convert isHuman to AI percentage\n",
    "                else:\n",
    "                    return None\n",
    "            else:\n",
    "                # Direct structure\n",
    "                if 'fakePercentage' in response:\n",
    "                    return response['fakePercentage']\n",
    "                elif 'isHuman' in response:\n",
    "                    return 100 - response['isHuman']\n",
    "                else:\n",
    "                    return None\n",
    "        else:\n",
    "            # If it's already a number or can be converted\n",
    "            return float(response_str)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing response: {e} | Response: {response_str[:100] if isinstance(response_str, str) else response_str}\")\n",
    "        return None\n",
    "\n",
    "def calculate_detection_metrics(df, ai_threshold=50.0):  # Changed threshold to 50% since we're dealing with percentages\n",
    "    \"\"\"Calculate detection accuracy metrics including AI probability statistics\"\"\"\n",
    "    # Parse AI probabilities\n",
    "    df = df.copy()  # Avoid SettingWithCopyWarning\n",
    "    df['ai_probability'] = df['zerogpt_response'].apply(parse_zerogpt_response)\n",
    "    \n",
    "    # Filter out rows where AI probability couldn't be parsed\n",
    "    valid_df = df[df['ai_probability'].notna()].copy()\n",
    "    \n",
    "    if len(valid_df) == 0:\n",
    "        return {\n",
    "            'true_positive_pct': 0, \n",
    "            'false_negative_pct': 0, \n",
    "            'total_samples': len(df), \n",
    "            'valid_samples': 0,\n",
    "            'mean_ai_score_generated': 0,\n",
    "            'median_ai_score_generated': 0,\n",
    "            'mean_ai_score_original': 0,\n",
    "            'median_ai_score_original': 0\n",
    "        }\n",
    "    \n",
    "    # Determine if detected as AI (above threshold)\n",
    "    valid_df['detected_as_ai'] = valid_df['ai_probability'] > ai_threshold\n",
    "    \n",
    "    # Calculate metrics\n",
    "    generated_entries = valid_df[valid_df['entry_type'] == 'generated']\n",
    "    original_entries = valid_df[valid_df['entry_type'] == 'original']\n",
    "    \n",
    "    # True Positive Rate: Generated content correctly identified as AI\n",
    "    tp_rate = 0\n",
    "    if len(generated_entries) > 0:\n",
    "        tp_rate = (generated_entries['detected_as_ai'].sum() / len(generated_entries)) * 100\n",
    "    \n",
    "    # False Positive Rate: Original content incorrectly identified as AI (this is what we call \"false negative\" in the context)\n",
    "    fp_rate = 0\n",
    "    if len(original_entries) > 0:\n",
    "        fp_rate = (original_entries['detected_as_ai'].sum() / len(original_entries)) * 100\n",
    "    \n",
    "    # AI probability statistics\n",
    "    mean_ai_generated = generated_entries['ai_probability'].mean() if len(generated_entries) > 0 else 0\n",
    "    median_ai_generated = generated_entries['ai_probability'].median() if len(generated_entries) > 0 else 0\n",
    "    mean_ai_original = original_entries['ai_probability'].mean() if len(original_entries) > 0 else 0\n",
    "    median_ai_original = original_entries['ai_probability'].median() if len(original_entries) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'true_positive_pct': tp_rate,\n",
    "        'false_negative_pct': fp_rate,  # FP on original = FN from human perspective\n",
    "        'total_samples': len(df),\n",
    "        'valid_samples': len(valid_df),\n",
    "        'mean_ai_score_generated': mean_ai_generated,\n",
    "        'median_ai_score_generated': median_ai_generated,\n",
    "        'mean_ai_score_original': mean_ai_original,\n",
    "        'median_ai_score_original': median_ai_original\n",
    "    }\n",
    "\n",
    "def calculate_text_metrics(text_series):\n",
    "    \"\"\"Calculate text-based metrics\"\"\"\n",
    "    if len(text_series) == 0:\n",
    "        return {'mean_length': 0, 'median_length': 0}\n",
    "    \n",
    "    lengths = text_series.str.len()\n",
    "    return {\n",
    "        'mean_length': lengths.mean(),\n",
    "        'median_length': lengths.median()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f467d23",
   "metadata": {},
   "source": [
    "## Main Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c97fa1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debugging merged data structure:\n",
      "Merged data shape: (704, 31)\n",
      "Entry types: entry_type\n",
      "original     352\n",
      "generated    352\n",
      "Name: count, dtype: int64\n",
      "Prompt variations: ['P-10_Full_Plus_One_Shot', 'P-11_Full_Plus_Few_Shot', 'P-1_Minimal', 'P-2_Basic', 'P-3_Diffs_Only', 'P-4_Diffs_Plus_Title', 'P-5_Code_Only', 'P-6_Issue_Only', 'P-7_Template_Plus_Title', 'P-8_Full_Context', 'P-9_Basic_One_Shot']\n",
      "\n",
      "ZeroGPT response data availability:\n",
      "Non-null zerogpt_response: 704\n",
      "Empty zerogpt_response: 0\n",
      "\n",
      "Sample zerogpt responses:\n",
      "Sample 1: {\"success\": true, \"code\": 200, \"message\": \"detection result passed to proxy\", \"data\": {\"sentences\": [], \"isHuman\": 100, \"additional_feedback\": \"\", \"h\": [], \"hi\": [], \"textWords\": 77, \"aiWords\": 0, \"fa...\n",
      "Sample 2: {\"success\": true, \"code\": 200, \"message\": \"detection result passed to proxy\", \"data\": {\"sentences\": [], \"isHuman\": 100, \"additional_feedback\": \"\", \"h\": [], \"hi\": [], \"textWords\": 87, \"aiWords\": 0, \"fa...\n",
      "Sample 3: {\"success\": true, \"code\": 200, \"message\": \"detection result passed to proxy\", \"data\": {\"sentences\": [], \"isHuman\": 100, \"additional_feedback\": \"\", \"h\": [], \"hi\": [], \"textWords\": 77, \"aiWords\": 0, \"fa...\n",
      "\n",
      "Analyzing prompt variations: ['P-10_Full_Plus_One_Shot', 'P-11_Full_Plus_Few_Shot', 'P-1_Minimal', 'P-2_Basic', 'P-3_Diffs_Only', 'P-4_Diffs_Plus_Title', 'P-5_Code_Only', 'P-6_Issue_Only', 'P-7_Template_Plus_Title', 'P-8_Full_Context', 'P-9_Basic_One_Shot']\n",
      "\n",
      "Analyzing Original descriptions...\n",
      "Original data shape: (352, 31)\n",
      "  Total samples: 352\n",
      "  Valid samples: 352\n",
      "  Mean description length: 1184.0\n",
      "  False negative rate: 0.0%\n",
      "  Mean AI score (original): 13.5%\n",
      "  Median AI score (original): 9.9%\n",
      "  Total samples: 352\n",
      "  Valid samples: 352\n",
      "  Mean description length: 1184.0\n",
      "  False negative rate: 0.0%\n",
      "  Mean AI score (original): 13.5%\n",
      "  Median AI score (original): 9.9%\n"
     ]
    }
   ],
   "source": [
    "# Debug: Let's check the merged data structure\n",
    "print(\"Debugging merged data structure:\")\n",
    "print(f\"Merged data shape: {merged_df.shape}\")\n",
    "print(f\"Entry types: {merged_df['entry_type'].value_counts()}\")\n",
    "print(f\"Prompt variations: {sorted(merged_df['prompt_variation'].unique())}\")\n",
    "\n",
    "# Check if we have zerogpt_response data\n",
    "print(f\"\\nZeroGPT response data availability:\")\n",
    "print(f\"Non-null zerogpt_response: {merged_df['zerogpt_response'].notna().sum()}\")\n",
    "print(f\"Empty zerogpt_response: {(merged_df['zerogpt_response'] == '').sum()}\")\n",
    "\n",
    "# Look at a sample of zerogpt responses\n",
    "print(\"\\nSample zerogpt responses:\")\n",
    "sample_responses = merged_df[merged_df['zerogpt_response'].notna() & (merged_df['zerogpt_response'] != '')]['zerogpt_response'].head(3)\n",
    "for i, resp in enumerate(sample_responses):\n",
    "    print(f\"Sample {i+1}: {resp[:200]}...\")\n",
    "\n",
    "# Now re-analyze with this understanding\n",
    "analysis_results = []\n",
    "\n",
    "# Get all prompt variations plus 'Original'\n",
    "prompt_variations = sorted([pv for pv in merged_df['prompt_variation'].unique() if pv.startswith('P-')])\n",
    "print(f\"\\nAnalyzing prompt variations: {prompt_variations}\")\n",
    "\n",
    "# First, analyze 'Original' (using original descriptions from any prompt variation)\n",
    "print(\"\\nAnalyzing Original descriptions...\")\n",
    "original_data = merged_df[merged_df['entry_type'] == 'original']\n",
    "print(f\"Original data shape: {original_data.shape}\")\n",
    "\n",
    "if len(original_data) > 0:\n",
    "    # Token metrics - Original doesn't have token usage, so set to 0\n",
    "    original_metrics = {\n",
    "        'prompt_variation': 'Original',\n",
    "        'mean_prompt_tokens': 0,\n",
    "        'median_prompt_tokens': 0,\n",
    "        'mean_completion_tokens': 0,\n",
    "        'median_completion_tokens': 0\n",
    "    }\n",
    "    \n",
    "    # Text metrics\n",
    "    text_metrics = calculate_text_metrics(original_data['input_text'])\n",
    "    original_metrics.update({\n",
    "        'mean_description_length': text_metrics['mean_length'],\n",
    "        'median_description_length': text_metrics['median_length']\n",
    "    })\n",
    "    \n",
    "    # Detection metrics (including new AI probability metrics)\n",
    "    detection_metrics = calculate_detection_metrics(original_data)\n",
    "    original_metrics.update({\n",
    "        'true_positive_pct': detection_metrics['true_positive_pct'],\n",
    "        'false_negative_pct': detection_metrics['false_negative_pct'],\n",
    "        'mean_ai_score_generated': detection_metrics['mean_ai_score_generated'],\n",
    "        'median_ai_score_generated': detection_metrics['median_ai_score_generated'],\n",
    "        'mean_ai_score_original': detection_metrics['mean_ai_score_original'],\n",
    "        'median_ai_score_original': detection_metrics['median_ai_score_original'],\n",
    "        'total_samples': detection_metrics['total_samples'],\n",
    "        'valid_samples': detection_metrics['valid_samples']\n",
    "    })\n",
    "    \n",
    "    analysis_results.append(original_metrics)\n",
    "    print(f\"  Total samples: {detection_metrics['total_samples']}\")\n",
    "    print(f\"  Valid samples: {detection_metrics['valid_samples']}\")\n",
    "    print(f\"  Mean description length: {text_metrics['mean_length']:.1f}\")\n",
    "    print(f\"  False negative rate: {detection_metrics['false_negative_pct']:.1f}%\")\n",
    "    print(f\"  Mean AI score (original): {detection_metrics['mean_ai_score_original']:.1f}%\")\n",
    "    print(f\"  Median AI score (original): {detection_metrics['median_ai_score_original']:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "57916e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing P-10_Full_Plus_One_Shot...\n",
      "  Samples: 64 (Generated: 32)\n",
      "  Mean prompt tokens: 64351\n",
      "  Mean completion tokens: 557\n",
      "  Mean description length: 1112.8\n",
      "  True positive rate: 28.1%\n",
      "  False negative rate: 0.0%\n",
      "  Mean AI score (generated): 27.0%\n",
      "  Mean AI score (original): 13.5%\n",
      "\n",
      "Analyzing P-11_Full_Plus_Few_Shot...\n",
      "  Samples: 64 (Generated: 32)\n",
      "  Mean prompt tokens: 74742\n",
      "  Mean completion tokens: 451\n",
      "  Mean description length: 922.2\n",
      "  True positive rate: 15.6%\n",
      "  False negative rate: 0.0%\n",
      "  Mean AI score (generated): 22.5%\n",
      "  Mean AI score (original): 13.5%\n",
      "\n",
      "Analyzing P-1_Minimal...\n",
      "  Samples: 64 (Generated: 32)\n",
      "  Mean prompt tokens: 632\n",
      "  Mean completion tokens: 190\n",
      "  Mean description length: 500.2\n",
      "  True positive rate: 3.1%\n",
      "  False negative rate: 0.0%\n",
      "  Mean AI score (generated): 4.7%\n",
      "  Mean AI score (original): 13.5%\n",
      "\n",
      "Analyzing P-2_Basic...\n",
      "  Samples: 64 (Generated: 32)\n",
      "  Mean prompt tokens: 681\n",
      "  Mean completion tokens: 212\n",
      "  Mean description length: 511.4\n",
      "  True positive rate: 6.2%\n",
      "  False negative rate: 0.0%\n",
      "  Mean AI score (generated): 9.3%\n",
      "  Mean AI score (original): 13.5%\n",
      "\n",
      "Analyzing P-3_Diffs_Only...\n",
      "  Samples: 64 (Generated: 32)\n",
      "  Mean prompt tokens: 5575\n",
      "  Mean completion tokens: 289\n",
      "  Mean description length: 712.5\n",
      "  True positive rate: 6.2%\n",
      "  False negative rate: 0.0%\n",
      "  Mean AI score (generated): 6.8%\n",
      "  Mean AI score (original): 13.5%\n",
      "\n",
      "Analyzing P-4_Diffs_Plus_Title...\n",
      "  Samples: 64 (Generated: 32)\n",
      "  Mean prompt tokens: 5620\n",
      "  Mean completion tokens: 303\n",
      "  Mean description length: 698.6\n",
      "  True positive rate: 3.1%\n",
      "  False negative rate: 0.0%\n",
      "  Mean AI score (generated): 9.9%\n",
      "  Mean AI score (original): 13.5%\n",
      "\n",
      "Analyzing P-5_Code_Only...\n",
      "  Samples: 64 (Generated: 32)\n",
      "  Mean prompt tokens: 56585\n",
      "  Mean completion tokens: 348\n",
      "  Mean description length: 806.5\n",
      "  True positive rate: 12.5%\n",
      "  False negative rate: 0.0%\n",
      "  Mean AI score (generated): 14.4%\n",
      "  Mean AI score (original): 13.5%\n",
      "\n",
      "Analyzing P-6_Issue_Only...\n",
      "  Samples: 64 (Generated: 32)\n",
      "  Mean prompt tokens: 2110\n",
      "  Mean completion tokens: 345\n",
      "  Mean description length: 669.0\n",
      "  True positive rate: 12.5%\n",
      "  False negative rate: 0.0%\n",
      "  Mean AI score (generated): 17.4%\n",
      "  Mean AI score (original): 13.5%\n",
      "\n",
      "Analyzing P-7_Template_Plus_Title...\n",
      "  Samples: 64 (Generated: 32)\n",
      "  Mean prompt tokens: 873\n",
      "  Mean completion tokens: 327\n",
      "  Mean description length: 725.7\n",
      "  True positive rate: 3.1%\n",
      "  False negative rate: 0.0%\n",
      "  Mean AI score (generated): 13.2%\n",
      "  Mean AI score (original): 13.5%\n",
      "\n",
      "Analyzing P-8_Full_Context...\n",
      "  Samples: 64 (Generated: 32)\n",
      "  Mean prompt tokens: 58099\n",
      "  Mean completion tokens: 450\n",
      "  Mean description length: 958.5\n",
      "  True positive rate: 3.1%\n",
      "  False negative rate: 0.0%\n",
      "  Mean AI score (generated): 12.8%\n",
      "  Mean AI score (original): 13.5%\n",
      "\n",
      "Analyzing P-9_Basic_One_Shot...\n",
      "  Samples: 64 (Generated: 32)\n",
      "  Mean prompt tokens: 2154\n",
      "  Mean completion tokens: 310\n",
      "  Mean description length: 720.2\n",
      "  True positive rate: 28.1%\n",
      "  False negative rate: 0.0%\n",
      "  Mean AI score (generated): 27.9%\n",
      "  Mean AI score (original): 13.5%\n"
     ]
    }
   ],
   "source": [
    "# Now analyze each prompt variation\n",
    "for pv in prompt_variations:\n",
    "    print(f\"\\nAnalyzing {pv}...\")\n",
    "    \n",
    "    # Get data for this prompt variation\n",
    "    pv_data = merged_df[merged_df['prompt_variation'] == pv]\n",
    "    \n",
    "    if len(pv_data) == 0:\n",
    "        print(f\"  No data found for {pv}\")\n",
    "        continue\n",
    "    \n",
    "    # Get generation data for token metrics (only generated entries have token info)\n",
    "    pv_generated = pv_data[pv_data['entry_type'] == 'generated']\n",
    "    \n",
    "    pv_metrics = {'prompt_variation': pv}\n",
    "    \n",
    "    # Token metrics\n",
    "    if len(pv_generated) > 0 and 'total_input_tokens' in pv_generated.columns:\n",
    "        pv_metrics.update({\n",
    "            'mean_prompt_tokens': pv_generated['total_input_tokens'].mean(),\n",
    "            'median_prompt_tokens': pv_generated['total_input_tokens'].median(),\n",
    "            'mean_completion_tokens': pv_generated['total_output_tokens'].mean(),\n",
    "            'median_completion_tokens': pv_generated['total_output_tokens'].median()\n",
    "        })\n",
    "    else:\n",
    "        pv_metrics.update({\n",
    "            'mean_prompt_tokens': 0,\n",
    "            'median_prompt_tokens': 0,\n",
    "            'mean_completion_tokens': 0,\n",
    "            'median_completion_tokens': 0\n",
    "        })\n",
    "    \n",
    "    # Text metrics (using generated descriptions)\n",
    "    if len(pv_generated) > 0:\n",
    "        text_metrics = calculate_text_metrics(pv_generated['input_text'])\n",
    "        pv_metrics.update({\n",
    "            'mean_description_length': text_metrics['mean_length'],\n",
    "            'median_description_length': text_metrics['median_length']\n",
    "        })\n",
    "    else:\n",
    "        pv_metrics.update({\n",
    "            'mean_description_length': 0,\n",
    "            'median_description_length': 0\n",
    "        })\n",
    "    \n",
    "    # Detection metrics (using both original and generated, including new AI probability metrics)\n",
    "    detection_metrics = calculate_detection_metrics(pv_data)\n",
    "    pv_metrics.update({\n",
    "        'true_positive_pct': detection_metrics['true_positive_pct'],\n",
    "        'false_negative_pct': detection_metrics['false_negative_pct'],\n",
    "        'mean_ai_score_generated': detection_metrics['mean_ai_score_generated'],\n",
    "        'median_ai_score_generated': detection_metrics['median_ai_score_generated'],\n",
    "        'mean_ai_score_original': detection_metrics['mean_ai_score_original'],\n",
    "        'median_ai_score_original': detection_metrics['median_ai_score_original'],\n",
    "        'total_samples': detection_metrics['total_samples']\n",
    "    })\n",
    "    \n",
    "    analysis_results.append(pv_metrics)\n",
    "    \n",
    "    print(f\"  Samples: {detection_metrics['total_samples']} (Generated: {len(pv_generated)})\")\n",
    "    print(f\"  Mean prompt tokens: {pv_metrics['mean_prompt_tokens']:.0f}\")\n",
    "    print(f\"  Mean completion tokens: {pv_metrics['mean_completion_tokens']:.0f}\")\n",
    "    print(f\"  Mean description length: {pv_metrics['mean_description_length']:.1f}\")\n",
    "    print(f\"  True positive rate: {detection_metrics['true_positive_pct']:.1f}%\")\n",
    "    print(f\"  False negative rate: {detection_metrics['false_negative_pct']:.1f}%\")\n",
    "    print(f\"  Mean AI score (generated): {detection_metrics['mean_ai_score_generated']:.1f}%\")\n",
    "    print(f\"  Mean AI score (original): {detection_metrics['mean_ai_score_original']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b835b3c3",
   "metadata": {},
   "source": [
    "## Results Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3a232af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PROMPT VARIATIONS ANALYSIS RESULTS ===\n",
      "\n",
      "Target Output Table with AI Probability Scores:\n",
      "================================================================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_variation</th>\n",
       "      <th>mean_prompt_tokens</th>\n",
       "      <th>median_prompt_tokens</th>\n",
       "      <th>mean_completion_tokens</th>\n",
       "      <th>median_completion_tokens</th>\n",
       "      <th>mean_description_length</th>\n",
       "      <th>median_description_length</th>\n",
       "      <th>true_positive_pct</th>\n",
       "      <th>false_negative_pct</th>\n",
       "      <th>mean_ai_score_generated</th>\n",
       "      <th>median_ai_score_generated</th>\n",
       "      <th>mean_ai_score_original</th>\n",
       "      <th>median_ai_score_original</th>\n",
       "      <th>total_samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Original</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1184.0</td>\n",
       "      <td>1187.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P-10_Full_Plus_One_Shot</td>\n",
       "      <td>64351.4</td>\n",
       "      <td>28815.5</td>\n",
       "      <td>557.1</td>\n",
       "      <td>303.5</td>\n",
       "      <td>1112.8</td>\n",
       "      <td>983.0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>16.8</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P-11_Full_Plus_Few_Shot</td>\n",
       "      <td>74741.9</td>\n",
       "      <td>35300.0</td>\n",
       "      <td>451.0</td>\n",
       "      <td>246.5</td>\n",
       "      <td>922.2</td>\n",
       "      <td>761.5</td>\n",
       "      <td>15.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.5</td>\n",
       "      <td>18.1</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P-1_Minimal</td>\n",
       "      <td>632.5</td>\n",
       "      <td>365.0</td>\n",
       "      <td>189.8</td>\n",
       "      <td>85.5</td>\n",
       "      <td>500.2</td>\n",
       "      <td>373.5</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P-2_Basic</td>\n",
       "      <td>681.2</td>\n",
       "      <td>412.5</td>\n",
       "      <td>212.5</td>\n",
       "      <td>117.5</td>\n",
       "      <td>511.4</td>\n",
       "      <td>396.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>P-3_Diffs_Only</td>\n",
       "      <td>5574.5</td>\n",
       "      <td>2129.0</td>\n",
       "      <td>288.8</td>\n",
       "      <td>155.0</td>\n",
       "      <td>712.5</td>\n",
       "      <td>480.5</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>P-4_Diffs_Plus_Title</td>\n",
       "      <td>5619.8</td>\n",
       "      <td>2143.5</td>\n",
       "      <td>303.1</td>\n",
       "      <td>167.0</td>\n",
       "      <td>698.6</td>\n",
       "      <td>513.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>P-5_Code_Only</td>\n",
       "      <td>56585.1</td>\n",
       "      <td>24574.5</td>\n",
       "      <td>348.1</td>\n",
       "      <td>174.5</td>\n",
       "      <td>806.5</td>\n",
       "      <td>658.5</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>P-6_Issue_Only</td>\n",
       "      <td>2109.8</td>\n",
       "      <td>882.5</td>\n",
       "      <td>345.1</td>\n",
       "      <td>181.5</td>\n",
       "      <td>669.0</td>\n",
       "      <td>575.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>P-7_Template_Plus_Title</td>\n",
       "      <td>872.8</td>\n",
       "      <td>525.0</td>\n",
       "      <td>326.9</td>\n",
       "      <td>175.5</td>\n",
       "      <td>725.7</td>\n",
       "      <td>604.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>P-8_Full_Context</td>\n",
       "      <td>58098.7</td>\n",
       "      <td>24909.0</td>\n",
       "      <td>450.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>958.5</td>\n",
       "      <td>816.5</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>P-9_Basic_One_Shot</td>\n",
       "      <td>2154.0</td>\n",
       "      <td>1318.0</td>\n",
       "      <td>310.4</td>\n",
       "      <td>170.5</td>\n",
       "      <td>720.2</td>\n",
       "      <td>650.0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.9</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           prompt_variation  mean_prompt_tokens  median_prompt_tokens  \\\n",
       "0                  Original                 0.0                   0.0   \n",
       "1   P-10_Full_Plus_One_Shot             64351.4               28815.5   \n",
       "2   P-11_Full_Plus_Few_Shot             74741.9               35300.0   \n",
       "3               P-1_Minimal               632.5                 365.0   \n",
       "4                 P-2_Basic               681.2                 412.5   \n",
       "5            P-3_Diffs_Only              5574.5                2129.0   \n",
       "6      P-4_Diffs_Plus_Title              5619.8                2143.5   \n",
       "7             P-5_Code_Only             56585.1               24574.5   \n",
       "8            P-6_Issue_Only              2109.8                 882.5   \n",
       "9   P-7_Template_Plus_Title               872.8                 525.0   \n",
       "10         P-8_Full_Context             58098.7               24909.0   \n",
       "11       P-9_Basic_One_Shot              2154.0                1318.0   \n",
       "\n",
       "    mean_completion_tokens  median_completion_tokens  mean_description_length  \\\n",
       "0                      0.0                       0.0                   1184.0   \n",
       "1                    557.1                     303.5                   1112.8   \n",
       "2                    451.0                     246.5                    922.2   \n",
       "3                    189.8                      85.5                    500.2   \n",
       "4                    212.5                     117.5                    511.4   \n",
       "5                    288.8                     155.0                    712.5   \n",
       "6                    303.1                     167.0                    698.6   \n",
       "7                    348.1                     174.5                    806.5   \n",
       "8                    345.1                     181.5                    669.0   \n",
       "9                    326.9                     175.5                    725.7   \n",
       "10                   450.0                     227.0                    958.5   \n",
       "11                   310.4                     170.5                    720.2   \n",
       "\n",
       "    median_description_length  true_positive_pct  false_negative_pct  \\\n",
       "0                      1187.5                0.0                 0.0   \n",
       "1                       983.0               28.1                 0.0   \n",
       "2                       761.5               15.6                 0.0   \n",
       "3                       373.5                3.1                 0.0   \n",
       "4                       396.0                6.2                 0.0   \n",
       "5                       480.5                6.2                 0.0   \n",
       "6                       513.0                3.1                 0.0   \n",
       "7                       658.5               12.5                 0.0   \n",
       "8                       575.0               12.5                 0.0   \n",
       "9                       604.0                3.1                 0.0   \n",
       "10                      816.5                3.1                 0.0   \n",
       "11                      650.0               28.1                 0.0   \n",
       "\n",
       "    mean_ai_score_generated  median_ai_score_generated  \\\n",
       "0                       0.0                        0.0   \n",
       "1                      27.0                       16.8   \n",
       "2                      22.5                       18.1   \n",
       "3                       4.7                        0.0   \n",
       "4                       9.3                        0.0   \n",
       "5                       6.8                        0.0   \n",
       "6                       9.9                        0.0   \n",
       "7                      14.4                        0.0   \n",
       "8                      17.4                        0.0   \n",
       "9                      13.2                        0.0   \n",
       "10                     12.8                        0.0   \n",
       "11                     27.9                        6.0   \n",
       "\n",
       "    mean_ai_score_original  median_ai_score_original  total_samples  \n",
       "0                     13.5                       9.9            352  \n",
       "1                     13.5                       9.9             64  \n",
       "2                     13.5                       9.9             64  \n",
       "3                     13.5                       9.9             64  \n",
       "4                     13.5                       9.9             64  \n",
       "5                     13.5                       9.9             64  \n",
       "6                     13.5                       9.9             64  \n",
       "7                     13.5                       9.9             64  \n",
       "8                     13.5                       9.9             64  \n",
       "9                     13.5                       9.9             64  \n",
       "10                    13.5                       9.9             64  \n",
       "11                    13.5                       9.9             64  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create results dataframe\n",
    "results_df = pd.DataFrame(analysis_results)\n",
    "\n",
    "# Reorder columns as requested, including new AI probability metrics\n",
    "column_order = [\n",
    "    'prompt_variation',\n",
    "    'mean_prompt_tokens',\n",
    "    'median_prompt_tokens', \n",
    "    'mean_completion_tokens',\n",
    "    'median_completion_tokens',\n",
    "    'mean_description_length',\n",
    "    'median_description_length',\n",
    "    'true_positive_pct',\n",
    "    'false_negative_pct',\n",
    "    'mean_ai_score_generated',\n",
    "    'median_ai_score_generated',\n",
    "    'mean_ai_score_original',\n",
    "    'median_ai_score_original',\n",
    "    'total_samples'\n",
    "]\n",
    "\n",
    "results_df = results_df[column_order]\n",
    "\n",
    "# Round numeric columns for better display\n",
    "numeric_cols = [col for col in results_df.columns if col != 'prompt_variation']\n",
    "results_df[numeric_cols] = results_df[numeric_cols].round(1)\n",
    "\n",
    "print(\"\\n=== PROMPT VARIATIONS ANALYSIS RESULTS ===\")\n",
    "print(\"\\nTarget Output Table with AI Probability Scores:\")\n",
    "print(\"=\" * 160)\n",
    "\n",
    "# Display the table\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ba72e713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved to: prompt_variations_analysis_results.csv\n",
      "\n",
      "=== SUMMARY STATISTICS ===\n",
      "Total prompt variations analyzed: 12\n",
      "Best performing prompt (highest true positive rate): P-10_Full_Plus_One_Shot\n",
      "Most efficient prompt (lowest mean prompt tokens): P-1_Minimal\n",
      "Longest descriptions (highest mean length): Original\n"
     ]
    }
   ],
   "source": [
    "# Save results to CSV\n",
    "output_file = \"prompt_variations_analysis_results.csv\"\n",
    "results_df.to_csv(output_file, index=False)\n",
    "print(f\"\\nResults saved to: {output_file}\")\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\n=== SUMMARY STATISTICS ===\")\n",
    "print(f\"Total prompt variations analyzed: {len(results_df)}\")\n",
    "print(f\"Best performing prompt (highest true positive rate): {results_df.loc[results_df['true_positive_pct'].idxmax(), 'prompt_variation']}\")\n",
    "print(f\"Most efficient prompt (lowest mean prompt tokens): {results_df[results_df['prompt_variation'] != 'Original'].loc[results_df[results_df['prompt_variation'] != 'Original']['mean_prompt_tokens'].idxmin(), 'prompt_variation']}\")\n",
    "print(f\"Longest descriptions (highest mean length): {results_df.loc[results_df['mean_description_length'].idxmax(), 'prompt_variation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88735b38",
   "metadata": {},
   "source": [
    "## Additional Textual Analysis Features\n",
    "\n",
    "Now let's add some additional textual analysis features similar to the jabref-prs-comparison notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e98d514e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended textual analysis functions defined.\n"
     ]
    }
   ],
   "source": [
    "def extract_zerogpt_text_metrics(response_str):\n",
    "    \"\"\"Extract textual metrics from ZeroGPT response\"\"\"\n",
    "    try:\n",
    "        if pd.isna(response_str) or response_str == \"\":\n",
    "            return {'textWords': None, 'aiWords': None, 'word_count': None}\n",
    "        \n",
    "        response = json.loads(response_str)\n",
    "        \n",
    "        if 'data' in response:\n",
    "            data = response['data']\n",
    "            return {\n",
    "                'textWords': data.get('textWords', None),\n",
    "                'aiWords': data.get('aiWords', None),\n",
    "                'word_count': data.get('textWords', None)  # Same as textWords\n",
    "            }\n",
    "        else:\n",
    "            return {'textWords': None, 'aiWords': None, 'word_count': None}\n",
    "    except:\n",
    "        return {'textWords': None, 'aiWords': None, 'word_count': None}\n",
    "\n",
    "def calculate_extended_text_metrics(df):\n",
    "    \"\"\"Calculate extended textual metrics\"\"\"\n",
    "    if len(df) == 0:\n",
    "        return {}\n",
    "    \n",
    "    # Basic length metrics\n",
    "    lengths = df['input_text'].str.len()\n",
    "    \n",
    "    # Extract ZeroGPT word counts\n",
    "    zerogpt_metrics = df['zerogpt_response'].apply(extract_zerogpt_text_metrics)\n",
    "    word_counts = pd.DataFrame(zerogpt_metrics.tolist())['textWords'].dropna()\n",
    "    \n",
    "    # Calculate sentences (approximate by counting periods, exclamation marks, question marks)\n",
    "    sentence_counts = df['input_text'].str.count(r'[.!?]+')\n",
    "    \n",
    "    # Calculate newlines (as a proxy for paragraph structure)\n",
    "    newline_counts = df['input_text'].str.count(r'\\\\n')\n",
    "    \n",
    "    return {\n",
    "        'mean_char_length': lengths.mean(),\n",
    "        'median_char_length': lengths.median(),\n",
    "        'mean_word_count': word_counts.mean() if len(word_counts) > 0 else 0,\n",
    "        'median_word_count': word_counts.median() if len(word_counts) > 0 else 0,\n",
    "        'mean_sentence_count': sentence_counts.mean(),\n",
    "        'median_sentence_count': sentence_counts.median(),\n",
    "        'mean_newline_count': newline_counts.mean(),\n",
    "        'median_newline_count': newline_counts.median()\n",
    "    }\n",
    "\n",
    "print(\"Extended textual analysis functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5add51c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXTENDED TEXTUAL ANALYSIS ===\\n\n",
      "Original - Samples: 352\n",
      "  Mean words: 160.7\n",
      "  Mean sentences: 19.8\n",
      "  Mean newlines: 0.0\\n\n",
      "P-10_Full_Plus_One_Shot - Samples: 32\n",
      "  Mean words: 159.9\n",
      "  Mean sentences: 9.8\n",
      "  Mean newlines: 0.0\\n\n",
      "P-11_Full_Plus_Few_Shot - Samples: 32\n",
      "  Mean words: 132.0\n",
      "  Mean sentences: 8.2\n",
      "  Mean newlines: 0.0\\n\n",
      "P-1_Minimal - Samples: 32\n",
      "  Mean words: 62.0\n",
      "  Mean sentences: 6.2\n",
      "  Mean newlines: 0.0\\n\n",
      "P-2_Basic - Samples: 32\n",
      "  Mean words: 67.8\n",
      "  Mean sentences: 5.3\n",
      "  Mean newlines: 0.0\\n\n",
      "P-3_Diffs_Only - Samples: 32\n",
      "  Mean words: 93.9\n",
      "  Mean sentences: 6.7\n",
      "  Mean newlines: 0.0\\n\n",
      "P-4_Diffs_Plus_Title - Samples: 32\n",
      "  Mean words: 94.4\n",
      "  Mean sentences: 6.5\n",
      "  Mean newlines: 0.0\\n\n",
      "P-5_Code_Only - Samples: 32\n",
      "  Mean words: 109.9\n",
      "  Mean sentences: 6.9\n",
      "  Mean newlines: 0.0\\n\n",
      "P-6_Issue_Only - Samples: 32\n",
      "  Mean words: 94.2\n",
      "  Mean sentences: 6.4\n",
      "  Mean newlines: 0.0\\n\n",
      "P-7_Template_Plus_Title - Samples: 32\n",
      "  Mean words: 98.7\n",
      "  Mean sentences: 8.0\n",
      "  Mean newlines: 0.0\\n\n",
      "P-8_Full_Context - Samples: 32\n",
      "  Mean words: 134.7\n",
      "  Mean sentences: 8.3\n",
      "  Mean newlines: 0.0\\n\n",
      "P-9_Basic_One_Shot - Samples: 32\n",
      "  Mean words: 95.9\n",
      "  Mean sentences: 7.0\n",
      "  Mean newlines: 0.0\\n\n",
      "Original - Samples: 352\n",
      "  Mean words: 160.7\n",
      "  Mean sentences: 19.8\n",
      "  Mean newlines: 0.0\\n\n",
      "P-10_Full_Plus_One_Shot - Samples: 32\n",
      "  Mean words: 159.9\n",
      "  Mean sentences: 9.8\n",
      "  Mean newlines: 0.0\\n\n",
      "P-11_Full_Plus_Few_Shot - Samples: 32\n",
      "  Mean words: 132.0\n",
      "  Mean sentences: 8.2\n",
      "  Mean newlines: 0.0\\n\n",
      "P-1_Minimal - Samples: 32\n",
      "  Mean words: 62.0\n",
      "  Mean sentences: 6.2\n",
      "  Mean newlines: 0.0\\n\n",
      "P-2_Basic - Samples: 32\n",
      "  Mean words: 67.8\n",
      "  Mean sentences: 5.3\n",
      "  Mean newlines: 0.0\\n\n",
      "P-3_Diffs_Only - Samples: 32\n",
      "  Mean words: 93.9\n",
      "  Mean sentences: 6.7\n",
      "  Mean newlines: 0.0\\n\n",
      "P-4_Diffs_Plus_Title - Samples: 32\n",
      "  Mean words: 94.4\n",
      "  Mean sentences: 6.5\n",
      "  Mean newlines: 0.0\\n\n",
      "P-5_Code_Only - Samples: 32\n",
      "  Mean words: 109.9\n",
      "  Mean sentences: 6.9\n",
      "  Mean newlines: 0.0\\n\n",
      "P-6_Issue_Only - Samples: 32\n",
      "  Mean words: 94.2\n",
      "  Mean sentences: 6.4\n",
      "  Mean newlines: 0.0\\n\n",
      "P-7_Template_Plus_Title - Samples: 32\n",
      "  Mean words: 98.7\n",
      "  Mean sentences: 8.0\n",
      "  Mean newlines: 0.0\\n\n",
      "P-8_Full_Context - Samples: 32\n",
      "  Mean words: 134.7\n",
      "  Mean sentences: 8.3\n",
      "  Mean newlines: 0.0\\n\n",
      "P-9_Basic_One_Shot - Samples: 32\n",
      "  Mean words: 95.9\n",
      "  Mean sentences: 7.0\n",
      "  Mean newlines: 0.0\\n\n"
     ]
    }
   ],
   "source": [
    "# Extended textual analysis for each prompt variation\n",
    "print(\"=== EXTENDED TEXTUAL ANALYSIS ===\\\\n\")\n",
    "\n",
    "extended_analysis_results = []\n",
    "\n",
    "# Analyze Original\n",
    "original_data = merged_df[merged_df['entry_type'] == 'original']\n",
    "if len(original_data) > 0:\n",
    "    ext_metrics = calculate_extended_text_metrics(original_data)\n",
    "    ext_metrics['prompt_variation'] = 'Original'\n",
    "    extended_analysis_results.append(ext_metrics)\n",
    "    print(f\"Original - Samples: {len(original_data)}\")\n",
    "    print(f\"  Mean words: {ext_metrics['mean_word_count']:.1f}\")\n",
    "    print(f\"  Mean sentences: {ext_metrics['mean_sentence_count']:.1f}\")\n",
    "    print(f\"  Mean newlines: {ext_metrics['mean_newline_count']:.1f}\\\\n\")\n",
    "\n",
    "# Analyze each prompt variation\n",
    "for pv in prompt_variations:\n",
    "    pv_data = merged_df[merged_df['prompt_variation'] == pv]\n",
    "    pv_generated = pv_data[pv_data['entry_type'] == 'generated']\n",
    "    \n",
    "    if len(pv_generated) > 0:\n",
    "        ext_metrics = calculate_extended_text_metrics(pv_generated)\n",
    "        ext_metrics['prompt_variation'] = pv\n",
    "        extended_analysis_results.append(ext_metrics)\n",
    "        print(f\"{pv} - Samples: {len(pv_generated)}\")\n",
    "        print(f\"  Mean words: {ext_metrics['mean_word_count']:.1f}\")\n",
    "        print(f\"  Mean sentences: {ext_metrics['mean_sentence_count']:.1f}\")\n",
    "        print(f\"  Mean newlines: {ext_metrics['mean_newline_count']:.1f}\\\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "63d3ea97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COMPREHENSIVE ANALYSIS RESULTS ===\n",
      "\n",
      "Final Table with Textual Features and AI Probability Scores:\n",
      "====================================================================================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_variation</th>\n",
       "      <th>mean_prompt_tokens</th>\n",
       "      <th>median_prompt_tokens</th>\n",
       "      <th>mean_completion_tokens</th>\n",
       "      <th>median_completion_tokens</th>\n",
       "      <th>mean_description_length</th>\n",
       "      <th>median_description_length</th>\n",
       "      <th>mean_word_count</th>\n",
       "      <th>median_word_count</th>\n",
       "      <th>mean_sentence_count</th>\n",
       "      <th>median_sentence_count</th>\n",
       "      <th>true_positive_pct</th>\n",
       "      <th>false_negative_pct</th>\n",
       "      <th>mean_ai_score_generated</th>\n",
       "      <th>median_ai_score_generated</th>\n",
       "      <th>mean_ai_score_original</th>\n",
       "      <th>median_ai_score_original</th>\n",
       "      <th>total_samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Original</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1184.0</td>\n",
       "      <td>1187.5</td>\n",
       "      <td>160.7</td>\n",
       "      <td>169.0</td>\n",
       "      <td>19.8</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P-10_Full_Plus_One_Shot</td>\n",
       "      <td>64351.4</td>\n",
       "      <td>28815.5</td>\n",
       "      <td>557.1</td>\n",
       "      <td>303.5</td>\n",
       "      <td>1112.8</td>\n",
       "      <td>983.0</td>\n",
       "      <td>159.9</td>\n",
       "      <td>140.0</td>\n",
       "      <td>9.8</td>\n",
       "      <td>9.5</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>16.8</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P-11_Full_Plus_Few_Shot</td>\n",
       "      <td>74741.9</td>\n",
       "      <td>35300.0</td>\n",
       "      <td>451.0</td>\n",
       "      <td>246.5</td>\n",
       "      <td>922.2</td>\n",
       "      <td>761.5</td>\n",
       "      <td>132.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>8.2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.5</td>\n",
       "      <td>18.1</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P-1_Minimal</td>\n",
       "      <td>632.5</td>\n",
       "      <td>365.0</td>\n",
       "      <td>189.8</td>\n",
       "      <td>85.5</td>\n",
       "      <td>500.2</td>\n",
       "      <td>373.5</td>\n",
       "      <td>62.0</td>\n",
       "      <td>46.5</td>\n",
       "      <td>6.2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P-2_Basic</td>\n",
       "      <td>681.2</td>\n",
       "      <td>412.5</td>\n",
       "      <td>212.5</td>\n",
       "      <td>117.5</td>\n",
       "      <td>511.4</td>\n",
       "      <td>396.0</td>\n",
       "      <td>67.8</td>\n",
       "      <td>62.0</td>\n",
       "      <td>5.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>P-3_Diffs_Only</td>\n",
       "      <td>5574.5</td>\n",
       "      <td>2129.0</td>\n",
       "      <td>288.8</td>\n",
       "      <td>155.0</td>\n",
       "      <td>712.5</td>\n",
       "      <td>480.5</td>\n",
       "      <td>93.9</td>\n",
       "      <td>65.5</td>\n",
       "      <td>6.7</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>P-4_Diffs_Plus_Title</td>\n",
       "      <td>5619.8</td>\n",
       "      <td>2143.5</td>\n",
       "      <td>303.1</td>\n",
       "      <td>167.0</td>\n",
       "      <td>698.6</td>\n",
       "      <td>513.0</td>\n",
       "      <td>94.4</td>\n",
       "      <td>71.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>P-5_Code_Only</td>\n",
       "      <td>56585.1</td>\n",
       "      <td>24574.5</td>\n",
       "      <td>348.1</td>\n",
       "      <td>174.5</td>\n",
       "      <td>806.5</td>\n",
       "      <td>658.5</td>\n",
       "      <td>109.9</td>\n",
       "      <td>86.5</td>\n",
       "      <td>6.9</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>P-6_Issue_Only</td>\n",
       "      <td>2109.8</td>\n",
       "      <td>882.5</td>\n",
       "      <td>345.1</td>\n",
       "      <td>181.5</td>\n",
       "      <td>669.0</td>\n",
       "      <td>575.0</td>\n",
       "      <td>94.2</td>\n",
       "      <td>85.5</td>\n",
       "      <td>6.4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>P-7_Template_Plus_Title</td>\n",
       "      <td>872.8</td>\n",
       "      <td>525.0</td>\n",
       "      <td>326.9</td>\n",
       "      <td>175.5</td>\n",
       "      <td>725.7</td>\n",
       "      <td>604.0</td>\n",
       "      <td>98.7</td>\n",
       "      <td>85.5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>P-8_Full_Context</td>\n",
       "      <td>58098.7</td>\n",
       "      <td>24909.0</td>\n",
       "      <td>450.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>958.5</td>\n",
       "      <td>816.5</td>\n",
       "      <td>134.7</td>\n",
       "      <td>116.5</td>\n",
       "      <td>8.3</td>\n",
       "      <td>7.5</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>P-9_Basic_One_Shot</td>\n",
       "      <td>2154.0</td>\n",
       "      <td>1318.0</td>\n",
       "      <td>310.4</td>\n",
       "      <td>170.5</td>\n",
       "      <td>720.2</td>\n",
       "      <td>650.0</td>\n",
       "      <td>95.9</td>\n",
       "      <td>95.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.9</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           prompt_variation  mean_prompt_tokens  median_prompt_tokens  \\\n",
       "0                  Original                 0.0                   0.0   \n",
       "1   P-10_Full_Plus_One_Shot             64351.4               28815.5   \n",
       "2   P-11_Full_Plus_Few_Shot             74741.9               35300.0   \n",
       "3               P-1_Minimal               632.5                 365.0   \n",
       "4                 P-2_Basic               681.2                 412.5   \n",
       "5            P-3_Diffs_Only              5574.5                2129.0   \n",
       "6      P-4_Diffs_Plus_Title              5619.8                2143.5   \n",
       "7             P-5_Code_Only             56585.1               24574.5   \n",
       "8            P-6_Issue_Only              2109.8                 882.5   \n",
       "9   P-7_Template_Plus_Title               872.8                 525.0   \n",
       "10         P-8_Full_Context             58098.7               24909.0   \n",
       "11       P-9_Basic_One_Shot              2154.0                1318.0   \n",
       "\n",
       "    mean_completion_tokens  median_completion_tokens  mean_description_length  \\\n",
       "0                      0.0                       0.0                   1184.0   \n",
       "1                    557.1                     303.5                   1112.8   \n",
       "2                    451.0                     246.5                    922.2   \n",
       "3                    189.8                      85.5                    500.2   \n",
       "4                    212.5                     117.5                    511.4   \n",
       "5                    288.8                     155.0                    712.5   \n",
       "6                    303.1                     167.0                    698.6   \n",
       "7                    348.1                     174.5                    806.5   \n",
       "8                    345.1                     181.5                    669.0   \n",
       "9                    326.9                     175.5                    725.7   \n",
       "10                   450.0                     227.0                    958.5   \n",
       "11                   310.4                     170.5                    720.2   \n",
       "\n",
       "    median_description_length  mean_word_count  median_word_count  \\\n",
       "0                      1187.5            160.7              169.0   \n",
       "1                       983.0            159.9              140.0   \n",
       "2                       761.5            132.0              116.0   \n",
       "3                       373.5             62.0               46.5   \n",
       "4                       396.0             67.8               62.0   \n",
       "5                       480.5             93.9               65.5   \n",
       "6                       513.0             94.4               71.5   \n",
       "7                       658.5            109.9               86.5   \n",
       "8                       575.0             94.2               85.5   \n",
       "9                       604.0             98.7               85.5   \n",
       "10                      816.5            134.7              116.5   \n",
       "11                      650.0             95.9               95.0   \n",
       "\n",
       "    mean_sentence_count  median_sentence_count  true_positive_pct  \\\n",
       "0                  19.8                   19.0                0.0   \n",
       "1                   9.8                    9.5               28.1   \n",
       "2                   8.2                    7.0               15.6   \n",
       "3                   6.2                    5.0                3.1   \n",
       "4                   5.3                    5.0                6.2   \n",
       "5                   6.7                    6.0                6.2   \n",
       "6                   6.5                    5.0                3.1   \n",
       "7                   6.9                    6.0               12.5   \n",
       "8                   6.4                    6.0               12.5   \n",
       "9                   8.0                    8.0                3.1   \n",
       "10                  8.3                    7.5                3.1   \n",
       "11                  7.0                    6.0               28.1   \n",
       "\n",
       "    false_negative_pct  mean_ai_score_generated  median_ai_score_generated  \\\n",
       "0                  0.0                      0.0                        0.0   \n",
       "1                  0.0                     27.0                       16.8   \n",
       "2                  0.0                     22.5                       18.1   \n",
       "3                  0.0                      4.7                        0.0   \n",
       "4                  0.0                      9.3                        0.0   \n",
       "5                  0.0                      6.8                        0.0   \n",
       "6                  0.0                      9.9                        0.0   \n",
       "7                  0.0                     14.4                        0.0   \n",
       "8                  0.0                     17.4                        0.0   \n",
       "9                  0.0                     13.2                        0.0   \n",
       "10                 0.0                     12.8                        0.0   \n",
       "11                 0.0                     27.9                        6.0   \n",
       "\n",
       "    mean_ai_score_original  median_ai_score_original  total_samples  \n",
       "0                     13.5                       9.9            352  \n",
       "1                     13.5                       9.9             64  \n",
       "2                     13.5                       9.9             64  \n",
       "3                     13.5                       9.9             64  \n",
       "4                     13.5                       9.9             64  \n",
       "5                     13.5                       9.9             64  \n",
       "6                     13.5                       9.9             64  \n",
       "7                     13.5                       9.9             64  \n",
       "8                     13.5                       9.9             64  \n",
       "9                     13.5                       9.9             64  \n",
       "10                    13.5                       9.9             64  \n",
       "11                    13.5                       9.9             64  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create extended results DataFrame\n",
    "extended_results_df = pd.DataFrame(extended_analysis_results)\n",
    "\n",
    "# Merge with original results\n",
    "final_results = pd.merge(results_df, extended_results_df, on='prompt_variation', how='left')\n",
    "\n",
    "# Reorder columns for the final comprehensive table\n",
    "final_column_order = [\n",
    "    'prompt_variation',\n",
    "    'mean_prompt_tokens',\n",
    "    'median_prompt_tokens', \n",
    "    'mean_completion_tokens',\n",
    "    'median_completion_tokens',\n",
    "    'mean_description_length',\n",
    "    'median_description_length',\n",
    "    'mean_word_count',\n",
    "    'median_word_count',\n",
    "    'mean_sentence_count',\n",
    "    'median_sentence_count',\n",
    "    'true_positive_pct',\n",
    "    'false_negative_pct',\n",
    "    'mean_ai_score_generated',\n",
    "    'median_ai_score_generated',\n",
    "    'mean_ai_score_original',\n",
    "    'median_ai_score_original',\n",
    "    'total_samples'\n",
    "]\n",
    "\n",
    "final_results = final_results[final_column_order]\n",
    "\n",
    "# Round numeric columns for better display\n",
    "numeric_cols = [col for col in final_results.columns if col != 'prompt_variation']\n",
    "final_results[numeric_cols] = final_results[numeric_cols].round(1)\n",
    "\n",
    "print(\"\\n=== COMPREHENSIVE ANALYSIS RESULTS ===\\n\")\n",
    "print(\"Final Table with Textual Features and AI Probability Scores:\")\n",
    "print(\"=\" * 180)\n",
    "\n",
    "# Display the table\n",
    "final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "afad6542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comprehensive results saved to: comprehensive_prompt_variations_analysis.csv\n",
      "\n",
      "=== FINAL COMPREHENSIVE SUMMARY ===\n",
      "Total prompt variations analyzed: 12\n",
      "\n",
      "Best detection performance: P-10_Full_Plus_One_Shot (28.1% true positive rate)\n",
      "Most efficient (lowest tokens): P-1_Minimal (632 mean prompt tokens)\n",
      "Longest descriptions: Original (1184 chars)\n",
      "Most words per description: Original (160.7 words)\n",
      "Highest AI detectability: P-9_Basic_One_Shot (27.9% mean AI score)\n",
      "\n",
      "=== KEY INSIGHTS ===\n",
      "1. Detection Performance:\n",
      "   - Average true positive rate across all prompts: 11.1%\n",
      "   - Best performing prompts: P-10_Full_Plus_One_Shot, P-9_Basic_One_Shot, P-11_Full_Plus_Few_Shot\n",
      "   - Average AI score for generated content: 15.1%\n",
      "   - Highest AI scores: P-9_Basic_One_Shot, P-10_Full_Plus_One_Shot, P-11_Full_Plus_Few_Shot\n",
      "\n",
      "2. Token Efficiency:\n",
      "   - Most token-efficient prompts: P-1_Minimal, P-2_Basic, P-7_Template_Plus_Title\n",
      "   - Average prompt tokens: 24675\n",
      "\n",
      "3. Content Quality:\n",
      "   - Original descriptions average 161 words, AI score: 13.5%\n",
      "   - Generated descriptions range from 62 to 160 words\n",
      "   - Generated AI scores range from 4.7% to 27.9%\n",
      "   - Average generated content length: 104 words\n",
      "\n",
      "4. AI Detection Score Analysis:\n",
      "   - Original (human) content: 13.5% mean AI score (all below 50% threshold)\n",
      "   - Generated content: 15.1% average mean AI score\n",
      "   - Detection gap: 1.6 percentage points higher for AI content\n",
      "   - Best performing prompts: P-10_Full_Plus_One_Shot, P-9_Basic_One_Shot, P-11_Full_Plus_Few_Shot\n",
      "   - Average AI score for generated content: 15.1%\n",
      "   - Highest AI scores: P-9_Basic_One_Shot, P-10_Full_Plus_One_Shot, P-11_Full_Plus_Few_Shot\n",
      "\n",
      "2. Token Efficiency:\n",
      "   - Most token-efficient prompts: P-1_Minimal, P-2_Basic, P-7_Template_Plus_Title\n",
      "   - Average prompt tokens: 24675\n",
      "\n",
      "3. Content Quality:\n",
      "   - Original descriptions average 161 words, AI score: 13.5%\n",
      "   - Generated descriptions range from 62 to 160 words\n",
      "   - Generated AI scores range from 4.7% to 27.9%\n",
      "   - Average generated content length: 104 words\n",
      "\n",
      "4. AI Detection Score Analysis:\n",
      "   - Original (human) content: 13.5% mean AI score (all below 50% threshold)\n",
      "   - Generated content: 15.1% average mean AI score\n",
      "   - Detection gap: 1.6 percentage points higher for AI content\n"
     ]
    }
   ],
   "source": [
    "# Save comprehensive results\n",
    "comprehensive_output_file = \"comprehensive_prompt_variations_analysis.csv\"\n",
    "final_results.to_csv(comprehensive_output_file, index=False)\n",
    "print(f\"\\nComprehensive results saved to: {comprehensive_output_file}\")\n",
    "\n",
    "# Final comprehensive summary\n",
    "print(\"\\n=== FINAL COMPREHENSIVE SUMMARY ===\")\n",
    "print(f\"Total prompt variations analyzed: {len(final_results)}\")\n",
    "\n",
    "# Best performers\n",
    "best_detection = final_results.loc[final_results['true_positive_pct'].idxmax()]\n",
    "most_efficient = final_results[final_results['prompt_variation'] != 'Original'].loc[final_results[final_results['prompt_variation'] != 'Original']['mean_prompt_tokens'].idxmin()]\n",
    "longest_content = final_results.loc[final_results['mean_description_length'].idxmax()]\n",
    "most_words = final_results.loc[final_results['mean_word_count'].idxmax()]\n",
    "highest_ai_score = final_results.loc[final_results['mean_ai_score_generated'].idxmax()]\n",
    "\n",
    "print(f\"\\nBest detection performance: {best_detection['prompt_variation']} ({best_detection['true_positive_pct']:.1f}% true positive rate)\")\n",
    "print(f\"Most efficient (lowest tokens): {most_efficient['prompt_variation']} ({most_efficient['mean_prompt_tokens']:.0f} mean prompt tokens)\")\n",
    "print(f\"Longest descriptions: {longest_content['prompt_variation']} ({longest_content['mean_description_length']:.0f} chars)\")\n",
    "print(f\"Most words per description: {most_words['prompt_variation']} ({most_words['mean_word_count']:.1f} words)\")\n",
    "print(f\"Highest AI detectability: {highest_ai_score['prompt_variation']} ({highest_ai_score['mean_ai_score_generated']:.1f}% mean AI score)\")\n",
    "\n",
    "print(\"\\n=== KEY INSIGHTS ===\")\n",
    "print(\"1. Detection Performance:\")\n",
    "generated_only = final_results[final_results['prompt_variation'] != 'Original']\n",
    "print(f\"   - Average true positive rate across all prompts: {generated_only['true_positive_pct'].mean():.1f}%\")\n",
    "print(f\"   - Best performing prompts: {', '.join(generated_only.nlargest(3, 'true_positive_pct')['prompt_variation'].tolist())}\")\n",
    "print(f\"   - Average AI score for generated content: {generated_only['mean_ai_score_generated'].mean():.1f}%\")\n",
    "print(f\"   - Highest AI scores: {', '.join(generated_only.nlargest(3, 'mean_ai_score_generated')['prompt_variation'].tolist())}\")\n",
    "\n",
    "print(\"\\n2. Token Efficiency:\")\n",
    "print(f\"   - Most token-efficient prompts: {', '.join(generated_only.nsmallest(3, 'mean_prompt_tokens')['prompt_variation'].tolist())}\")\n",
    "print(f\"   - Average prompt tokens: {generated_only['mean_prompt_tokens'].mean():.0f}\")\n",
    "\n",
    "print(\"\\n3. Content Quality:\")\n",
    "original_row = final_results[final_results['prompt_variation'] == 'Original']\n",
    "print(f\"   - Original descriptions average {original_row['mean_word_count'].iloc[0]:.0f} words, AI score: {original_row['mean_ai_score_original'].iloc[0]:.1f}%\")\n",
    "print(f\"   - Generated descriptions range from {generated_only['mean_word_count'].min():.0f} to {generated_only['mean_word_count'].max():.0f} words\")\n",
    "print(f\"   - Generated AI scores range from {generated_only['mean_ai_score_generated'].min():.1f}% to {generated_only['mean_ai_score_generated'].max():.1f}%\")\n",
    "print(f\"   - Average generated content length: {generated_only['mean_word_count'].mean():.0f} words\")\n",
    "\n",
    "print(\"\\n4. AI Detection Score Analysis:\")\n",
    "print(f\"   - Original (human) content: {original_row['mean_ai_score_original'].iloc[0]:.1f}% mean AI score (all below 50% threshold)\")\n",
    "print(f\"   - Generated content: {generated_only['mean_ai_score_generated'].mean():.1f}% average mean AI score\")\n",
    "print(f\"   - Detection gap: {generated_only['mean_ai_score_generated'].mean() - original_row['mean_ai_score_original'].iloc[0]:.1f} percentage points higher for AI content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2def9ce",
   "metadata": {},
   "source": [
    "## Debug: Understanding Original Detection Metrics\n",
    "\n",
    "Let's examine why the Original descriptions show 0.0 for both true_positive_pct and false_negative_pct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e3b80844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DEBUGGING ORIGINAL DETECTION METRICS ===\n",
      "\n",
      "1. Original data shape: (352, 31)\n",
      "   Entry types: entry_type\n",
      "original    352\n",
      "Name: count, dtype: int64\n",
      "\n",
      "2. ZeroGPT response availability for Original data:\n",
      "   Non-null responses: 352\n",
      "   Null/empty responses: 0\n",
      "\n",
      "3. Sample AI probabilities for Original descriptions:\n",
      "   Sample 0: AI probability = 0.0\n",
      "   Sample 2: AI probability = 0.0\n",
      "   Sample 4: AI probability = 0.0\n",
      "   Sample 6: AI probability = 0.0\n",
      "   Sample 8: AI probability = 0.0\n",
      "\n",
      "4. Running detection metrics calculation for Original:\n",
      "   AI probabilities parsed: 352 out of 352\n",
      "   AI probability range: 0.0 to 48.4\n",
      "   Valid samples for analysis: 352\n",
      "   Using AI threshold: 50.0%\n",
      "   Detected as AI: 0 out of 352\n",
      "   Generated entries: 0 (detected as AI: 0)\n",
      "   Original entries: 352 (detected as AI: 0)\n",
      "   True Positive Rate: 0.0% (Generated correctly identified as AI)\n",
      "   False Positive Rate: 0.0% (Original incorrectly identified as AI)\n",
      "\n",
      "5. The issue explanation:\n",
      "   - For 'Original' analysis, we only have original (human-written) descriptions\n",
      "   - True Positive Rate = Generated content correctly identified as AI / Total Generated\n",
      "   - But we have 0 generated entries in the 'Original' dataset\n",
      "   - False Negative Rate = Original content incorrectly identified as AI / Total Original\n",
      "   - This should show the misclassification rate of human content\n"
     ]
    }
   ],
   "source": [
    "# Debug: Let's examine the Original descriptions detection logic step by step\n",
    "print(\"=== DEBUGGING ORIGINAL DETECTION METRICS ===\\n\")\n",
    "\n",
    "# Get original data again\n",
    "original_data = merged_df[merged_df['entry_type'] == 'original']\n",
    "print(f\"1. Original data shape: {original_data.shape}\")\n",
    "print(f\"   Entry types: {original_data['entry_type'].value_counts()}\")\n",
    "\n",
    "# Check zerogpt responses for original data\n",
    "print(f\"\\n2. ZeroGPT response availability for Original data:\")\n",
    "print(f\"   Non-null responses: {original_data['zerogpt_response'].notna().sum()}\")\n",
    "print(f\"   Null/empty responses: {original_data['zerogpt_response'].isna().sum()}\")\n",
    "\n",
    "# Try parsing a few responses manually\n",
    "print(f\"\\n3. Sample AI probabilities for Original descriptions:\")\n",
    "sample_original = original_data.head(5)\n",
    "for idx, row in sample_original.iterrows():\n",
    "    ai_prob = parse_zerogpt_response(row['zerogpt_response'])\n",
    "    print(f\"   Sample {idx}: AI probability = {ai_prob}\")\n",
    "\n",
    "# Now let's run the detection metrics function step by step\n",
    "print(f\"\\n4. Running detection metrics calculation for Original:\")\n",
    "\n",
    "# Copy the function logic but with debug prints\n",
    "original_debug = original_data.copy()\n",
    "original_debug['ai_probability'] = original_debug['zerogpt_response'].apply(parse_zerogpt_response)\n",
    "\n",
    "print(f\"   AI probabilities parsed: {original_debug['ai_probability'].notna().sum()} out of {len(original_debug)}\")\n",
    "print(f\"   AI probability range: {original_debug['ai_probability'].min():.1f} to {original_debug['ai_probability'].max():.1f}\")\n",
    "\n",
    "valid_df = original_debug[original_debug['ai_probability'].notna()].copy()\n",
    "print(f\"   Valid samples for analysis: {len(valid_df)}\")\n",
    "\n",
    "if len(valid_df) > 0:\n",
    "    # Check detection threshold\n",
    "    ai_threshold = 50.0\n",
    "    valid_df['detected_as_ai'] = valid_df['ai_probability'] > ai_threshold\n",
    "    print(f\"   Using AI threshold: {ai_threshold}%\")\n",
    "    print(f\"   Detected as AI: {valid_df['detected_as_ai'].sum()} out of {len(valid_df)}\")\n",
    "    \n",
    "    # Break down by entry type\n",
    "    generated_entries = valid_df[valid_df['entry_type'] == 'generated']\n",
    "    original_entries = valid_df[valid_df['entry_type'] == 'original']\n",
    "    \n",
    "    print(f\"   Generated entries: {len(generated_entries)} (detected as AI: {generated_entries['detected_as_ai'].sum() if len(generated_entries) > 0 else 0})\")\n",
    "    print(f\"   Original entries: {len(original_entries)} (detected as AI: {original_entries['detected_as_ai'].sum() if len(original_entries) > 0 else 0})\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    tp_rate = 0\n",
    "    if len(generated_entries) > 0:\n",
    "        tp_rate = (generated_entries['detected_as_ai'].sum() / len(generated_entries)) * 100\n",
    "    \n",
    "    fp_rate = 0  \n",
    "    if len(original_entries) > 0:\n",
    "        fp_rate = (original_entries['detected_as_ai'].sum() / len(original_entries)) * 100\n",
    "    \n",
    "    print(f\"   True Positive Rate: {tp_rate:.1f}% (Generated correctly identified as AI)\")\n",
    "    print(f\"   False Positive Rate: {fp_rate:.1f}% (Original incorrectly identified as AI)\")\n",
    "    \n",
    "else:\n",
    "    print(\"   No valid samples to analyze!\")\n",
    "\n",
    "print(f\"\\n5. The issue explanation:\")\n",
    "print(f\"   - For 'Original' analysis, we only have original (human-written) descriptions\")\n",
    "print(f\"   - True Positive Rate = Generated content correctly identified as AI / Total Generated\")\n",
    "print(f\"   - But we have 0 generated entries in the 'Original' dataset\")\n",
    "print(f\"   - False Negative Rate = Original content incorrectly identified as AI / Total Original\")\n",
    "print(f\"   - This should show the misclassification rate of human content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "503dc05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n=== EXPLANATION OF ORIGINAL DETECTION METRICS ===\\n\n",
      "Why Original shows 0.0 for both metrics:\n",
      "\\n1. **True Positive Rate (0.0%)**:\n",
      "   - This measures: Generated content correctly identified as AI\n",
      "   - For 'Original' row: We have 0 generated entries (only human-written content)\n",
      "   - So: 0 generated entries detected as AI / 0 total generated entries = 0/0 = 0%\n",
      "   - This is correct but not meaningful for the Original baseline\n",
      "\\n2. **False Negative Rate (0.0%)**:\n",
      "   - This measures: Original (human) content incorrectly identified as AI\n",
      "   - For 'Original' row: 0 out of 352 human descriptions were misclassified as AI\n",
      "   - So: 0 misclassified / 352 total human descriptions = 0%\n",
      "   - This means the detector correctly identified ALL original content as human-written!\n",
      "\\n=== CORRECTED INTERPRETATION ===\\n\n",
      "The Original row should be interpreted as:\n",
      "- True Positive Rate: N/A (no generated content to detect)\n",
      "- **Human Content Accuracy: 100%** (all 352 original descriptions correctly identified as human)\n",
      "- **AI Probability range: 0.0% to 48.4%** (all below 50% threshold)\n",
      "\\n=== ORIGINAL CONTENT DETECTION STATISTICS ===\n",
      "Total original descriptions analyzed: 352\n",
      "AI probability statistics:\n",
      "  Mean: 13.5%\n",
      "  Median: 9.9%\n",
      "  Std Dev: 14.7%\n",
      "  Max: 48.4%\n",
      "  Min: 0.0%\n",
      "\\nDistribution of AI probabilities:\n",
      "  0-10%: 176 descriptions\n",
      "  10-20%: 66 descriptions\n",
      "  20-30%: 55 descriptions\n",
      "  30-40%: 44 descriptions\n",
      "  40-50%: 11 descriptions\n",
      "  Above 50%: 0 descriptions (would be misclassified)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\\\n=== EXPLANATION OF ORIGINAL DETECTION METRICS ===\\\\n\")\n",
    "\n",
    "print(\"Why Original shows 0.0 for both metrics:\")\n",
    "print(\"\\\\n1. **True Positive Rate (0.0%)**:\")\n",
    "print(\"   - This measures: Generated content correctly identified as AI\")\n",
    "print(\"   - For 'Original' row: We have 0 generated entries (only human-written content)\")\n",
    "print(\"   - So: 0 generated entries detected as AI / 0 total generated entries = 0/0 = 0%\")\n",
    "print(\"   - This is correct but not meaningful for the Original baseline\")\n",
    "\n",
    "print(\"\\\\n2. **False Negative Rate (0.0%)**:\")  \n",
    "print(\"   - This measures: Original (human) content incorrectly identified as AI\")\n",
    "print(\"   - For 'Original' row: 0 out of 352 human descriptions were misclassified as AI\")\n",
    "print(\"   - So: 0 misclassified / 352 total human descriptions = 0%\")\n",
    "print(\"   - This means the detector correctly identified ALL original content as human-written!\")\n",
    "\n",
    "print(\"\\\\n=== CORRECTED INTERPRETATION ===\\\\n\")\n",
    "print(\"The Original row should be interpreted as:\")\n",
    "print(\"- True Positive Rate: N/A (no generated content to detect)\")\n",
    "print(\"- **Human Content Accuracy: 100%** (all 352 original descriptions correctly identified as human)\")\n",
    "print(\"- **AI Probability range: 0.0% to 48.4%** (all below 50% threshold)\")\n",
    "\n",
    "# Let's also check a few more statistics for Original\n",
    "original_debug = merged_df[merged_df['entry_type'] == 'original'].copy()\n",
    "original_debug['ai_probability'] = original_debug['zerogpt_response'].apply(parse_zerogpt_response)\n",
    "\n",
    "print(f\"\\\\n=== ORIGINAL CONTENT DETECTION STATISTICS ===\")\n",
    "print(f\"Total original descriptions analyzed: {len(original_debug)}\")\n",
    "print(f\"AI probability statistics:\")\n",
    "print(f\"  Mean: {original_debug['ai_probability'].mean():.1f}%\")\n",
    "print(f\"  Median: {original_debug['ai_probability'].median():.1f}%\")\n",
    "print(f\"  Std Dev: {original_debug['ai_probability'].std():.1f}%\")\n",
    "print(f\"  Max: {original_debug['ai_probability'].max():.1f}%\")\n",
    "print(f\"  Min: {original_debug['ai_probability'].min():.1f}%\")\n",
    "print(f\"\\\\nDistribution of AI probabilities:\")\n",
    "print(f\"  0-10%: {((original_debug['ai_probability'] >= 0) & (original_debug['ai_probability'] < 10)).sum()} descriptions\")\n",
    "print(f\"  10-20%: {((original_debug['ai_probability'] >= 10) & (original_debug['ai_probability'] < 20)).sum()} descriptions\")\n",
    "print(f\"  20-30%: {((original_debug['ai_probability'] >= 20) & (original_debug['ai_probability'] < 30)).sum()} descriptions\")\n",
    "print(f\"  30-40%: {((original_debug['ai_probability'] >= 30) & (original_debug['ai_probability'] < 40)).sum()} descriptions\")\n",
    "print(f\"  40-50%: {((original_debug['ai_probability'] >= 40) & (original_debug['ai_probability'] < 50)).sum()} descriptions\")\n",
    "print(f\"  Above 50%: {(original_debug['ai_probability'] >= 50).sum()} descriptions (would be misclassified)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3578682a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VERIFICATION: ZeroGPT RESPONSES FOR ORIGINAL ENTRIES ===\n",
      "\n",
      "1. Checking detection CSV structure:\n",
      "   Total entries in detection file: 704\n",
      "   Entry types: entry_type\n",
      "original     352\n",
      "generated    352\n",
      "Name: count, dtype: int64\n",
      "\n",
      "2. Original entries in detection file:\n",
      "   Count: 352\n",
      "   Non-null zerogpt_response: 352\n",
      "   Sample ZeroGPT responses for original entries:\n",
      "     Sample 1: AI probability = 17.46% | Response: {\"success\": true, \"code\": 200, \"message\": \"detection result passed to proxy\", \"data\": {\"sentences\": ...\n",
      "     Sample 2: AI probability = 26.09% | Response: {\"success\": true, \"code\": 200, \"message\": \"detection result passed to proxy\", \"data\": {\"sentences\": ...\n",
      "     Sample 3: AI probability = 29.56% | Response: {\"success\": true, \"code\": 200, \"message\": \"detection result passed to proxy\", \"data\": {\"sentences\": ...\n",
      "\n",
      "3. After merging with generation data:\n",
      "   Count: 352\n",
      "   AI probabilities calculated:\n",
      "     Mean: 13.5%\n",
      "     Max: 48.4%\n",
      "     Above 50% threshold: 0\n",
      "\n",
      "4. IMPORTANT INSIGHT:\n",
      "   ✅ The false_negative_pct is CORRECTLY 0.0% because:\n",
      "   ✅ ALL 352 original descriptions scored below 50% AI probability\n",
      "   ✅ This means ZeroGPT correctly identified ALL original content as human-written\n",
      "   ✅ No original content was misclassified as AI-generated\n"
     ]
    }
   ],
   "source": [
    "# VERIFY: Let's confirm the ZeroGPT responses for original entries are being used\n",
    "print(\"=== VERIFICATION: ZeroGPT RESPONSES FOR ORIGINAL ENTRIES ===\\n\")\n",
    "\n",
    "# Check the detection CSV file directly\n",
    "print(\"1. Checking detection CSV structure:\")\n",
    "print(f\"   Total entries in detection file: {len(detection_df)}\")\n",
    "print(f\"   Entry types: {detection_df['entry_type'].value_counts()}\")\n",
    "\n",
    "# Check original entries specifically\n",
    "original_detection_entries = detection_df[detection_df['entry_type'] == 'original']\n",
    "print(f\"\\n2. Original entries in detection file:\")\n",
    "print(f\"   Count: {len(original_detection_entries)}\")\n",
    "print(f\"   Non-null zerogpt_response: {original_detection_entries['zerogpt_response'].notna().sum()}\")\n",
    "print(f\"   Sample ZeroGPT responses for original entries:\")\n",
    "\n",
    "# Parse a few original entry responses\n",
    "for i, (idx, row) in enumerate(original_detection_entries.head(3).iterrows()):\n",
    "    response_sample = row['zerogpt_response'][:100] if isinstance(row['zerogpt_response'], str) else str(row['zerogpt_response'])\n",
    "    ai_prob = parse_zerogpt_response(row['zerogpt_response'])\n",
    "    print(f\"     Sample {i+1}: AI probability = {ai_prob}% | Response: {response_sample}...\")\n",
    "\n",
    "# Now let's check what happens when we merge with generation data\n",
    "print(f\"\\n3. After merging with generation data:\")\n",
    "original_merged = merged_df[merged_df['entry_type'] == 'original']\n",
    "print(f\"   Count: {len(original_merged)}\")\n",
    "print(f\"   AI probabilities calculated:\")\n",
    "original_merged_copy = original_merged.copy()\n",
    "original_merged_copy['ai_probability'] = original_merged_copy['zerogpt_response'].apply(parse_zerogpt_response)\n",
    "print(f\"     Mean: {original_merged_copy['ai_probability'].mean():.1f}%\")\n",
    "print(f\"     Max: {original_merged_copy['ai_probability'].max():.1f}%\")\n",
    "print(f\"     Above 50% threshold: {(original_merged_copy['ai_probability'] > 50).sum()}\")\n",
    "\n",
    "print(f\"\\n4. IMPORTANT INSIGHT:\")\n",
    "if (original_merged_copy['ai_probability'] > 50).sum() == 0:\n",
    "    print(f\"   ✅ The false_negative_pct is CORRECTLY 0.0% because:\")\n",
    "    print(f\"   ✅ ALL {len(original_merged_copy)} original descriptions scored below 50% AI probability\")\n",
    "    print(f\"   ✅ This means ZeroGPT correctly identified ALL original content as human-written\")\n",
    "    print(f\"   ✅ No original content was misclassified as AI-generated\")\n",
    "else:\n",
    "    misclassified = (original_merged_copy['ai_probability'] > 50).sum()\n",
    "    total = len(original_merged_copy)\n",
    "    print(f\"   ❌ Something's wrong: {misclassified} out of {total} original entries scored > 50%\")\n",
    "    print(f\"   ❌ False negative rate should be: {(misclassified/total)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2ce8b2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FINAL CONFIRMATION OF DETECTION CALCULATION ===\n",
      "\n",
      "Manual calculation verification:\n",
      "1. Total original entries: 352\n",
      "2. AI probabilities successfully parsed: 352\n",
      "3. AI threshold used: 50.0%\n",
      "4. Entries above threshold (detected as AI): 0\n",
      "5. False Negative Rate calculation:\n",
      "   = Entries incorrectly identified as AI / Total original entries\n",
      "   = 0 / 352\n",
      "   = 0.0%\n",
      "\n",
      "6. Distribution of AI probabilities for original entries:\n",
      "   Min: 0.0%\n",
      "   25th percentile: 0.0%\n",
      "   Median: 9.9%\n",
      "   75th percentile: 25.7%\n",
      "   Max: 48.4%\n",
      "\n",
      "✅ CONFIRMATION: The ZeroGPT responses for original entries ARE being used!\n",
      "✅ The 0.0% false negative rate is CORRECT - it means perfect accuracy!\n",
      "✅ ZeroGPT successfully identified all original content as human-written.\n",
      "2. AI probabilities successfully parsed: 352\n",
      "3. AI threshold used: 50.0%\n",
      "4. Entries above threshold (detected as AI): 0\n",
      "5. False Negative Rate calculation:\n",
      "   = Entries incorrectly identified as AI / Total original entries\n",
      "   = 0 / 352\n",
      "   = 0.0%\n",
      "\n",
      "6. Distribution of AI probabilities for original entries:\n",
      "   Min: 0.0%\n",
      "   25th percentile: 0.0%\n",
      "   Median: 9.9%\n",
      "   75th percentile: 25.7%\n",
      "   Max: 48.4%\n",
      "\n",
      "✅ CONFIRMATION: The ZeroGPT responses for original entries ARE being used!\n",
      "✅ The 0.0% false negative rate is CORRECT - it means perfect accuracy!\n",
      "✅ ZeroGPT successfully identified all original content as human-written.\n"
     ]
    }
   ],
   "source": [
    "print(\"=== FINAL CONFIRMATION OF DETECTION CALCULATION ===\\n\")\n",
    "\n",
    "# Let's manually reproduce the exact calculation from our function\n",
    "original_for_verification = merged_df[merged_df['entry_type'] == 'original'].copy()\n",
    "original_for_verification['ai_probability'] = original_for_verification['zerogpt_response'].apply(parse_zerogpt_response)\n",
    "\n",
    "print(\"Manual calculation verification:\")\n",
    "print(f\"1. Total original entries: {len(original_for_verification)}\")\n",
    "print(f\"2. AI probabilities successfully parsed: {original_for_verification['ai_probability'].notna().sum()}\")\n",
    "print(f\"3. AI threshold used: 50.0%\")\n",
    "print(f\"4. Entries above threshold (detected as AI): {(original_for_verification['ai_probability'] > 50.0).sum()}\")\n",
    "print(f\"5. False Negative Rate calculation:\")\n",
    "print(f\"   = Entries incorrectly identified as AI / Total original entries\")\n",
    "print(f\"   = {(original_for_verification['ai_probability'] > 50.0).sum()} / {len(original_for_verification)}\")\n",
    "print(f\"   = {((original_for_verification['ai_probability'] > 50.0).sum() / len(original_for_verification)) * 100:.1f}%\")\n",
    "\n",
    "print(f\"\\n6. Distribution of AI probabilities for original entries:\")\n",
    "ai_probs = original_for_verification['ai_probability']\n",
    "print(f\"   Min: {ai_probs.min():.1f}%\")\n",
    "print(f\"   25th percentile: {ai_probs.quantile(0.25):.1f}%\") \n",
    "print(f\"   Median: {ai_probs.median():.1f}%\")\n",
    "print(f\"   75th percentile: {ai_probs.quantile(0.75):.1f}%\")\n",
    "print(f\"   Max: {ai_probs.max():.1f}%\")\n",
    "\n",
    "print(f\"\\n✅ CONFIRMATION: The ZeroGPT responses for original entries ARE being used!\")\n",
    "print(f\"✅ The 0.0% false negative rate is CORRECT - it means perfect accuracy!\")\n",
    "print(f\"✅ ZeroGPT successfully identified all original content as human-written.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245e2a8e",
   "metadata": {},
   "source": [
    "## Side-by-Side Description Comparison\n",
    "\n",
    "Let's create a side-by-side comparison of descriptions from all prompt variations plus the original for a single PR. This will be useful for presentation slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c0493b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SIDE-BY-SIDE DESCRIPTION COMPARISON ===\n",
      "\n",
      "PR coverage across prompt variations:\n",
      "Max variations per PR: 11\n",
      "PRs with most variations: pr_id\n",
      "MDExOlB1bGxSZXF1ZXN0MTYwNDk0NTcy    11\n",
      "MDExOlB1bGxSZXF1ZXN0MTYxNDI3MDY3    11\n",
      "PR_kwDOAQ0TF86g3kgZ                 11\n",
      "PR_kwDOAQ0TF86eA-bB                 11\n",
      "PR_kwDOAQ0TF86awU2h                 11\n",
      "Name: prompt_variation, dtype: int64\n",
      "\n",
      "Using PR ID: MDExOlB1bGxSZXF1ZXN0MTYxNDI3MDY3\n",
      "\n",
      "Found 22 descriptions for PR MDExOlB1bGxSZXF1ZXN0MTYxNDI3MDY3\n",
      "Entry types: ['generated' 'original']\n",
      "Prompt variations: ['P-10_Full_Plus_One_Shot', 'P-11_Full_Plus_Few_Shot', 'P-1_Minimal', 'P-2_Basic', 'P-3_Diffs_Only', 'P-4_Diffs_Plus_Title', 'P-5_Code_Only', 'P-6_Issue_Only', 'P-7_Template_Plus_Title', 'P-8_Full_Context', 'P-9_Basic_One_Shot']\n",
      "\n",
      "=== SIDE-BY-SIDE COMPARISON FOR PR MDExOlB1bGxSZXF1ZXN0MTYxNDI3MDY3 ===\n",
      "Variation | AI Score | Words | Description Preview\n",
      "========================================================================================================================\n",
      "Original (Human)          |    0.0% |    90 | <!-- describe the changes you have made here: what, why, ... -->\n",
      "The bug mentioned in #3145 was finally fixed in controlsfx. Thus we can reenable the validation messages.\n",
      "Closes #3145.\n",
      "\n",
      "----\n",
      "\n",
      "- [x] Change in CHANGELOG.md described\n",
      "- [ ] Tests created for changes\n",
      "- [ ] Screenshots added (for bigger UI changes)\n",
      "- [x] Manually tested changed features in running JabRef\n",
      "- [ ] Check documentation status (Issue created for outdated help page at [help.jabref.org](https://github.com/JabRef/help.jabref.or...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "P-10 Full Plus One Shot   |   26.9% |   196 | This PR fixes issue #3145 by correcting the position and display of validation messages in the entry editor. Previously, validation messages were shown shifted to the bottom by a fixed offset, resulting in a poor user experience. The fix includes the implementation of a new IconValidationDecorator class that uses font-based icons to display validation icons and tooltips at the correct position. This decorator is integrated into the EditorValidator class to show validation messages with appropria...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "P-11 Full Plus Few Shot   |    0.0% |   185 | Fixes issue #3145 by correcting the position and display of validation messages in the entry editor. Previously, validation messages were shown at an incorrect position, shifted to the bottom due to a fixed constant, resulting in poor user experience. This PR introduces a new validation decorator that displays validation messages with a small notification icon at the bottom left of the entry editor fields, using font-based icons with appropriate styling and tooltips for errors and warnings. The ...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "P-1 Minimal               |    0.0% |    82 | Introduce IconValidationDecorator to enhance validation feedback with icons in the entry editor. Update EntryEditorPrefsTab to support new validation features and improve user preferences management. Modify EditorValidator and IconTheme to integrate with the new decorator. Enhance EntryEditor.css with additional styles for validation icons. Refactored JabRefPreferences.java with minor code improvements to enhance code clarity and maintainability. Added two new entries to the English localization...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "P-2 Basic                 |    0.0% |    92 | This PR implements issue #3145 by adding support for displaying validation messages directly within the entry editor. It introduces a new IconValidationDecorator to visually indicate validation status and updates the EditorValidator for enhanced validation logic. The EntryEditorPrefsTab and JabRefPreferences are modified to manage related preferences and support new validation message settings. The UI is improved with new styles in EntryEditor.css and updated icons in IconTheme.java. Additionall...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "P-3 Diffs Only            |    0.0% |   175 | Add validation message icons to the entry editor with customizable display options. This includes the introduction of IconValidationDecorator to display warning and error icons with tooltips within the entry editor, enhancing the visualization of validation results. New icons (SUCCESS, WARNING, ERROR) have been added to the JabRef icon theme to support this feature. The EditorValidator has been updated to utilize IconValidationDecorator for improved validation visualization. A new preference opt...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "P-4 Diffs Plus Title      |   23.0% |    94 | Implements issue #3145 by adding validation messages displayed as icons within the entry editor. Introduces a new IconValidationDecorator to show error and warning icons with tooltips indicating validation problems. Adds a preference option, enabled by default, to show validation messages in the entry editor settings, improving user feedback and data quality during entry editing. Updates UI styles for the validation icons and tooltips, adds a new localization key 'Show validation messages' in th...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "P-5 Code Only             |    0.0% |   132 | Implements issue #3145 by adding support for displaying validation messages directly in the entry editor to enhance user awareness of entry validation status. A small notification icon appears in the entry editor when data inconsistencies or other problems are detected. This feature can be enabled or disabled via the preferences tab 'Entry editor', with the default preference set to enabled (VALIDATE_IN_ENTRY_EDITOR = true). A new IconValidationDecorator is introduced to provide font-based icons...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "P-6 Issue Only            |   25.0% |   109 | Fixes issue #3145 by correcting the positioning of validation messages in the entry editor. Previously, the validation messages were shifted too far down due to a fixed offset, causing display and usability issues. This PR introduces a new IconValidationDecorator to properly handle the display of validation messages, updates styles in EntryEditor.css for correct positioning, and adjusts related preferences in JabRefPreferences.java. Additionally, localization entries were added to the JabRef_en....\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "P-7 Template Plus Title   |    0.0% |   130 | This PR implements issue #3145 by adding validation messages directly in the entry editor to enhance user feedback and improve data quality. A new class, IconValidationDecorator, has been introduced to visually indicate validation states with icons, and related UI components have been updated accordingly. The entry editor's CSS has been extended to style these validation messages. Preferences for entry editor validation have been refined in EntryEditorPrefsTab and the JabRefPreferences class to ...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "P-8 Full Context          |    0.0% |   183 | Fixes issue #3145 by implementing proper display and positioning of validation messages in the entry editor. Previously, validation messages were shifted to the bottom due to a fixed offset, causing misalignment and poor user experience. This PR introduces a new IconValidationDecorator that uses font-based icons for error and warning messages, enhancing visual integration by showing a small notification icon at the correct position near the relevant fields. The default preference VALIDATE_IN_ENT...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "P-9 Basic One Shot        |    0.0% |   129 | This PR implements issue #3145 by adding the functionality to show validation messages directly within the entry editor, improving user feedback and data entry accuracy. It introduces a new class, IconValidationDecorator, to visually indicate validation status using icons. The entry editor UI has been enhanced with CSS updates to support the display of validation messages. Preferences related to the entry editor have been updated in the JabRefPreferences class to allow configuration of validatio...\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create side-by-side comparison for a single PR\n",
    "print(\"=== SIDE-BY-SIDE DESCRIPTION COMPARISON ===\\n\")\n",
    "\n",
    "# Let's pick a PR that has descriptions for all prompt variations\n",
    "# First, find a PR ID that appears across all variations\n",
    "pr_counts = merged_df.groupby('pr_id')['prompt_variation'].nunique().sort_values(ascending=False)\n",
    "print(f\"PR coverage across prompt variations:\")\n",
    "print(f\"Max variations per PR: {pr_counts.max()}\")\n",
    "print(f\"PRs with most variations: {pr_counts.head()}\")\n",
    "\n",
    "# Pick the first PR with good coverage\n",
    "sample_pr_id = pr_counts.index[1]\n",
    "print(f\"\\nUsing PR ID: {sample_pr_id}\")\n",
    "\n",
    "# Get all descriptions for this PR\n",
    "sample_descriptions = merged_df[merged_df['pr_id'] == sample_pr_id].copy()\n",
    "\n",
    "# Sort by entry_type and prompt_variation for consistent ordering\n",
    "sample_descriptions = sample_descriptions.sort_values(['entry_type', 'prompt_variation'])\n",
    "\n",
    "print(f\"\\nFound {len(sample_descriptions)} descriptions for PR {sample_pr_id}\")\n",
    "print(f\"Entry types: {sample_descriptions['entry_type'].unique()}\")\n",
    "print(f\"Prompt variations: {sorted(sample_descriptions['prompt_variation'].unique())}\")\n",
    "\n",
    "# Create the comparison\n",
    "comparison_data = []\n",
    "\n",
    "# First add the original description\n",
    "original_desc = sample_descriptions[sample_descriptions['entry_type'] == 'original']\n",
    "if len(original_desc) > 0:\n",
    "    comparison_data.append({\n",
    "        'Variation': 'Original (Human)',\n",
    "        'Description': original_desc['input_text'].iloc[0][:500] + \"...\" if len(original_desc['input_text'].iloc[0]) > 500 else original_desc['input_text'].iloc[0],\n",
    "        'Full_Description': original_desc['input_text'].iloc[0],\n",
    "        'AI_Score': f\"{parse_zerogpt_response(original_desc['zerogpt_response'].iloc[0]):.1f}%\",\n",
    "        'Word_Count': len(original_desc['input_text'].iloc[0].split())\n",
    "    })\n",
    "\n",
    "# Then add all generated descriptions\n",
    "generated_descs = sample_descriptions[sample_descriptions['entry_type'] == 'generated']\n",
    "for _, row in generated_descs.iterrows():\n",
    "    desc = row['input_text']\n",
    "    comparison_data.append({\n",
    "        'Variation': row['prompt_variation'].replace('_', ' '),\n",
    "        'Description': desc[:500] + \"...\" if len(desc) > 500 else desc,\n",
    "        'Full_Description': desc,\n",
    "        'AI_Score': f\"{parse_zerogpt_response(row['zerogpt_response']):.1f}%\",\n",
    "        'Word_Count': len(desc.split())\n",
    "    })\n",
    "\n",
    "# Create DataFrame for easy viewing\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(f\"\\n=== SIDE-BY-SIDE COMPARISON FOR PR {sample_pr_id} ===\")\n",
    "print(f\"Variation | AI Score | Words | Description Preview\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "for _, row in comparison_df.iterrows():\n",
    "    print(f\"{row['Variation']:<25} | {row['AI_Score']:>7} | {row['Word_Count']:>5} | {row['Description']}\")\n",
    "    print(\"-\" * 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bf72d3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== FULL DESCRIPTIONS FOR PRESENTATION SLIDE ===\n",
      "PR ID: MDExOlB1bGxSZXF1ZXN0MTYxNDI3MDY3\n",
      "====================================================================================================\n",
      "\n",
      "📝 **Original (Human)** (AI Score: 0.0%, Words: 90)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "<!-- describe the changes you have made here: what, why, ... -->\n",
      "The bug mentioned in #3145 was finally fixed in controlsfx. Thus we can reenable the validation messages.\n",
      "Closes #3145.\n",
      "\n",
      "----\n",
      "\n",
      "- [x] Change in CHANGELOG.md described\n",
      "- [ ] Tests created for changes\n",
      "- [ ] Screenshots added (for bigger UI changes)\n",
      "- [x] Manually tested changed features in running JabRef\n",
      "- [ ] Check documentation status (Issue created for outdated help page at [help.jabref.org](https://github.com/JabRef/help.jabref.org/issues)?)\n",
      "- [ ] If you changed the localization: Did you run `gradle localizationUpdate`?\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "📝 **P-10 Full Plus One Shot** (AI Score: 26.9%, Words: 196)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "This PR fixes issue #3145 by correcting the position and display of validation messages in the entry editor. Previously, validation messages were shown shifted to the bottom by a fixed offset, resulting in a poor user experience. The fix includes the implementation of a new IconValidationDecorator class that uses font-based icons to display validation icons and tooltips at the correct position. This decorator is integrated into the EditorValidator class to show validation messages with appropriate icons. Additionally, a new 'Show validation messages' checkbox has been added to the Entry Editor preferences tab, allowing users to enable or disable validation messages, with the default preference VALIDATE_IN_ENTRY_EDITOR set to true to ensure correct display by default. UI styles in EntryEditor.css have been updated to style the validation icons and tooltips properly, and the IconTheme has been updated to include icons for success, warning, and error states. A new localization key for 'Show validation messages' has also been added to the English properties file to support this feature. The changes have been manually tested, confirming that validation messages now appear correctly next to relevant fields in the entry editor, significantly improving the user experience. This PR closes issue #3145.\n",
      "\n",
      "\n",
      "📝 **P-11 Full Plus Few Shot** (AI Score: 0.0%, Words: 185)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Fixes issue #3145 by correcting the position and display of validation messages in the entry editor. Previously, validation messages were shown at an incorrect position, shifted to the bottom due to a fixed constant, resulting in poor user experience. This PR introduces a new validation decorator that displays validation messages with a small notification icon at the bottom left of the entry editor fields, using font-based icons with appropriate styling and tooltips for errors and warnings. The default preference 'VALIDATE_IN_ENTRY_EDITOR' has been changed from false to true to ensure validation messages are enabled and shown properly during entry editing. Additionally, a preference option was added to allow users to enable or disable validation messages in the entry editor. The PR also includes updates to the UI, CSS styling, integration of the new decorator into the validation framework, and the addition of a localization key 'Show validation messages' to the JabRef English properties file. These changes improve the visibility, usability, and clarity of validation feedback while editing entries. The fix has been manually tested to confirm correct display of validation messages in the entry editor UI.\n",
      "\n",
      "\n",
      "📝 **P-1 Minimal** (AI Score: 0.0%, Words: 82)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Introduce IconValidationDecorator to enhance validation feedback with icons in the entry editor. Update EntryEditorPrefsTab to support new validation features and improve user preferences management. Modify EditorValidator and IconTheme to integrate with the new decorator. Enhance EntryEditor.css with additional styles for validation icons. Refactored JabRefPreferences.java with minor code improvements to enhance code clarity and maintainability. Added two new entries to the English localization properties file (JabRef_en.properties) to improve or update the user interface text. Adjust build.gradle and CHANGELOG.md accordingly to reflect these changes.\n",
      "\n",
      "\n",
      "📝 **P-2 Basic** (AI Score: 0.0%, Words: 92)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "This PR implements issue #3145 by adding support for displaying validation messages directly within the entry editor. It introduces a new IconValidationDecorator to visually indicate validation status and updates the EditorValidator for enhanced validation logic. The EntryEditorPrefsTab and JabRefPreferences are modified to manage related preferences and support new validation message settings. The UI is improved with new styles in EntryEditor.css and updated icons in IconTheme.java. Additionally, localization is enhanced by updating the JabRef_en.properties file with new validation message strings. Build scripts and the changelog have been updated accordingly to reflect these changes.\n",
      "\n",
      "\n",
      "📝 **P-3 Diffs Only** (AI Score: 0.0%, Words: 175)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Add validation message icons to the entry editor with customizable display options. This includes the introduction of IconValidationDecorator to display warning and error icons with tooltips within the entry editor, enhancing the visualization of validation results. New icons (SUCCESS, WARNING, ERROR) have been added to the JabRef icon theme to support this feature. The EditorValidator has been updated to utilize IconValidationDecorator for improved validation visualization. A new preference option has been added in EntryEditorPrefsTab to enable or disable the display of validation messages, which is now enabled by default by updating the VALIDATE_IN_ENTRY_EDITOR setting from false to true in JabRef preferences, thereby improving data validation during entry editing. Styles in EntryEditor.css have been updated to style the warning and error icons and their tooltips appropriately. Additionally, the English localization properties file has been updated with a new entry 'Show validation messages' to support this feature. The changelog has been updated to document the new notification icon feature for data inconsistency detection. The build.gradle file was also updated to bump the controlsfx dependency to version 8.40.15-SNAPSHOT.\n",
      "\n",
      "\n",
      "📝 **P-4 Diffs Plus Title** (AI Score: 23.0%, Words: 94)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Implements issue #3145 by adding validation messages displayed as icons within the entry editor. Introduces a new IconValidationDecorator to show error and warning icons with tooltips indicating validation problems. Adds a preference option, enabled by default, to show validation messages in the entry editor settings, improving user feedback and data quality during entry editing. Updates UI styles for the validation icons and tooltips, adds a new localization key 'Show validation messages' in the English properties file, and updates dependencies and icons to support the new feature. The changelog is updated to reflect this enhancement.\n",
      "\n",
      "\n",
      "📝 **P-5 Code Only** (AI Score: 0.0%, Words: 132)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Implements issue #3145 by adding support for displaying validation messages directly in the entry editor to enhance user awareness of entry validation status. A small notification icon appears in the entry editor when data inconsistencies or other problems are detected. This feature can be enabled or disabled via the preferences tab 'Entry editor', with the default preference set to enabled (VALIDATE_IN_ENTRY_EDITOR = true). A new IconValidationDecorator is introduced to provide font-based icons for validation messages with custom styling, and the controlsfx validation visualizer is configured to use this new icon decorator for improved visual feedback. Additionally, the localization resource file `JabRef_en.properties` was updated to include a new key `Show validation messages` to toggle the visibility of validation messages in the entry editor UI. The changelog and preferences UI have been updated accordingly.\n",
      "\n",
      "\n",
      "📝 **P-6 Issue Only** (AI Score: 25.0%, Words: 109)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Fixes issue #3145 by correcting the positioning of validation messages in the entry editor. Previously, the validation messages were shifted too far down due to a fixed offset, causing display and usability issues. This PR introduces a new IconValidationDecorator to properly handle the display of validation messages, updates styles in EntryEditor.css for correct positioning, and adjusts related preferences in JabRefPreferences.java. Additionally, localization entries were added to the JabRef_en.properties file to support validation messages. Minor changes were also made to icon themes, build configuration, and the changelog to reflect these improvements. Overall, this update ensures validation messages are aligned correctly with their corresponding fields, improving visual consistency and user experience.\n",
      "\n",
      "\n",
      "📝 **P-7 Template Plus Title** (AI Score: 0.0%, Words: 130)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "This PR implements issue #3145 by adding validation messages directly in the entry editor to enhance user feedback and improve data quality. A new class, IconValidationDecorator, has been introduced to visually indicate validation states with icons, and related UI components have been updated accordingly. The entry editor's CSS has been extended to style these validation messages. Preferences for entry editor validation have been refined in EntryEditorPrefsTab and the JabRefPreferences class to support this feature. Updates were also made to the localization file `JabRef_en.properties` to add new validation message strings. Manual testing has been performed to ensure validation messages display correctly and help users identify and correct errors more efficiently. The icon theme and build configuration have been updated, and the changelog reflects these improvements. No additional documentation updates were necessary.\n",
      "\n",
      "\n",
      "📝 **P-8 Full Context** (AI Score: 0.0%, Words: 183)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Fixes issue #3145 by implementing proper display and positioning of validation messages in the entry editor. Previously, validation messages were shifted to the bottom due to a fixed offset, causing misalignment and poor user experience. This PR introduces a new IconValidationDecorator that uses font-based icons for error and warning messages, enhancing visual integration by showing a small notification icon at the correct position near the relevant fields. The default preference VALIDATE_IN_ENTRY_EDITOR has been changed from false to true, enabling validation messages to be shown properly within the entry editor. A new preference toggle for showing validation messages has been added to the Entry Editor preferences tab, allowing users to enable or disable this feature. Additionally, a new localization key 'Show validation messages' was added to the English properties file. The build configuration was updated to use a newer snapshot version of controlsfx to support these changes. All changes have been manually tested and integrated with existing preferences and UI components. The changelog has been updated to document the addition of the validation message icon in the entry editor. This update closes issue #3145.\n",
      "\n",
      "\n",
      "📝 **P-9 Basic One Shot** (AI Score: 0.0%, Words: 129)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "This PR implements issue #3145 by adding the functionality to show validation messages directly within the entry editor, improving user feedback and data entry accuracy. It introduces a new class, IconValidationDecorator, to visually indicate validation status using icons. The entry editor UI has been enhanced with CSS updates to support the display of validation messages. Preferences related to the entry editor have been updated in the JabRefPreferences class to allow configuration of validation features. Minor changes to IconTheme and EditorValidator support the new validation display. Additionally, localization files (JabRef_en.properties) have been updated to include necessary strings for displaying validation messages. The build configuration and changelog have been updated accordingly. Overall, these changes enhance the user experience by providing immediate and accessible feedback on entry validation within the editor interface.\n",
      "\n",
      "💾 Slide content saved to: side_by_side_comparison_PR_MDExOlB1bGxSZXF1ZXN0MTYxNDI3MDY3.md\n",
      "\n",
      "📊 **SUMMARY TABLE FOR SLIDE**\n",
      "       Prompt Variation AI Detection Score  Word Count\n",
      "       Original (Human)               0.0%          90\n",
      "P-10 Full Plus One Shot              26.9%         196\n",
      "P-11 Full Plus Few Shot               0.0%         185\n",
      "            P-1 Minimal               0.0%          82\n",
      "              P-2 Basic               0.0%          92\n",
      "         P-3 Diffs Only               0.0%         175\n",
      "   P-4 Diffs Plus Title              23.0%          94\n",
      "          P-5 Code Only               0.0%         132\n",
      "         P-6 Issue Only              25.0%         109\n",
      "P-7 Template Plus Title               0.0%         130\n",
      "       P-8 Full Context               0.0%         183\n",
      "     P-9 Basic One Shot               0.0%         129\n"
     ]
    }
   ],
   "source": [
    "# Create a presentation-friendly format with full descriptions\n",
    "print(f\"\\n\\n=== FULL DESCRIPTIONS FOR PRESENTATION SLIDE ===\")\n",
    "print(f\"PR ID: {sample_pr_id}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for i, row in comparison_df.iterrows():\n",
    "    print(f\"\\n📝 **{row['Variation']}** (AI Score: {row['AI_Score']}, Words: {row['Word_Count']})\")\n",
    "    print(\"─\" * 80)\n",
    "    print(row['Full_Description'])\n",
    "    print()\n",
    "\n",
    "# Also save to a text file for easy copying\n",
    "slide_content = f\"Side-by-Side PR Description Comparison - PR {sample_pr_id}\\n\"\n",
    "slide_content += \"=\" * 60 + \"\\n\\n\"\n",
    "\n",
    "for _, row in comparison_df.iterrows():\n",
    "    slide_content += f\"{row['Variation']} (AI Score: {row['AI_Score']}, Words: {row['Word_Count']})\\n\"\n",
    "    slide_content += \"─\" * 50 + \"\\n\"\n",
    "    slide_content += f\"{row['Full_Description']}\\n\\n\"\n",
    "\n",
    "# Save to file\n",
    "slide_file = f\"side_by_side_comparison_PR_{sample_pr_id}.md\"\n",
    "with open(slide_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(slide_content)\n",
    "\n",
    "print(f\"💾 Slide content saved to: {slide_file}\")\n",
    "\n",
    "# Create a summary table for the slide\n",
    "print(f\"\\n📊 **SUMMARY TABLE FOR SLIDE**\")\n",
    "summary_table = comparison_df[['Variation', 'AI_Score', 'Word_Count']].copy()\n",
    "summary_table.columns = ['Prompt Variation', 'AI Detection Score', 'Word Count']\n",
    "print(summary_table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3807f4b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
