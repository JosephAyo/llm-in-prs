{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d62644f",
   "metadata": {},
   "source": [
    "# Prompt Variations Analysis\n",
    "\n",
    "This notebook analyzes the performance of different prompt variations for generating PR descriptions and their detectability by AI detection tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "73a88c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103b12ab",
   "metadata": {},
   "source": [
    "## Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "65e47b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading detection results...\n",
      "Detection data shape: (704, 6)\n",
      "Detection data columns: ['pr_id', 'prompt_variation', 'entry_key', 'entry_type', 'input_text', 'zerogpt_response']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pr_id</th>\n",
       "      <th>prompt_variation</th>\n",
       "      <th>entry_key</th>\n",
       "      <th>entry_type</th>\n",
       "      <th>input_text</th>\n",
       "      <th>zerogpt_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PR_kwDOAQ0TF85oN6RH</td>\n",
       "      <td>P-7_Template_Plus_Title</td>\n",
       "      <td>PR_kwDOAQ0TF85oN6RH_P-7_Template_Plus_Title_or...</td>\n",
       "      <td>original</td>\n",
       "      <td>&lt;!-- \\nDescribe the changes you have made here...</td>\n",
       "      <td>{\"success\": true, \"code\": 200, \"message\": \"det...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PR_kwDOAQ0TF85oN6RH</td>\n",
       "      <td>P-7_Template_Plus_Title</td>\n",
       "      <td>PR_kwDOAQ0TF85oN6RH_P-7_Template_Plus_Title_ge...</td>\n",
       "      <td>generated</td>\n",
       "      <td>Fixed a modularity issue with the HTML convert...</td>\n",
       "      <td>{\"success\": true, \"code\": 200, \"message\": \"det...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2</td>\n",
       "      <td>P-7_Template_Plus_Title</td>\n",
       "      <td>MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2_P-7_Template_...</td>\n",
       "      <td>original</td>\n",
       "      <td>Add zbmath to the public databases which can b...</td>\n",
       "      <td>{\"success\": true, \"code\": 200, \"message\": \"det...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2</td>\n",
       "      <td>P-7_Template_Plus_Title</td>\n",
       "      <td>MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2_P-7_Template_...</td>\n",
       "      <td>generated</td>\n",
       "      <td>Added a new fetcher for ZbMATH to JabRef to en...</td>\n",
       "      <td>{\"success\": true, \"code\": 200, \"message\": \"det...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PR_kwDOAQ0TF86DGkyK</td>\n",
       "      <td>P-7_Template_Plus_Title</td>\n",
       "      <td>PR_kwDOAQ0TF86DGkyK_P-7_Template_Plus_Title_or...</td>\n",
       "      <td>original</td>\n",
       "      <td>Fixes https://github.com/JabRef/jabref/issues/...</td>\n",
       "      <td>{\"success\": true, \"code\": 200, \"message\": \"det...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              pr_id         prompt_variation  \\\n",
       "0               PR_kwDOAQ0TF85oN6RH  P-7_Template_Plus_Title   \n",
       "1               PR_kwDOAQ0TF85oN6RH  P-7_Template_Plus_Title   \n",
       "2  MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2  P-7_Template_Plus_Title   \n",
       "3  MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2  P-7_Template_Plus_Title   \n",
       "4               PR_kwDOAQ0TF86DGkyK  P-7_Template_Plus_Title   \n",
       "\n",
       "                                           entry_key entry_type  \\\n",
       "0  PR_kwDOAQ0TF85oN6RH_P-7_Template_Plus_Title_or...   original   \n",
       "1  PR_kwDOAQ0TF85oN6RH_P-7_Template_Plus_Title_ge...  generated   \n",
       "2  MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2_P-7_Template_...   original   \n",
       "3  MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2_P-7_Template_...  generated   \n",
       "4  PR_kwDOAQ0TF86DGkyK_P-7_Template_Plus_Title_or...   original   \n",
       "\n",
       "                                          input_text  \\\n",
       "0  <!-- \\nDescribe the changes you have made here...   \n",
       "1  Fixed a modularity issue with the HTML convert...   \n",
       "2  Add zbmath to the public databases which can b...   \n",
       "3  Added a new fetcher for ZbMATH to JabRef to en...   \n",
       "4  Fixes https://github.com/JabRef/jabref/issues/...   \n",
       "\n",
       "                                    zerogpt_response  \n",
       "0  {\"success\": true, \"code\": 200, \"message\": \"det...  \n",
       "1  {\"success\": true, \"code\": 200, \"message\": \"det...  \n",
       "2  {\"success\": true, \"code\": 200, \"message\": \"det...  \n",
       "3  {\"success\": true, \"code\": 200, \"message\": \"det...  \n",
       "4  {\"success\": true, \"code\": 200, \"message\": \"det...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define paths\n",
    "generation_path = \"../generation/datasets/\"\n",
    "detection_path = \"../detection/datasets/prompt_variations/prompt_variations-detection.csv\"\n",
    "\n",
    "# Load detection results\n",
    "print(\"Loading detection results...\")\n",
    "detection_df = pd.read_csv(detection_path)\n",
    "print(f\"Detection data shape: {detection_df.shape}\")\n",
    "print(f\"Detection data columns: {detection_df.columns.tolist()}\")\n",
    "detection_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3caa1d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 prompt variation files:\n",
      "  prompt_variation_P-10_Full_Plus_One_Shot_generated.csv\n",
      "  prompt_variation_P-11_Full_Plus_Few_Shot_generated.csv\n",
      "  prompt_variation_P-1_Minimal_generated.csv\n",
      "  prompt_variation_P-2_Basic_generated.csv\n",
      "  prompt_variation_P-3_Diffs_Only_generated.csv\n",
      "  prompt_variation_P-4_Diffs_Plus_Title_generated.csv\n",
      "  prompt_variation_P-5_Code_Only_generated.csv\n",
      "  prompt_variation_P-6_Issue_Only_generated.csv\n",
      "  prompt_variation_P-7_Template_Plus_Title_generated.csv\n",
      "  prompt_variation_P-8_Full_Context_generated.csv\n",
      "  prompt_variation_P-9_Basic_One_Shot_generated.csv\n"
     ]
    }
   ],
   "source": [
    "# Find all prompt variation CSV files\n",
    "prompt_variation_files = glob.glob(os.path.join(generation_path, \"prompt_variation_P-*_generated.csv\"))\n",
    "prompt_variation_files.sort()\n",
    "\n",
    "print(f\"Found {len(prompt_variation_files)} prompt variation files:\")\n",
    "for file in prompt_variation_files:\n",
    "    print(f\"  {os.path.basename(file)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "13fa3138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading prompt variation data...\n",
      "Loading prompt_variation_P-10_Full_Plus_One_Shot_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-10_Full_Plus_One_Shot\n",
      "Loading prompt_variation_P-11_Full_Plus_Few_Shot_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-11_Full_Plus_Few_Shot\n",
      "Loading prompt_variation_P-1_Minimal_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-1_Minimal\n",
      "Loading prompt_variation_P-2_Basic_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-11_Full_Plus_Few_Shot\n",
      "Loading prompt_variation_P-1_Minimal_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-1_Minimal\n",
      "Loading prompt_variation_P-2_Basic_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-2_Basic\n",
      "Loading prompt_variation_P-3_Diffs_Only_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-3_Diffs_Only\n",
      "Loading prompt_variation_P-4_Diffs_Plus_Title_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-2_Basic\n",
      "Loading prompt_variation_P-3_Diffs_Only_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-3_Diffs_Only\n",
      "Loading prompt_variation_P-4_Diffs_Plus_Title_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-4_Diffs_Plus_Title\n",
      "Loading prompt_variation_P-5_Code_Only_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-5_Code_Only\n",
      "Loading prompt_variation_P-6_Issue_Only_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-4_Diffs_Plus_Title\n",
      "Loading prompt_variation_P-5_Code_Only_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-5_Code_Only\n",
      "Loading prompt_variation_P-6_Issue_Only_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-6_Issue_Only\n",
      "Loading prompt_variation_P-7_Template_Plus_Title_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-7_Template_Plus_Title\n",
      "Loading prompt_variation_P-8_Full_Context_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-6_Issue_Only\n",
      "Loading prompt_variation_P-7_Template_Plus_Title_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-7_Template_Plus_Title\n",
      "Loading prompt_variation_P-8_Full_Context_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-8_Full_Context\n",
      "Loading prompt_variation_P-9_Basic_One_Shot_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-9_Basic_One_Shot\n",
      "\n",
      "Combined generation data shape: (2640, 26)\n",
      "Prompt variations found: ['P-10_Full_Plus_One_Shot', 'P-11_Full_Plus_Few_Shot', 'P-1_Minimal', 'P-2_Basic', 'P-3_Diffs_Only', 'P-4_Diffs_Plus_Title', 'P-5_Code_Only', 'P-6_Issue_Only', 'P-7_Template_Plus_Title', 'P-8_Full_Context', 'P-9_Basic_One_Shot']\n",
      "  Shape: (240, 26), Prompt variation: P-8_Full_Context\n",
      "Loading prompt_variation_P-9_Basic_One_Shot_generated.csv...\n",
      "  Shape: (240, 26), Prompt variation: P-9_Basic_One_Shot\n",
      "\n",
      "Combined generation data shape: (2640, 26)\n",
      "Prompt variations found: ['P-10_Full_Plus_One_Shot', 'P-11_Full_Plus_Few_Shot', 'P-1_Minimal', 'P-2_Basic', 'P-3_Diffs_Only', 'P-4_Diffs_Plus_Title', 'P-5_Code_Only', 'P-6_Issue_Only', 'P-7_Template_Plus_Title', 'P-8_Full_Context', 'P-9_Basic_One_Shot']\n"
     ]
    }
   ],
   "source": [
    "# Load and combine all prompt variation data\n",
    "print(\"\\nLoading prompt variation data...\")\n",
    "all_generation_data = []\n",
    "\n",
    "for file_path in prompt_variation_files:\n",
    "    print(f\"Loading {os.path.basename(file_path)}...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Extract prompt variation from filename\n",
    "    filename = os.path.basename(file_path)\n",
    "    prompt_var = filename.split('_')[2] + '_' + filename.split('_')[3]  # e.g., P-1_Minimal\n",
    "    \n",
    "    # Add prompt variation if not present\n",
    "    if 'prompt_variation' not in df.columns:\n",
    "        df['prompt_variation'] = prompt_var\n",
    "    \n",
    "    print(f\"  Shape: {df.shape}, Prompt variation: {df['prompt_variation'].iloc[0] if len(df) > 0 else 'N/A'}\")\n",
    "    all_generation_data.append(df)\n",
    "\n",
    "# Combine all generation data\n",
    "generation_df = pd.concat(all_generation_data, ignore_index=True)\n",
    "print(f\"\\nCombined generation data shape: {generation_df.shape}\")\n",
    "print(f\"Prompt variations found: {sorted(generation_df['prompt_variation'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e2f4b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation data columns:\n",
      "['id', 'title', 'description', 'state', 'repository', 'pr_number', 'filename', 'status', 'additions', 'deletions', 'changes', 'sha', 'blob_url', 'raw_url', 'patch', 'file_size_bytes', 'file_content', 'pr_total_size_bytes', 'issue_titles', 'issue_bodies', 'issue_comments', 'generated_description', 'prompt_variation', 'total_input_tokens', 'total_output_tokens', 'total_tokens']\n",
      "\n",
      "Generation data sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>state</th>\n",
       "      <th>repository</th>\n",
       "      <th>pr_number</th>\n",
       "      <th>filename</th>\n",
       "      <th>status</th>\n",
       "      <th>additions</th>\n",
       "      <th>deletions</th>\n",
       "      <th>...</th>\n",
       "      <th>file_content</th>\n",
       "      <th>pr_total_size_bytes</th>\n",
       "      <th>issue_titles</th>\n",
       "      <th>issue_bodies</th>\n",
       "      <th>issue_comments</th>\n",
       "      <th>generated_description</th>\n",
       "      <th>prompt_variation</th>\n",
       "      <th>total_input_tokens</th>\n",
       "      <th>total_output_tokens</th>\n",
       "      <th>total_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PR_kwDOAQ0TF85oN6RH</td>\n",
       "      <td>Fix modularity issue with html converter</td>\n",
       "      <td>&lt;!-- \\nDescribe the changes you have made here...</td>\n",
       "      <td>MERGED</td>\n",
       "      <td>JabRef/jabref</td>\n",
       "      <td>10943</td>\n",
       "      <td>build.gradle</td>\n",
       "      <td>modified</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>import org.gradle.internal.os.OperatingSystem\\...</td>\n",
       "      <td>28005</td>\n",
       "      <td>issue #10942: Fix: abstract field loses markdo...</td>\n",
       "      <td>issue #10942: After PR #10896, the abstract fi...</td>\n",
       "      <td>Comment #1 by LoayGhreeb in issue #10942: Ther...</td>\n",
       "      <td>This PR fixes a modularity issue related to th...</td>\n",
       "      <td>P-10_Full_Plus_One_Shot</td>\n",
       "      <td>10278</td>\n",
       "      <td>179</td>\n",
       "      <td>10457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2</td>\n",
       "      <td>Zbmath fetcher</td>\n",
       "      <td>Add zbmath to the public databases which can b...</td>\n",
       "      <td>MERGED</td>\n",
       "      <td>JabRef/jabref</td>\n",
       "      <td>7440</td>\n",
       "      <td>CHANGELOG.md</td>\n",
       "      <td>modified</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td># Changelog\\n\\nAll notable changes to this pro...</td>\n",
       "      <td>97657</td>\n",
       "      <td>issue #7437: Enhance bibliographic information...</td>\n",
       "      <td>issue #7437: It is possible to enhance bibliog...</td>\n",
       "      <td>Comment #1 by Siedlerchr in issue #7437: Sound...</td>\n",
       "      <td>This PR adds support for fetching bibliographi...</td>\n",
       "      <td>P-10_Full_Plus_One_Shot</td>\n",
       "      <td>33439</td>\n",
       "      <td>662</td>\n",
       "      <td>34101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2</td>\n",
       "      <td>Zbmath fetcher</td>\n",
       "      <td>Add zbmath to the public databases which can b...</td>\n",
       "      <td>MERGED</td>\n",
       "      <td>JabRef/jabref</td>\n",
       "      <td>7440</td>\n",
       "      <td>src/main/java/org/jabref/logic/importer/EntryB...</td>\n",
       "      <td>modified</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>package org.jabref.logic.importer;\\n\\nimport j...</td>\n",
       "      <td>97657</td>\n",
       "      <td>issue #7437: Enhance bibliographic information...</td>\n",
       "      <td>issue #7437: It is possible to enhance bibliog...</td>\n",
       "      <td>Comment #1 by Siedlerchr in issue #7437: Sound...</td>\n",
       "      <td>This PR adds support for fetching bibliographi...</td>\n",
       "      <td>P-10_Full_Plus_One_Shot</td>\n",
       "      <td>33439</td>\n",
       "      <td>662</td>\n",
       "      <td>34101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2</td>\n",
       "      <td>Zbmath fetcher</td>\n",
       "      <td>Add zbmath to the public databases which can b...</td>\n",
       "      <td>MERGED</td>\n",
       "      <td>JabRef/jabref</td>\n",
       "      <td>7440</td>\n",
       "      <td>src/main/java/org/jabref/logic/importer/WebFet...</td>\n",
       "      <td>modified</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>package org.jabref.logic.importer;\\n\\nimport j...</td>\n",
       "      <td>97657</td>\n",
       "      <td>issue #7437: Enhance bibliographic information...</td>\n",
       "      <td>issue #7437: It is possible to enhance bibliog...</td>\n",
       "      <td>Comment #1 by Siedlerchr in issue #7437: Sound...</td>\n",
       "      <td>This PR adds support for fetching bibliographi...</td>\n",
       "      <td>P-10_Full_Plus_One_Shot</td>\n",
       "      <td>33439</td>\n",
       "      <td>662</td>\n",
       "      <td>34101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2</td>\n",
       "      <td>Zbmath fetcher</td>\n",
       "      <td>Add zbmath to the public databases which can b...</td>\n",
       "      <td>MERGED</td>\n",
       "      <td>JabRef/jabref</td>\n",
       "      <td>7440</td>\n",
       "      <td>src/main/java/org/jabref/logic/importer/fetche...</td>\n",
       "      <td>modified</td>\n",
       "      <td>62</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>package org.jabref.logic.importer.fetcher;\\n\\n...</td>\n",
       "      <td>97657</td>\n",
       "      <td>issue #7437: Enhance bibliographic information...</td>\n",
       "      <td>issue #7437: It is possible to enhance bibliog...</td>\n",
       "      <td>Comment #1 by Siedlerchr in issue #7437: Sound...</td>\n",
       "      <td>This PR adds support for fetching bibliographi...</td>\n",
       "      <td>P-10_Full_Plus_One_Shot</td>\n",
       "      <td>33439</td>\n",
       "      <td>662</td>\n",
       "      <td>34101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id                                     title  \\\n",
       "0               PR_kwDOAQ0TF85oN6RH  Fix modularity issue with html converter   \n",
       "1  MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2                            Zbmath fetcher   \n",
       "2  MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2                            Zbmath fetcher   \n",
       "3  MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2                            Zbmath fetcher   \n",
       "4  MDExOlB1bGxSZXF1ZXN0NTcwMzYyODc2                            Zbmath fetcher   \n",
       "\n",
       "                                         description   state     repository  \\\n",
       "0  <!-- \\nDescribe the changes you have made here...  MERGED  JabRef/jabref   \n",
       "1  Add zbmath to the public databases which can b...  MERGED  JabRef/jabref   \n",
       "2  Add zbmath to the public databases which can b...  MERGED  JabRef/jabref   \n",
       "3  Add zbmath to the public databases which can b...  MERGED  JabRef/jabref   \n",
       "4  Add zbmath to the public databases which can b...  MERGED  JabRef/jabref   \n",
       "\n",
       "   pr_number                                           filename    status  \\\n",
       "0      10943                                       build.gradle  modified   \n",
       "1       7440                                       CHANGELOG.md  modified   \n",
       "2       7440  src/main/java/org/jabref/logic/importer/EntryB...  modified   \n",
       "3       7440  src/main/java/org/jabref/logic/importer/WebFet...  modified   \n",
       "4       7440  src/main/java/org/jabref/logic/importer/fetche...  modified   \n",
       "\n",
       "   additions  deletions  ...  \\\n",
       "0          4          2  ...   \n",
       "1          1          0  ...   \n",
       "2         11          3  ...   \n",
       "3          1          0  ...   \n",
       "4         62          9  ...   \n",
       "\n",
       "                                        file_content pr_total_size_bytes  \\\n",
       "0  import org.gradle.internal.os.OperatingSystem\\...               28005   \n",
       "1  # Changelog\\n\\nAll notable changes to this pro...               97657   \n",
       "2  package org.jabref.logic.importer;\\n\\nimport j...               97657   \n",
       "3  package org.jabref.logic.importer;\\n\\nimport j...               97657   \n",
       "4  package org.jabref.logic.importer.fetcher;\\n\\n...               97657   \n",
       "\n",
       "                                        issue_titles  \\\n",
       "0  issue #10942: Fix: abstract field loses markdo...   \n",
       "1  issue #7437: Enhance bibliographic information...   \n",
       "2  issue #7437: Enhance bibliographic information...   \n",
       "3  issue #7437: Enhance bibliographic information...   \n",
       "4  issue #7437: Enhance bibliographic information...   \n",
       "\n",
       "                                        issue_bodies  \\\n",
       "0  issue #10942: After PR #10896, the abstract fi...   \n",
       "1  issue #7437: It is possible to enhance bibliog...   \n",
       "2  issue #7437: It is possible to enhance bibliog...   \n",
       "3  issue #7437: It is possible to enhance bibliog...   \n",
       "4  issue #7437: It is possible to enhance bibliog...   \n",
       "\n",
       "                                      issue_comments  \\\n",
       "0  Comment #1 by LoayGhreeb in issue #10942: Ther...   \n",
       "1  Comment #1 by Siedlerchr in issue #7437: Sound...   \n",
       "2  Comment #1 by Siedlerchr in issue #7437: Sound...   \n",
       "3  Comment #1 by Siedlerchr in issue #7437: Sound...   \n",
       "4  Comment #1 by Siedlerchr in issue #7437: Sound...   \n",
       "\n",
       "                               generated_description         prompt_variation  \\\n",
       "0  This PR fixes a modularity issue related to th...  P-10_Full_Plus_One_Shot   \n",
       "1  This PR adds support for fetching bibliographi...  P-10_Full_Plus_One_Shot   \n",
       "2  This PR adds support for fetching bibliographi...  P-10_Full_Plus_One_Shot   \n",
       "3  This PR adds support for fetching bibliographi...  P-10_Full_Plus_One_Shot   \n",
       "4  This PR adds support for fetching bibliographi...  P-10_Full_Plus_One_Shot   \n",
       "\n",
       "   total_input_tokens total_output_tokens total_tokens  \n",
       "0               10278                 179        10457  \n",
       "1               33439                 662        34101  \n",
       "2               33439                 662        34101  \n",
       "3               33439                 662        34101  \n",
       "4               33439                 662        34101  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the structure of generation data\n",
    "print(\"Generation data columns:\")\n",
    "print(generation_df.columns.tolist())\n",
    "print(\"\\nGeneration data sample:\")\n",
    "generation_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd403629",
   "metadata": {},
   "source": [
    "## Data Merging and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "38691479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation data after deduplication: (352, 26)\n",
      "\n",
      "Detection data entry types: entry_type\n",
      "original     352\n",
      "generated    352\n",
      "Name: count, dtype: int64\n",
      "Detection data prompt variations: ['P-10_Full_Plus_One_Shot', 'P-11_Full_Plus_Few_Shot', 'P-1_Minimal', 'P-2_Basic', 'P-3_Diffs_Only', 'P-4_Diffs_Plus_Title', 'P-5_Code_Only', 'P-6_Issue_Only', 'P-7_Template_Plus_Title', 'P-8_Full_Context', 'P-9_Basic_One_Shot']\n"
     ]
    }
   ],
   "source": [
    "# Prepare generation data for merging\n",
    "# Group by PR ID and prompt variation to get unique records (since multiple files per PR have same generated description)\n",
    "generation_unique = generation_df.groupby(['id', 'prompt_variation']).first().reset_index()\n",
    "print(f\"Generation data after deduplication: {generation_unique.shape}\")\n",
    "\n",
    "# Prepare detection data for merging\n",
    "print(f\"\\nDetection data entry types: {detection_df['entry_type'].value_counts()}\")\n",
    "print(f\"Detection data prompt variations: {sorted(detection_df['prompt_variation'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5fd20804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data shape: (704, 31)\n",
      "Merged data entry types: entry_type\n",
      "original     352\n",
      "generated    352\n",
      "Name: count, dtype: int64\n",
      "Merged data prompt variations: ['P-10_Full_Plus_One_Shot', 'P-11_Full_Plus_Few_Shot', 'P-1_Minimal', 'P-2_Basic', 'P-3_Diffs_Only', 'P-4_Diffs_Plus_Title', 'P-5_Code_Only', 'P-6_Issue_Only', 'P-7_Template_Plus_Title', 'P-8_Full_Context', 'P-9_Basic_One_Shot']\n"
     ]
    }
   ],
   "source": [
    "# Merge generation and detection data\n",
    "# For generation data: use id as pr_id\n",
    "generation_unique['pr_id'] = generation_unique['id']\n",
    "\n",
    "# Merge on pr_id and prompt_variation\n",
    "merged_df = pd.merge(\n",
    "    generation_unique,\n",
    "    detection_df,\n",
    "    on=['pr_id', 'prompt_variation'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"Merged data shape: {merged_df.shape}\")\n",
    "print(f\"Merged data entry types: {merged_df['entry_type'].value_counts()}\")\n",
    "print(f\"Merged data prompt variations: {sorted(merged_df['prompt_variation'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d46294b",
   "metadata": {},
   "source": [
    "## Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4cd649ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_zerogpt_response(response_str):\n",
    "    \"\"\"Parse ZeroGPT response to extract AI probability\"\"\"\n",
    "    try:\n",
    "        if pd.isna(response_str) or response_str == \"\":\n",
    "            return None\n",
    "        \n",
    "        # Try to parse as JSON\n",
    "        if isinstance(response_str, str):\n",
    "            response = json.loads(response_str)\n",
    "            \n",
    "            # Handle nested structure - check if there's a 'data' field\n",
    "            if 'data' in response:\n",
    "                data = response['data']\n",
    "                # Use fakePercentage if available, otherwise calculate from isHuman\n",
    "                if 'fakePercentage' in data:\n",
    "                    return data['fakePercentage']\n",
    "                elif 'isHuman' in data:\n",
    "                    return 100 - data['isHuman']  # Convert isHuman to AI percentage\n",
    "                else:\n",
    "                    return None\n",
    "            else:\n",
    "                # Direct structure\n",
    "                if 'fakePercentage' in response:\n",
    "                    return response['fakePercentage']\n",
    "                elif 'isHuman' in response:\n",
    "                    return 100 - response['isHuman']\n",
    "                else:\n",
    "                    return None\n",
    "        else:\n",
    "            # If it's already a number or can be converted\n",
    "            return float(response_str)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing response: {e} | Response: {response_str[:100] if isinstance(response_str, str) else response_str}\")\n",
    "        return None\n",
    "\n",
    "def calculate_detection_metrics(df, ai_threshold=50.0):  # Changed threshold to 50% since we're dealing with percentages\n",
    "    \"\"\"Calculate detection accuracy metrics including AI probability statistics\"\"\"\n",
    "    # Parse AI probabilities\n",
    "    df = df.copy()  # Avoid SettingWithCopyWarning\n",
    "    df['ai_probability'] = df['zerogpt_response'].apply(parse_zerogpt_response)\n",
    "    \n",
    "    # Filter out rows where AI probability couldn't be parsed\n",
    "    valid_df = df[df['ai_probability'].notna()].copy()\n",
    "    \n",
    "    if len(valid_df) == 0:\n",
    "        return {\n",
    "            'true_positive_pct': 0, \n",
    "            'false_negative_pct': 0, \n",
    "            'total_samples': len(df), \n",
    "            'valid_samples': 0,\n",
    "            'mean_ai_score_generated': 0,\n",
    "            'median_ai_score_generated': 0,\n",
    "            'mean_ai_score_original': 0,\n",
    "            'median_ai_score_original': 0\n",
    "        }\n",
    "    \n",
    "    # Determine if detected as AI (above threshold)\n",
    "    valid_df['detected_as_ai'] = valid_df['ai_probability'] > ai_threshold\n",
    "    \n",
    "    # Calculate metrics\n",
    "    generated_entries = valid_df[valid_df['entry_type'] == 'generated']\n",
    "    original_entries = valid_df[valid_df['entry_type'] == 'original']\n",
    "    \n",
    "    # True Positive Rate: Generated content correctly identified as AI\n",
    "    tp_rate = 0\n",
    "    if len(generated_entries) > 0:\n",
    "        tp_rate = (generated_entries['detected_as_ai'].sum() / len(generated_entries)) * 100\n",
    "    \n",
    "    # False Positive Rate: Original content incorrectly identified as AI (this is what we call \"false negative\" in the context)\n",
    "    fp_rate = 0\n",
    "    if len(original_entries) > 0:\n",
    "        fp_rate = (original_entries['detected_as_ai'].sum() / len(original_entries)) * 100\n",
    "    \n",
    "    # AI probability statistics\n",
    "    mean_ai_generated = generated_entries['ai_probability'].mean() if len(generated_entries) > 0 else 0\n",
    "    median_ai_generated = generated_entries['ai_probability'].median() if len(generated_entries) > 0 else 0\n",
    "    mean_ai_original = original_entries['ai_probability'].mean() if len(original_entries) > 0 else 0\n",
    "    median_ai_original = original_entries['ai_probability'].median() if len(original_entries) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'true_positive_pct': tp_rate,\n",
    "        'false_negative_pct': fp_rate,  # FP on original = FN from human perspective\n",
    "        'total_samples': len(df),\n",
    "        'valid_samples': len(valid_df),\n",
    "        'mean_ai_score_generated': mean_ai_generated,\n",
    "        'median_ai_score_generated': median_ai_generated,\n",
    "        'mean_ai_score_original': mean_ai_original,\n",
    "        'median_ai_score_original': median_ai_original\n",
    "    }\n",
    "\n",
    "def calculate_text_metrics(text_series):\n",
    "    \"\"\"Calculate text-based metrics\"\"\"\n",
    "    if len(text_series) == 0:\n",
    "        return {'mean_length': 0, 'median_length': 0}\n",
    "    \n",
    "    lengths = text_series.str.len()\n",
    "    return {\n",
    "        'mean_length': lengths.mean(),\n",
    "        'median_length': lengths.median()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f467d23",
   "metadata": {},
   "source": [
    "## Main Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c97fa1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debugging merged data structure:\n",
      "Merged data shape: (704, 31)\n",
      "Entry types: entry_type\n",
      "original     352\n",
      "generated    352\n",
      "Name: count, dtype: int64\n",
      "Prompt variations: ['P-10_Full_Plus_One_Shot', 'P-11_Full_Plus_Few_Shot', 'P-1_Minimal', 'P-2_Basic', 'P-3_Diffs_Only', 'P-4_Diffs_Plus_Title', 'P-5_Code_Only', 'P-6_Issue_Only', 'P-7_Template_Plus_Title', 'P-8_Full_Context', 'P-9_Basic_One_Shot']\n",
      "\n",
      "ZeroGPT response data availability:\n",
      "Non-null zerogpt_response: 704\n",
      "Empty zerogpt_response: 0\n",
      "\n",
      "Sample zerogpt responses:\n",
      "Sample 1: {\"success\": true, \"code\": 200, \"message\": \"detection result passed to proxy\", \"data\": {\"sentences\": [], \"isHuman\": 100, \"additional_feedback\": \"\", \"h\": [], \"hi\": [], \"textWords\": 77, \"aiWords\": 0, \"fa...\n",
      "Sample 2: {\"success\": true, \"code\": 200, \"message\": \"detection result passed to proxy\", \"data\": {\"sentences\": [], \"isHuman\": 100, \"additional_feedback\": \"\", \"h\": [], \"hi\": [], \"textWords\": 87, \"aiWords\": 0, \"fa...\n",
      "Sample 3: {\"success\": true, \"code\": 200, \"message\": \"detection result passed to proxy\", \"data\": {\"sentences\": [], \"isHuman\": 100, \"additional_feedback\": \"\", \"h\": [], \"hi\": [], \"textWords\": 77, \"aiWords\": 0, \"fa...\n",
      "\n",
      "Analyzing prompt variations: ['P-10_Full_Plus_One_Shot', 'P-11_Full_Plus_Few_Shot', 'P-1_Minimal', 'P-2_Basic', 'P-3_Diffs_Only', 'P-4_Diffs_Plus_Title', 'P-5_Code_Only', 'P-6_Issue_Only', 'P-7_Template_Plus_Title', 'P-8_Full_Context', 'P-9_Basic_One_Shot']\n",
      "\n",
      "Analyzing Original descriptions...\n",
      "Original data shape: (352, 31)\n",
      "  Total samples: 352\n",
      "  Valid samples: 352\n",
      "  Mean description length: 1184.0\n",
      "  False negative rate: 0.0%\n",
      "  Mean AI score (original): 13.5%\n",
      "  Median AI score (original): 9.9%\n",
      "  Total samples: 352\n",
      "  Valid samples: 352\n",
      "  Mean description length: 1184.0\n",
      "  False negative rate: 0.0%\n",
      "  Mean AI score (original): 13.5%\n",
      "  Median AI score (original): 9.9%\n"
     ]
    }
   ],
   "source": [
    "# Debug: Let's check the merged data structure\n",
    "print(\"Debugging merged data structure:\")\n",
    "print(f\"Merged data shape: {merged_df.shape}\")\n",
    "print(f\"Entry types: {merged_df['entry_type'].value_counts()}\")\n",
    "print(f\"Prompt variations: {sorted(merged_df['prompt_variation'].unique())}\")\n",
    "\n",
    "# Check if we have zerogpt_response data\n",
    "print(f\"\\nZeroGPT response data availability:\")\n",
    "print(f\"Non-null zerogpt_response: {merged_df['zerogpt_response'].notna().sum()}\")\n",
    "print(f\"Empty zerogpt_response: {(merged_df['zerogpt_response'] == '').sum()}\")\n",
    "\n",
    "# Look at a sample of zerogpt responses\n",
    "print(\"\\nSample zerogpt responses:\")\n",
    "sample_responses = merged_df[merged_df['zerogpt_response'].notna() & (merged_df['zerogpt_response'] != '')]['zerogpt_response'].head(3)\n",
    "for i, resp in enumerate(sample_responses):\n",
    "    print(f\"Sample {i+1}: {resp[:200]}...\")\n",
    "\n",
    "# Now re-analyze with this understanding\n",
    "analysis_results = []\n",
    "\n",
    "# Get all prompt variations plus 'Original'\n",
    "prompt_variations = sorted([pv for pv in merged_df['prompt_variation'].unique() if pv.startswith('P-')])\n",
    "print(f\"\\nAnalyzing prompt variations: {prompt_variations}\")\n",
    "\n",
    "# First, analyze 'Original' (using original descriptions from any prompt variation)\n",
    "print(\"\\nAnalyzing Original descriptions...\")\n",
    "original_data = merged_df[merged_df['entry_type'] == 'original']\n",
    "print(f\"Original data shape: {original_data.shape}\")\n",
    "\n",
    "if len(original_data) > 0:\n",
    "    # Token metrics - Original doesn't have token usage, so set to 0\n",
    "    original_metrics = {\n",
    "        'prompt_variation': 'Original',\n",
    "        'mean_prompt_tokens': 0,\n",
    "        'median_prompt_tokens': 0,\n",
    "        'mean_completion_tokens': 0,\n",
    "        'median_completion_tokens': 0\n",
    "    }\n",
    "    \n",
    "    # Text metrics\n",
    "    text_metrics = calculate_text_metrics(original_data['input_text'])\n",
    "    original_metrics.update({\n",
    "        'mean_description_length': text_metrics['mean_length'],\n",
    "        'median_description_length': text_metrics['median_length']\n",
    "    })\n",
    "    \n",
    "    # Detection metrics (including new AI probability metrics)\n",
    "    detection_metrics = calculate_detection_metrics(original_data)\n",
    "    original_metrics.update({\n",
    "        'true_positive_pct': detection_metrics['true_positive_pct'],\n",
    "        'false_negative_pct': detection_metrics['false_negative_pct'],\n",
    "        'mean_ai_score_generated': detection_metrics['mean_ai_score_generated'],\n",
    "        'median_ai_score_generated': detection_metrics['median_ai_score_generated'],\n",
    "        'mean_ai_score_original': detection_metrics['mean_ai_score_original'],\n",
    "        'median_ai_score_original': detection_metrics['median_ai_score_original'],\n",
    "        'total_samples': detection_metrics['total_samples'],\n",
    "        'valid_samples': detection_metrics['valid_samples']\n",
    "    })\n",
    "    \n",
    "    analysis_results.append(original_metrics)\n",
    "    print(f\"  Total samples: {detection_metrics['total_samples']}\")\n",
    "    print(f\"  Valid samples: {detection_metrics['valid_samples']}\")\n",
    "    print(f\"  Mean description length: {text_metrics['mean_length']:.1f}\")\n",
    "    print(f\"  False negative rate: {detection_metrics['false_negative_pct']:.1f}%\")\n",
    "    print(f\"  Mean AI score (original): {detection_metrics['mean_ai_score_original']:.1f}%\")\n",
    "    print(f\"  Median AI score (original): {detection_metrics['median_ai_score_original']:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "57916e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing P-10_Full_Plus_One_Shot...\n",
      "  Samples: 64 (Generated: 32)\n",
      "  Mean prompt tokens: 64351\n",
      "  Mean completion tokens: 557\n",
      "  Mean description length: 1112.8\n",
      "  True positive rate: 28.1%\n",
      "  False negative rate: 0.0%\n",
      "  Mean AI score (generated): 27.0%\n",
      "  Mean AI score (original): 13.5%\n",
      "\n",
      "Analyzing P-11_Full_Plus_Few_Shot...\n",
      "  Samples: 64 (Generated: 32)\n",
      "  Mean prompt tokens: 74742\n",
      "  Mean completion tokens: 451\n",
      "  Mean description length: 922.2\n",
      "  True positive rate: 15.6%\n",
      "  False negative rate: 0.0%\n",
      "  Mean AI score (generated): 22.5%\n",
      "  Mean AI score (original): 13.5%\n",
      "\n",
      "Analyzing P-1_Minimal...\n",
      "  Samples: 64 (Generated: 32)\n",
      "  Mean prompt tokens: 632\n",
      "  Mean completion tokens: 190\n",
      "  Mean description length: 500.2\n",
      "  True positive rate: 3.1%\n",
      "  False negative rate: 0.0%\n",
      "  Mean AI score (generated): 4.7%\n",
      "  Mean AI score (original): 13.5%\n",
      "\n",
      "Analyzing P-2_Basic...\n",
      "  Samples: 64 (Generated: 32)\n",
      "  Mean prompt tokens: 681\n",
      "  Mean completion tokens: 212\n",
      "  Mean description length: 511.4\n",
      "  True positive rate: 6.2%\n",
      "  False negative rate: 0.0%\n",
      "  Mean AI score (generated): 9.3%\n",
      "  Mean AI score (original): 13.5%\n",
      "\n",
      "Analyzing P-3_Diffs_Only...\n",
      "  Samples: 64 (Generated: 32)\n",
      "  Mean prompt tokens: 5575\n",
      "  Mean completion tokens: 289\n",
      "  Mean description length: 712.5\n",
      "  True positive rate: 6.2%\n",
      "  False negative rate: 0.0%\n",
      "  Mean AI score (generated): 6.8%\n",
      "  Mean AI score (original): 13.5%\n",
      "\n",
      "Analyzing P-4_Diffs_Plus_Title...\n",
      "  Samples: 64 (Generated: 32)\n",
      "  Mean prompt tokens: 5620\n",
      "  Mean completion tokens: 303\n",
      "  Mean description length: 698.6\n",
      "  True positive rate: 3.1%\n",
      "  False negative rate: 0.0%\n",
      "  Mean AI score (generated): 9.9%\n",
      "  Mean AI score (original): 13.5%\n",
      "\n",
      "Analyzing P-5_Code_Only...\n",
      "  Samples: 64 (Generated: 32)\n",
      "  Mean prompt tokens: 56585\n",
      "  Mean completion tokens: 348\n",
      "  Mean description length: 806.5\n",
      "  True positive rate: 12.5%\n",
      "  False negative rate: 0.0%\n",
      "  Mean AI score (generated): 14.4%\n",
      "  Mean AI score (original): 13.5%\n",
      "\n",
      "Analyzing P-6_Issue_Only...\n",
      "  Samples: 64 (Generated: 32)\n",
      "  Mean prompt tokens: 2110\n",
      "  Mean completion tokens: 345\n",
      "  Mean description length: 669.0\n",
      "  True positive rate: 12.5%\n",
      "  False negative rate: 0.0%\n",
      "  Mean AI score (generated): 17.4%\n",
      "  Mean AI score (original): 13.5%\n",
      "\n",
      "Analyzing P-7_Template_Plus_Title...\n",
      "  Samples: 64 (Generated: 32)\n",
      "  Mean prompt tokens: 873\n",
      "  Mean completion tokens: 327\n",
      "  Mean description length: 725.7\n",
      "  True positive rate: 3.1%\n",
      "  False negative rate: 0.0%\n",
      "  Mean AI score (generated): 13.2%\n",
      "  Mean AI score (original): 13.5%\n",
      "\n",
      "Analyzing P-8_Full_Context...\n",
      "  Samples: 64 (Generated: 32)\n",
      "  Mean prompt tokens: 58099\n",
      "  Mean completion tokens: 450\n",
      "  Mean description length: 958.5\n",
      "  True positive rate: 3.1%\n",
      "  False negative rate: 0.0%\n",
      "  Mean AI score (generated): 12.8%\n",
      "  Mean AI score (original): 13.5%\n",
      "\n",
      "Analyzing P-9_Basic_One_Shot...\n",
      "  Samples: 64 (Generated: 32)\n",
      "  Mean prompt tokens: 2154\n",
      "  Mean completion tokens: 310\n",
      "  Mean description length: 720.2\n",
      "  True positive rate: 28.1%\n",
      "  False negative rate: 0.0%\n",
      "  Mean AI score (generated): 27.9%\n",
      "  Mean AI score (original): 13.5%\n"
     ]
    }
   ],
   "source": [
    "# Now analyze each prompt variation\n",
    "for pv in prompt_variations:\n",
    "    print(f\"\\nAnalyzing {pv}...\")\n",
    "    \n",
    "    # Get data for this prompt variation\n",
    "    pv_data = merged_df[merged_df['prompt_variation'] == pv]\n",
    "    \n",
    "    if len(pv_data) == 0:\n",
    "        print(f\"  No data found for {pv}\")\n",
    "        continue\n",
    "    \n",
    "    # Get generation data for token metrics (only generated entries have token info)\n",
    "    pv_generated = pv_data[pv_data['entry_type'] == 'generated']\n",
    "    \n",
    "    pv_metrics = {'prompt_variation': pv}\n",
    "    \n",
    "    # Token metrics\n",
    "    if len(pv_generated) > 0 and 'total_input_tokens' in pv_generated.columns:\n",
    "        pv_metrics.update({\n",
    "            'mean_prompt_tokens': pv_generated['total_input_tokens'].mean(),\n",
    "            'median_prompt_tokens': pv_generated['total_input_tokens'].median(),\n",
    "            'mean_completion_tokens': pv_generated['total_output_tokens'].mean(),\n",
    "            'median_completion_tokens': pv_generated['total_output_tokens'].median()\n",
    "        })\n",
    "    else:\n",
    "        pv_metrics.update({\n",
    "            'mean_prompt_tokens': 0,\n",
    "            'median_prompt_tokens': 0,\n",
    "            'mean_completion_tokens': 0,\n",
    "            'median_completion_tokens': 0\n",
    "        })\n",
    "    \n",
    "    # Text metrics (using generated descriptions)\n",
    "    if len(pv_generated) > 0:\n",
    "        text_metrics = calculate_text_metrics(pv_generated['input_text'])\n",
    "        pv_metrics.update({\n",
    "            'mean_description_length': text_metrics['mean_length'],\n",
    "            'median_description_length': text_metrics['median_length']\n",
    "        })\n",
    "    else:\n",
    "        pv_metrics.update({\n",
    "            'mean_description_length': 0,\n",
    "            'median_description_length': 0\n",
    "        })\n",
    "    \n",
    "    # Detection metrics (using both original and generated, including new AI probability metrics)\n",
    "    detection_metrics = calculate_detection_metrics(pv_data)\n",
    "    pv_metrics.update({\n",
    "        'true_positive_pct': detection_metrics['true_positive_pct'],\n",
    "        'false_negative_pct': detection_metrics['false_negative_pct'],\n",
    "        'mean_ai_score_generated': detection_metrics['mean_ai_score_generated'],\n",
    "        'median_ai_score_generated': detection_metrics['median_ai_score_generated'],\n",
    "        'mean_ai_score_original': detection_metrics['mean_ai_score_original'],\n",
    "        'median_ai_score_original': detection_metrics['median_ai_score_original'],\n",
    "        'total_samples': detection_metrics['total_samples']\n",
    "    })\n",
    "    \n",
    "    analysis_results.append(pv_metrics)\n",
    "    \n",
    "    print(f\"  Samples: {detection_metrics['total_samples']} (Generated: {len(pv_generated)})\")\n",
    "    print(f\"  Mean prompt tokens: {pv_metrics['mean_prompt_tokens']:.0f}\")\n",
    "    print(f\"  Mean completion tokens: {pv_metrics['mean_completion_tokens']:.0f}\")\n",
    "    print(f\"  Mean description length: {pv_metrics['mean_description_length']:.1f}\")\n",
    "    print(f\"  True positive rate: {detection_metrics['true_positive_pct']:.1f}%\")\n",
    "    print(f\"  False negative rate: {detection_metrics['false_negative_pct']:.1f}%\")\n",
    "    print(f\"  Mean AI score (generated): {detection_metrics['mean_ai_score_generated']:.1f}%\")\n",
    "    print(f\"  Mean AI score (original): {detection_metrics['mean_ai_score_original']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b835b3c3",
   "metadata": {},
   "source": [
    "## Results Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3a232af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PROMPT VARIATIONS ANALYSIS RESULTS ===\n",
      "\n",
      "Target Output Table with AI Probability Scores:\n",
      "================================================================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_variation</th>\n",
       "      <th>mean_prompt_tokens</th>\n",
       "      <th>median_prompt_tokens</th>\n",
       "      <th>mean_completion_tokens</th>\n",
       "      <th>median_completion_tokens</th>\n",
       "      <th>mean_description_length</th>\n",
       "      <th>median_description_length</th>\n",
       "      <th>true_positive_pct</th>\n",
       "      <th>false_negative_pct</th>\n",
       "      <th>mean_ai_score_generated</th>\n",
       "      <th>median_ai_score_generated</th>\n",
       "      <th>mean_ai_score_original</th>\n",
       "      <th>median_ai_score_original</th>\n",
       "      <th>total_samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Original</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1184.0</td>\n",
       "      <td>1187.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P-10_Full_Plus_One_Shot</td>\n",
       "      <td>64351.4</td>\n",
       "      <td>28815.5</td>\n",
       "      <td>557.1</td>\n",
       "      <td>303.5</td>\n",
       "      <td>1112.8</td>\n",
       "      <td>983.0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>16.8</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P-11_Full_Plus_Few_Shot</td>\n",
       "      <td>74741.9</td>\n",
       "      <td>35300.0</td>\n",
       "      <td>451.0</td>\n",
       "      <td>246.5</td>\n",
       "      <td>922.2</td>\n",
       "      <td>761.5</td>\n",
       "      <td>15.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.5</td>\n",
       "      <td>18.1</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P-1_Minimal</td>\n",
       "      <td>632.5</td>\n",
       "      <td>365.0</td>\n",
       "      <td>189.8</td>\n",
       "      <td>85.5</td>\n",
       "      <td>500.2</td>\n",
       "      <td>373.5</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P-2_Basic</td>\n",
       "      <td>681.2</td>\n",
       "      <td>412.5</td>\n",
       "      <td>212.5</td>\n",
       "      <td>117.5</td>\n",
       "      <td>511.4</td>\n",
       "      <td>396.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>P-3_Diffs_Only</td>\n",
       "      <td>5574.5</td>\n",
       "      <td>2129.0</td>\n",
       "      <td>288.8</td>\n",
       "      <td>155.0</td>\n",
       "      <td>712.5</td>\n",
       "      <td>480.5</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>P-4_Diffs_Plus_Title</td>\n",
       "      <td>5619.8</td>\n",
       "      <td>2143.5</td>\n",
       "      <td>303.1</td>\n",
       "      <td>167.0</td>\n",
       "      <td>698.6</td>\n",
       "      <td>513.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>P-5_Code_Only</td>\n",
       "      <td>56585.1</td>\n",
       "      <td>24574.5</td>\n",
       "      <td>348.1</td>\n",
       "      <td>174.5</td>\n",
       "      <td>806.5</td>\n",
       "      <td>658.5</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>P-6_Issue_Only</td>\n",
       "      <td>2109.8</td>\n",
       "      <td>882.5</td>\n",
       "      <td>345.1</td>\n",
       "      <td>181.5</td>\n",
       "      <td>669.0</td>\n",
       "      <td>575.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>P-7_Template_Plus_Title</td>\n",
       "      <td>872.8</td>\n",
       "      <td>525.0</td>\n",
       "      <td>326.9</td>\n",
       "      <td>175.5</td>\n",
       "      <td>725.7</td>\n",
       "      <td>604.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>P-8_Full_Context</td>\n",
       "      <td>58098.7</td>\n",
       "      <td>24909.0</td>\n",
       "      <td>450.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>958.5</td>\n",
       "      <td>816.5</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>P-9_Basic_One_Shot</td>\n",
       "      <td>2154.0</td>\n",
       "      <td>1318.0</td>\n",
       "      <td>310.4</td>\n",
       "      <td>170.5</td>\n",
       "      <td>720.2</td>\n",
       "      <td>650.0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.9</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           prompt_variation  mean_prompt_tokens  median_prompt_tokens  \\\n",
       "0                  Original                 0.0                   0.0   \n",
       "1   P-10_Full_Plus_One_Shot             64351.4               28815.5   \n",
       "2   P-11_Full_Plus_Few_Shot             74741.9               35300.0   \n",
       "3               P-1_Minimal               632.5                 365.0   \n",
       "4                 P-2_Basic               681.2                 412.5   \n",
       "5            P-3_Diffs_Only              5574.5                2129.0   \n",
       "6      P-4_Diffs_Plus_Title              5619.8                2143.5   \n",
       "7             P-5_Code_Only             56585.1               24574.5   \n",
       "8            P-6_Issue_Only              2109.8                 882.5   \n",
       "9   P-7_Template_Plus_Title               872.8                 525.0   \n",
       "10         P-8_Full_Context             58098.7               24909.0   \n",
       "11       P-9_Basic_One_Shot              2154.0                1318.0   \n",
       "\n",
       "    mean_completion_tokens  median_completion_tokens  mean_description_length  \\\n",
       "0                      0.0                       0.0                   1184.0   \n",
       "1                    557.1                     303.5                   1112.8   \n",
       "2                    451.0                     246.5                    922.2   \n",
       "3                    189.8                      85.5                    500.2   \n",
       "4                    212.5                     117.5                    511.4   \n",
       "5                    288.8                     155.0                    712.5   \n",
       "6                    303.1                     167.0                    698.6   \n",
       "7                    348.1                     174.5                    806.5   \n",
       "8                    345.1                     181.5                    669.0   \n",
       "9                    326.9                     175.5                    725.7   \n",
       "10                   450.0                     227.0                    958.5   \n",
       "11                   310.4                     170.5                    720.2   \n",
       "\n",
       "    median_description_length  true_positive_pct  false_negative_pct  \\\n",
       "0                      1187.5                0.0                 0.0   \n",
       "1                       983.0               28.1                 0.0   \n",
       "2                       761.5               15.6                 0.0   \n",
       "3                       373.5                3.1                 0.0   \n",
       "4                       396.0                6.2                 0.0   \n",
       "5                       480.5                6.2                 0.0   \n",
       "6                       513.0                3.1                 0.0   \n",
       "7                       658.5               12.5                 0.0   \n",
       "8                       575.0               12.5                 0.0   \n",
       "9                       604.0                3.1                 0.0   \n",
       "10                      816.5                3.1                 0.0   \n",
       "11                      650.0               28.1                 0.0   \n",
       "\n",
       "    mean_ai_score_generated  median_ai_score_generated  \\\n",
       "0                       0.0                        0.0   \n",
       "1                      27.0                       16.8   \n",
       "2                      22.5                       18.1   \n",
       "3                       4.7                        0.0   \n",
       "4                       9.3                        0.0   \n",
       "5                       6.8                        0.0   \n",
       "6                       9.9                        0.0   \n",
       "7                      14.4                        0.0   \n",
       "8                      17.4                        0.0   \n",
       "9                      13.2                        0.0   \n",
       "10                     12.8                        0.0   \n",
       "11                     27.9                        6.0   \n",
       "\n",
       "    mean_ai_score_original  median_ai_score_original  total_samples  \n",
       "0                     13.5                       9.9            352  \n",
       "1                     13.5                       9.9             64  \n",
       "2                     13.5                       9.9             64  \n",
       "3                     13.5                       9.9             64  \n",
       "4                     13.5                       9.9             64  \n",
       "5                     13.5                       9.9             64  \n",
       "6                     13.5                       9.9             64  \n",
       "7                     13.5                       9.9             64  \n",
       "8                     13.5                       9.9             64  \n",
       "9                     13.5                       9.9             64  \n",
       "10                    13.5                       9.9             64  \n",
       "11                    13.5                       9.9             64  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create results dataframe\n",
    "results_df = pd.DataFrame(analysis_results)\n",
    "\n",
    "# Reorder columns as requested, including new AI probability metrics\n",
    "column_order = [\n",
    "    'prompt_variation',\n",
    "    'mean_prompt_tokens',\n",
    "    'median_prompt_tokens', \n",
    "    'mean_completion_tokens',\n",
    "    'median_completion_tokens',\n",
    "    'mean_description_length',\n",
    "    'median_description_length',\n",
    "    'true_positive_pct',\n",
    "    'false_negative_pct',\n",
    "    'mean_ai_score_generated',\n",
    "    'median_ai_score_generated',\n",
    "    'mean_ai_score_original',\n",
    "    'median_ai_score_original',\n",
    "    'total_samples'\n",
    "]\n",
    "\n",
    "results_df = results_df[column_order]\n",
    "\n",
    "# Round numeric columns for better display\n",
    "numeric_cols = [col for col in results_df.columns if col != 'prompt_variation']\n",
    "results_df[numeric_cols] = results_df[numeric_cols].round(1)\n",
    "\n",
    "print(\"\\n=== PROMPT VARIATIONS ANALYSIS RESULTS ===\")\n",
    "print(\"\\nTarget Output Table with AI Probability Scores:\")\n",
    "print(\"=\" * 160)\n",
    "\n",
    "# Display the table\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ba72e713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved to: prompt_variations_analysis_results.csv\n",
      "\n",
      "=== SUMMARY STATISTICS ===\n",
      "Total prompt variations analyzed: 12\n",
      "Best performing prompt (highest true positive rate): P-10_Full_Plus_One_Shot\n",
      "Most efficient prompt (lowest mean prompt tokens): P-1_Minimal\n",
      "Longest descriptions (highest mean length): Original\n"
     ]
    }
   ],
   "source": [
    "# Save results to CSV\n",
    "output_file = \"prompt_variations_analysis_results.csv\"\n",
    "results_df.to_csv(output_file, index=False)\n",
    "print(f\"\\nResults saved to: {output_file}\")\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\n=== SUMMARY STATISTICS ===\")\n",
    "print(f\"Total prompt variations analyzed: {len(results_df)}\")\n",
    "print(f\"Best performing prompt (highest true positive rate): {results_df.loc[results_df['true_positive_pct'].idxmax(), 'prompt_variation']}\")\n",
    "print(f\"Most efficient prompt (lowest mean prompt tokens): {results_df[results_df['prompt_variation'] != 'Original'].loc[results_df[results_df['prompt_variation'] != 'Original']['mean_prompt_tokens'].idxmin(), 'prompt_variation']}\")\n",
    "print(f\"Longest descriptions (highest mean length): {results_df.loc[results_df['mean_description_length'].idxmax(), 'prompt_variation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88735b38",
   "metadata": {},
   "source": [
    "## Additional Textual Analysis Features\n",
    "\n",
    "Now let's add some additional textual analysis features similar to the jabref-prs-comparison notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e98d514e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended textual analysis functions defined.\n"
     ]
    }
   ],
   "source": [
    "def extract_zerogpt_text_metrics(response_str):\n",
    "    \"\"\"Extract textual metrics from ZeroGPT response\"\"\"\n",
    "    try:\n",
    "        if pd.isna(response_str) or response_str == \"\":\n",
    "            return {'textWords': None, 'aiWords': None, 'word_count': None}\n",
    "        \n",
    "        response = json.loads(response_str)\n",
    "        \n",
    "        if 'data' in response:\n",
    "            data = response['data']\n",
    "            return {\n",
    "                'textWords': data.get('textWords', None),\n",
    "                'aiWords': data.get('aiWords', None),\n",
    "                'word_count': data.get('textWords', None)  # Same as textWords\n",
    "            }\n",
    "        else:\n",
    "            return {'textWords': None, 'aiWords': None, 'word_count': None}\n",
    "    except:\n",
    "        return {'textWords': None, 'aiWords': None, 'word_count': None}\n",
    "\n",
    "def calculate_extended_text_metrics(df):\n",
    "    \"\"\"Calculate extended textual metrics\"\"\"\n",
    "    if len(df) == 0:\n",
    "        return {}\n",
    "    \n",
    "    # Basic length metrics\n",
    "    lengths = df['input_text'].str.len()\n",
    "    \n",
    "    # Extract ZeroGPT word counts\n",
    "    zerogpt_metrics = df['zerogpt_response'].apply(extract_zerogpt_text_metrics)\n",
    "    word_counts = pd.DataFrame(zerogpt_metrics.tolist())['textWords'].dropna()\n",
    "    \n",
    "    # Calculate sentences (approximate by counting periods, exclamation marks, question marks)\n",
    "    sentence_counts = df['input_text'].str.count(r'[.!?]+')\n",
    "    \n",
    "    # Calculate newlines (as a proxy for paragraph structure)\n",
    "    newline_counts = df['input_text'].str.count(r'\\\\n')\n",
    "    \n",
    "    return {\n",
    "        'mean_char_length': lengths.mean(),\n",
    "        'median_char_length': lengths.median(),\n",
    "        'mean_word_count': word_counts.mean() if len(word_counts) > 0 else 0,\n",
    "        'median_word_count': word_counts.median() if len(word_counts) > 0 else 0,\n",
    "        'mean_sentence_count': sentence_counts.mean(),\n",
    "        'median_sentence_count': sentence_counts.median(),\n",
    "        'mean_newline_count': newline_counts.mean(),\n",
    "        'median_newline_count': newline_counts.median()\n",
    "    }\n",
    "\n",
    "print(\"Extended textual analysis functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5add51c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXTENDED TEXTUAL ANALYSIS ===\\n\n",
      "Original - Samples: 352\n",
      "  Mean words: 160.7\n",
      "  Mean sentences: 19.8\n",
      "  Mean newlines: 0.0\\n\n",
      "P-10_Full_Plus_One_Shot - Samples: 32\n",
      "  Mean words: 159.9\n",
      "  Mean sentences: 9.8\n",
      "  Mean newlines: 0.0\\n\n",
      "P-11_Full_Plus_Few_Shot - Samples: 32\n",
      "  Mean words: 132.0\n",
      "  Mean sentences: 8.2\n",
      "  Mean newlines: 0.0\\n\n",
      "P-1_Minimal - Samples: 32\n",
      "  Mean words: 62.0\n",
      "  Mean sentences: 6.2\n",
      "  Mean newlines: 0.0\\n\n",
      "P-2_Basic - Samples: 32\n",
      "  Mean words: 67.8\n",
      "  Mean sentences: 5.3\n",
      "  Mean newlines: 0.0\\n\n",
      "P-3_Diffs_Only - Samples: 32\n",
      "  Mean words: 93.9\n",
      "  Mean sentences: 6.7\n",
      "  Mean newlines: 0.0\\n\n",
      "P-4_Diffs_Plus_Title - Samples: 32\n",
      "  Mean words: 94.4\n",
      "  Mean sentences: 6.5\n",
      "  Mean newlines: 0.0\\n\n",
      "P-5_Code_Only - Samples: 32\n",
      "  Mean words: 109.9\n",
      "  Mean sentences: 6.9\n",
      "  Mean newlines: 0.0\\n\n",
      "P-6_Issue_Only - Samples: 32\n",
      "  Mean words: 94.2\n",
      "  Mean sentences: 6.4\n",
      "  Mean newlines: 0.0\\n\n",
      "P-7_Template_Plus_Title - Samples: 32\n",
      "  Mean words: 98.7\n",
      "  Mean sentences: 8.0\n",
      "  Mean newlines: 0.0\\n\n",
      "P-8_Full_Context - Samples: 32\n",
      "  Mean words: 134.7\n",
      "  Mean sentences: 8.3\n",
      "  Mean newlines: 0.0\\n\n",
      "P-9_Basic_One_Shot - Samples: 32\n",
      "  Mean words: 95.9\n",
      "  Mean sentences: 7.0\n",
      "  Mean newlines: 0.0\\n\n",
      "Original - Samples: 352\n",
      "  Mean words: 160.7\n",
      "  Mean sentences: 19.8\n",
      "  Mean newlines: 0.0\\n\n",
      "P-10_Full_Plus_One_Shot - Samples: 32\n",
      "  Mean words: 159.9\n",
      "  Mean sentences: 9.8\n",
      "  Mean newlines: 0.0\\n\n",
      "P-11_Full_Plus_Few_Shot - Samples: 32\n",
      "  Mean words: 132.0\n",
      "  Mean sentences: 8.2\n",
      "  Mean newlines: 0.0\\n\n",
      "P-1_Minimal - Samples: 32\n",
      "  Mean words: 62.0\n",
      "  Mean sentences: 6.2\n",
      "  Mean newlines: 0.0\\n\n",
      "P-2_Basic - Samples: 32\n",
      "  Mean words: 67.8\n",
      "  Mean sentences: 5.3\n",
      "  Mean newlines: 0.0\\n\n",
      "P-3_Diffs_Only - Samples: 32\n",
      "  Mean words: 93.9\n",
      "  Mean sentences: 6.7\n",
      "  Mean newlines: 0.0\\n\n",
      "P-4_Diffs_Plus_Title - Samples: 32\n",
      "  Mean words: 94.4\n",
      "  Mean sentences: 6.5\n",
      "  Mean newlines: 0.0\\n\n",
      "P-5_Code_Only - Samples: 32\n",
      "  Mean words: 109.9\n",
      "  Mean sentences: 6.9\n",
      "  Mean newlines: 0.0\\n\n",
      "P-6_Issue_Only - Samples: 32\n",
      "  Mean words: 94.2\n",
      "  Mean sentences: 6.4\n",
      "  Mean newlines: 0.0\\n\n",
      "P-7_Template_Plus_Title - Samples: 32\n",
      "  Mean words: 98.7\n",
      "  Mean sentences: 8.0\n",
      "  Mean newlines: 0.0\\n\n",
      "P-8_Full_Context - Samples: 32\n",
      "  Mean words: 134.7\n",
      "  Mean sentences: 8.3\n",
      "  Mean newlines: 0.0\\n\n",
      "P-9_Basic_One_Shot - Samples: 32\n",
      "  Mean words: 95.9\n",
      "  Mean sentences: 7.0\n",
      "  Mean newlines: 0.0\\n\n"
     ]
    }
   ],
   "source": [
    "# Extended textual analysis for each prompt variation\n",
    "print(\"=== EXTENDED TEXTUAL ANALYSIS ===\\\\n\")\n",
    "\n",
    "extended_analysis_results = []\n",
    "\n",
    "# Analyze Original\n",
    "original_data = merged_df[merged_df['entry_type'] == 'original']\n",
    "if len(original_data) > 0:\n",
    "    ext_metrics = calculate_extended_text_metrics(original_data)\n",
    "    ext_metrics['prompt_variation'] = 'Original'\n",
    "    extended_analysis_results.append(ext_metrics)\n",
    "    print(f\"Original - Samples: {len(original_data)}\")\n",
    "    print(f\"  Mean words: {ext_metrics['mean_word_count']:.1f}\")\n",
    "    print(f\"  Mean sentences: {ext_metrics['mean_sentence_count']:.1f}\")\n",
    "    print(f\"  Mean newlines: {ext_metrics['mean_newline_count']:.1f}\\\\n\")\n",
    "\n",
    "# Analyze each prompt variation\n",
    "for pv in prompt_variations:\n",
    "    pv_data = merged_df[merged_df['prompt_variation'] == pv]\n",
    "    pv_generated = pv_data[pv_data['entry_type'] == 'generated']\n",
    "    \n",
    "    if len(pv_generated) > 0:\n",
    "        ext_metrics = calculate_extended_text_metrics(pv_generated)\n",
    "        ext_metrics['prompt_variation'] = pv\n",
    "        extended_analysis_results.append(ext_metrics)\n",
    "        print(f\"{pv} - Samples: {len(pv_generated)}\")\n",
    "        print(f\"  Mean words: {ext_metrics['mean_word_count']:.1f}\")\n",
    "        print(f\"  Mean sentences: {ext_metrics['mean_sentence_count']:.1f}\")\n",
    "        print(f\"  Mean newlines: {ext_metrics['mean_newline_count']:.1f}\\\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "63d3ea97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COMPREHENSIVE ANALYSIS RESULTS ===\n",
      "\n",
      "Final Table with Textual Features and AI Probability Scores:\n",
      "====================================================================================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_variation</th>\n",
       "      <th>mean_prompt_tokens</th>\n",
       "      <th>median_prompt_tokens</th>\n",
       "      <th>mean_completion_tokens</th>\n",
       "      <th>median_completion_tokens</th>\n",
       "      <th>mean_description_length</th>\n",
       "      <th>median_description_length</th>\n",
       "      <th>mean_word_count</th>\n",
       "      <th>median_word_count</th>\n",
       "      <th>mean_sentence_count</th>\n",
       "      <th>median_sentence_count</th>\n",
       "      <th>true_positive_pct</th>\n",
       "      <th>false_negative_pct</th>\n",
       "      <th>mean_ai_score_generated</th>\n",
       "      <th>median_ai_score_generated</th>\n",
       "      <th>mean_ai_score_original</th>\n",
       "      <th>median_ai_score_original</th>\n",
       "      <th>total_samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Original</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1184.0</td>\n",
       "      <td>1187.5</td>\n",
       "      <td>160.7</td>\n",
       "      <td>169.0</td>\n",
       "      <td>19.8</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P-10_Full_Plus_One_Shot</td>\n",
       "      <td>64351.4</td>\n",
       "      <td>28815.5</td>\n",
       "      <td>557.1</td>\n",
       "      <td>303.5</td>\n",
       "      <td>1112.8</td>\n",
       "      <td>983.0</td>\n",
       "      <td>159.9</td>\n",
       "      <td>140.0</td>\n",
       "      <td>9.8</td>\n",
       "      <td>9.5</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>16.8</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P-11_Full_Plus_Few_Shot</td>\n",
       "      <td>74741.9</td>\n",
       "      <td>35300.0</td>\n",
       "      <td>451.0</td>\n",
       "      <td>246.5</td>\n",
       "      <td>922.2</td>\n",
       "      <td>761.5</td>\n",
       "      <td>132.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>8.2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.5</td>\n",
       "      <td>18.1</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P-1_Minimal</td>\n",
       "      <td>632.5</td>\n",
       "      <td>365.0</td>\n",
       "      <td>189.8</td>\n",
       "      <td>85.5</td>\n",
       "      <td>500.2</td>\n",
       "      <td>373.5</td>\n",
       "      <td>62.0</td>\n",
       "      <td>46.5</td>\n",
       "      <td>6.2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P-2_Basic</td>\n",
       "      <td>681.2</td>\n",
       "      <td>412.5</td>\n",
       "      <td>212.5</td>\n",
       "      <td>117.5</td>\n",
       "      <td>511.4</td>\n",
       "      <td>396.0</td>\n",
       "      <td>67.8</td>\n",
       "      <td>62.0</td>\n",
       "      <td>5.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>P-3_Diffs_Only</td>\n",
       "      <td>5574.5</td>\n",
       "      <td>2129.0</td>\n",
       "      <td>288.8</td>\n",
       "      <td>155.0</td>\n",
       "      <td>712.5</td>\n",
       "      <td>480.5</td>\n",
       "      <td>93.9</td>\n",
       "      <td>65.5</td>\n",
       "      <td>6.7</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>P-4_Diffs_Plus_Title</td>\n",
       "      <td>5619.8</td>\n",
       "      <td>2143.5</td>\n",
       "      <td>303.1</td>\n",
       "      <td>167.0</td>\n",
       "      <td>698.6</td>\n",
       "      <td>513.0</td>\n",
       "      <td>94.4</td>\n",
       "      <td>71.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>P-5_Code_Only</td>\n",
       "      <td>56585.1</td>\n",
       "      <td>24574.5</td>\n",
       "      <td>348.1</td>\n",
       "      <td>174.5</td>\n",
       "      <td>806.5</td>\n",
       "      <td>658.5</td>\n",
       "      <td>109.9</td>\n",
       "      <td>86.5</td>\n",
       "      <td>6.9</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>P-6_Issue_Only</td>\n",
       "      <td>2109.8</td>\n",
       "      <td>882.5</td>\n",
       "      <td>345.1</td>\n",
       "      <td>181.5</td>\n",
       "      <td>669.0</td>\n",
       "      <td>575.0</td>\n",
       "      <td>94.2</td>\n",
       "      <td>85.5</td>\n",
       "      <td>6.4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>P-7_Template_Plus_Title</td>\n",
       "      <td>872.8</td>\n",
       "      <td>525.0</td>\n",
       "      <td>326.9</td>\n",
       "      <td>175.5</td>\n",
       "      <td>725.7</td>\n",
       "      <td>604.0</td>\n",
       "      <td>98.7</td>\n",
       "      <td>85.5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>P-8_Full_Context</td>\n",
       "      <td>58098.7</td>\n",
       "      <td>24909.0</td>\n",
       "      <td>450.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>958.5</td>\n",
       "      <td>816.5</td>\n",
       "      <td>134.7</td>\n",
       "      <td>116.5</td>\n",
       "      <td>8.3</td>\n",
       "      <td>7.5</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>P-9_Basic_One_Shot</td>\n",
       "      <td>2154.0</td>\n",
       "      <td>1318.0</td>\n",
       "      <td>310.4</td>\n",
       "      <td>170.5</td>\n",
       "      <td>720.2</td>\n",
       "      <td>650.0</td>\n",
       "      <td>95.9</td>\n",
       "      <td>95.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.9</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           prompt_variation  mean_prompt_tokens  median_prompt_tokens  \\\n",
       "0                  Original                 0.0                   0.0   \n",
       "1   P-10_Full_Plus_One_Shot             64351.4               28815.5   \n",
       "2   P-11_Full_Plus_Few_Shot             74741.9               35300.0   \n",
       "3               P-1_Minimal               632.5                 365.0   \n",
       "4                 P-2_Basic               681.2                 412.5   \n",
       "5            P-3_Diffs_Only              5574.5                2129.0   \n",
       "6      P-4_Diffs_Plus_Title              5619.8                2143.5   \n",
       "7             P-5_Code_Only             56585.1               24574.5   \n",
       "8            P-6_Issue_Only              2109.8                 882.5   \n",
       "9   P-7_Template_Plus_Title               872.8                 525.0   \n",
       "10         P-8_Full_Context             58098.7               24909.0   \n",
       "11       P-9_Basic_One_Shot              2154.0                1318.0   \n",
       "\n",
       "    mean_completion_tokens  median_completion_tokens  mean_description_length  \\\n",
       "0                      0.0                       0.0                   1184.0   \n",
       "1                    557.1                     303.5                   1112.8   \n",
       "2                    451.0                     246.5                    922.2   \n",
       "3                    189.8                      85.5                    500.2   \n",
       "4                    212.5                     117.5                    511.4   \n",
       "5                    288.8                     155.0                    712.5   \n",
       "6                    303.1                     167.0                    698.6   \n",
       "7                    348.1                     174.5                    806.5   \n",
       "8                    345.1                     181.5                    669.0   \n",
       "9                    326.9                     175.5                    725.7   \n",
       "10                   450.0                     227.0                    958.5   \n",
       "11                   310.4                     170.5                    720.2   \n",
       "\n",
       "    median_description_length  mean_word_count  median_word_count  \\\n",
       "0                      1187.5            160.7              169.0   \n",
       "1                       983.0            159.9              140.0   \n",
       "2                       761.5            132.0              116.0   \n",
       "3                       373.5             62.0               46.5   \n",
       "4                       396.0             67.8               62.0   \n",
       "5                       480.5             93.9               65.5   \n",
       "6                       513.0             94.4               71.5   \n",
       "7                       658.5            109.9               86.5   \n",
       "8                       575.0             94.2               85.5   \n",
       "9                       604.0             98.7               85.5   \n",
       "10                      816.5            134.7              116.5   \n",
       "11                      650.0             95.9               95.0   \n",
       "\n",
       "    mean_sentence_count  median_sentence_count  true_positive_pct  \\\n",
       "0                  19.8                   19.0                0.0   \n",
       "1                   9.8                    9.5               28.1   \n",
       "2                   8.2                    7.0               15.6   \n",
       "3                   6.2                    5.0                3.1   \n",
       "4                   5.3                    5.0                6.2   \n",
       "5                   6.7                    6.0                6.2   \n",
       "6                   6.5                    5.0                3.1   \n",
       "7                   6.9                    6.0               12.5   \n",
       "8                   6.4                    6.0               12.5   \n",
       "9                   8.0                    8.0                3.1   \n",
       "10                  8.3                    7.5                3.1   \n",
       "11                  7.0                    6.0               28.1   \n",
       "\n",
       "    false_negative_pct  mean_ai_score_generated  median_ai_score_generated  \\\n",
       "0                  0.0                      0.0                        0.0   \n",
       "1                  0.0                     27.0                       16.8   \n",
       "2                  0.0                     22.5                       18.1   \n",
       "3                  0.0                      4.7                        0.0   \n",
       "4                  0.0                      9.3                        0.0   \n",
       "5                  0.0                      6.8                        0.0   \n",
       "6                  0.0                      9.9                        0.0   \n",
       "7                  0.0                     14.4                        0.0   \n",
       "8                  0.0                     17.4                        0.0   \n",
       "9                  0.0                     13.2                        0.0   \n",
       "10                 0.0                     12.8                        0.0   \n",
       "11                 0.0                     27.9                        6.0   \n",
       "\n",
       "    mean_ai_score_original  median_ai_score_original  total_samples  \n",
       "0                     13.5                       9.9            352  \n",
       "1                     13.5                       9.9             64  \n",
       "2                     13.5                       9.9             64  \n",
       "3                     13.5                       9.9             64  \n",
       "4                     13.5                       9.9             64  \n",
       "5                     13.5                       9.9             64  \n",
       "6                     13.5                       9.9             64  \n",
       "7                     13.5                       9.9             64  \n",
       "8                     13.5                       9.9             64  \n",
       "9                     13.5                       9.9             64  \n",
       "10                    13.5                       9.9             64  \n",
       "11                    13.5                       9.9             64  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create extended results DataFrame\n",
    "extended_results_df = pd.DataFrame(extended_analysis_results)\n",
    "\n",
    "# Merge with original results\n",
    "final_results = pd.merge(results_df, extended_results_df, on='prompt_variation', how='left')\n",
    "\n",
    "# Reorder columns for the final comprehensive table\n",
    "final_column_order = [\n",
    "    'prompt_variation',\n",
    "    'mean_prompt_tokens',\n",
    "    'median_prompt_tokens', \n",
    "    'mean_completion_tokens',\n",
    "    'median_completion_tokens',\n",
    "    'mean_description_length',\n",
    "    'median_description_length',\n",
    "    'mean_word_count',\n",
    "    'median_word_count',\n",
    "    'mean_sentence_count',\n",
    "    'median_sentence_count',\n",
    "    'true_positive_pct',\n",
    "    'false_negative_pct',\n",
    "    'mean_ai_score_generated',\n",
    "    'median_ai_score_generated',\n",
    "    'mean_ai_score_original',\n",
    "    'median_ai_score_original',\n",
    "    'total_samples'\n",
    "]\n",
    "\n",
    "final_results = final_results[final_column_order]\n",
    "\n",
    "# Round numeric columns for better display\n",
    "numeric_cols = [col for col in final_results.columns if col != 'prompt_variation']\n",
    "final_results[numeric_cols] = final_results[numeric_cols].round(1)\n",
    "\n",
    "print(\"\\n=== COMPREHENSIVE ANALYSIS RESULTS ===\\n\")\n",
    "print(\"Final Table with Textual Features and AI Probability Scores:\")\n",
    "print(\"=\" * 180)\n",
    "\n",
    "# Display the table\n",
    "final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "afad6542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comprehensive results saved to: comprehensive_prompt_variations_analysis.csv\n",
      "\n",
      "=== FINAL COMPREHENSIVE SUMMARY ===\n",
      "Total prompt variations analyzed: 12\n",
      "\n",
      "Best detection performance: P-10_Full_Plus_One_Shot (28.1% true positive rate)\n",
      "Most efficient (lowest tokens): P-1_Minimal (632 mean prompt tokens)\n",
      "Longest descriptions: Original (1184 chars)\n",
      "Most words per description: Original (160.7 words)\n",
      "Highest AI detectability: P-9_Basic_One_Shot (27.9% mean AI score)\n",
      "\n",
      "=== KEY INSIGHTS ===\n",
      "1. Detection Performance:\n",
      "   - Average true positive rate across all prompts: 11.1%\n",
      "   - Best performing prompts: P-10_Full_Plus_One_Shot, P-9_Basic_One_Shot, P-11_Full_Plus_Few_Shot\n",
      "   - Average AI score for generated content: 15.1%\n",
      "   - Highest AI scores: P-9_Basic_One_Shot, P-10_Full_Plus_One_Shot, P-11_Full_Plus_Few_Shot\n",
      "\n",
      "2. Token Efficiency:\n",
      "   - Most token-efficient prompts: P-1_Minimal, P-2_Basic, P-7_Template_Plus_Title\n",
      "   - Average prompt tokens: 24675\n",
      "\n",
      "3. Content Quality:\n",
      "   - Original descriptions average 161 words, AI score: 13.5%\n",
      "   - Generated descriptions range from 62 to 160 words\n",
      "   - Generated AI scores range from 4.7% to 27.9%\n",
      "   - Average generated content length: 104 words\n",
      "\n",
      "4. AI Detection Score Analysis:\n",
      "   - Original (human) content: 13.5% mean AI score (all below 50% threshold)\n",
      "   - Generated content: 15.1% average mean AI score\n",
      "   - Detection gap: 1.6 percentage points higher for AI content\n",
      "   - Best performing prompts: P-10_Full_Plus_One_Shot, P-9_Basic_One_Shot, P-11_Full_Plus_Few_Shot\n",
      "   - Average AI score for generated content: 15.1%\n",
      "   - Highest AI scores: P-9_Basic_One_Shot, P-10_Full_Plus_One_Shot, P-11_Full_Plus_Few_Shot\n",
      "\n",
      "2. Token Efficiency:\n",
      "   - Most token-efficient prompts: P-1_Minimal, P-2_Basic, P-7_Template_Plus_Title\n",
      "   - Average prompt tokens: 24675\n",
      "\n",
      "3. Content Quality:\n",
      "   - Original descriptions average 161 words, AI score: 13.5%\n",
      "   - Generated descriptions range from 62 to 160 words\n",
      "   - Generated AI scores range from 4.7% to 27.9%\n",
      "   - Average generated content length: 104 words\n",
      "\n",
      "4. AI Detection Score Analysis:\n",
      "   - Original (human) content: 13.5% mean AI score (all below 50% threshold)\n",
      "   - Generated content: 15.1% average mean AI score\n",
      "   - Detection gap: 1.6 percentage points higher for AI content\n"
     ]
    }
   ],
   "source": [
    "# Save comprehensive results\n",
    "comprehensive_output_file = \"comprehensive_prompt_variations_analysis.csv\"\n",
    "final_results.to_csv(comprehensive_output_file, index=False)\n",
    "print(f\"\\nComprehensive results saved to: {comprehensive_output_file}\")\n",
    "\n",
    "# Final comprehensive summary\n",
    "print(\"\\n=== FINAL COMPREHENSIVE SUMMARY ===\")\n",
    "print(f\"Total prompt variations analyzed: {len(final_results)}\")\n",
    "\n",
    "# Best performers\n",
    "best_detection = final_results.loc[final_results['true_positive_pct'].idxmax()]\n",
    "most_efficient = final_results[final_results['prompt_variation'] != 'Original'].loc[final_results[final_results['prompt_variation'] != 'Original']['mean_prompt_tokens'].idxmin()]\n",
    "longest_content = final_results.loc[final_results['mean_description_length'].idxmax()]\n",
    "most_words = final_results.loc[final_results['mean_word_count'].idxmax()]\n",
    "highest_ai_score = final_results.loc[final_results['mean_ai_score_generated'].idxmax()]\n",
    "\n",
    "print(f\"\\nBest detection performance: {best_detection['prompt_variation']} ({best_detection['true_positive_pct']:.1f}% true positive rate)\")\n",
    "print(f\"Most efficient (lowest tokens): {most_efficient['prompt_variation']} ({most_efficient['mean_prompt_tokens']:.0f} mean prompt tokens)\")\n",
    "print(f\"Longest descriptions: {longest_content['prompt_variation']} ({longest_content['mean_description_length']:.0f} chars)\")\n",
    "print(f\"Most words per description: {most_words['prompt_variation']} ({most_words['mean_word_count']:.1f} words)\")\n",
    "print(f\"Highest AI detectability: {highest_ai_score['prompt_variation']} ({highest_ai_score['mean_ai_score_generated']:.1f}% mean AI score)\")\n",
    "\n",
    "print(\"\\n=== KEY INSIGHTS ===\")\n",
    "print(\"1. Detection Performance:\")\n",
    "generated_only = final_results[final_results['prompt_variation'] != 'Original']\n",
    "print(f\"   - Average true positive rate across all prompts: {generated_only['true_positive_pct'].mean():.1f}%\")\n",
    "print(f\"   - Best performing prompts: {', '.join(generated_only.nlargest(3, 'true_positive_pct')['prompt_variation'].tolist())}\")\n",
    "print(f\"   - Average AI score for generated content: {generated_only['mean_ai_score_generated'].mean():.1f}%\")\n",
    "print(f\"   - Highest AI scores: {', '.join(generated_only.nlargest(3, 'mean_ai_score_generated')['prompt_variation'].tolist())}\")\n",
    "\n",
    "print(\"\\n2. Token Efficiency:\")\n",
    "print(f\"   - Most token-efficient prompts: {', '.join(generated_only.nsmallest(3, 'mean_prompt_tokens')['prompt_variation'].tolist())}\")\n",
    "print(f\"   - Average prompt tokens: {generated_only['mean_prompt_tokens'].mean():.0f}\")\n",
    "\n",
    "print(\"\\n3. Content Quality:\")\n",
    "original_row = final_results[final_results['prompt_variation'] == 'Original']\n",
    "print(f\"   - Original descriptions average {original_row['mean_word_count'].iloc[0]:.0f} words, AI score: {original_row['mean_ai_score_original'].iloc[0]:.1f}%\")\n",
    "print(f\"   - Generated descriptions range from {generated_only['mean_word_count'].min():.0f} to {generated_only['mean_word_count'].max():.0f} words\")\n",
    "print(f\"   - Generated AI scores range from {generated_only['mean_ai_score_generated'].min():.1f}% to {generated_only['mean_ai_score_generated'].max():.1f}%\")\n",
    "print(f\"   - Average generated content length: {generated_only['mean_word_count'].mean():.0f} words\")\n",
    "\n",
    "print(\"\\n4. AI Detection Score Analysis:\")\n",
    "print(f\"   - Original (human) content: {original_row['mean_ai_score_original'].iloc[0]:.1f}% mean AI score (all below 50% threshold)\")\n",
    "print(f\"   - Generated content: {generated_only['mean_ai_score_generated'].mean():.1f}% average mean AI score\")\n",
    "print(f\"   - Detection gap: {generated_only['mean_ai_score_generated'].mean() - original_row['mean_ai_score_original'].iloc[0]:.1f} percentage points higher for AI content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2def9ce",
   "metadata": {},
   "source": [
    "## Debug: Understanding Original Detection Metrics\n",
    "\n",
    "Let's examine why the Original descriptions show 0.0 for both true_positive_pct and false_negative_pct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e3b80844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DEBUGGING ORIGINAL DETECTION METRICS ===\n",
      "\n",
      "1. Original data shape: (352, 31)\n",
      "   Entry types: entry_type\n",
      "original    352\n",
      "Name: count, dtype: int64\n",
      "\n",
      "2. ZeroGPT response availability for Original data:\n",
      "   Non-null responses: 352\n",
      "   Null/empty responses: 0\n",
      "\n",
      "3. Sample AI probabilities for Original descriptions:\n",
      "   Sample 0: AI probability = 0.0\n",
      "   Sample 2: AI probability = 0.0\n",
      "   Sample 4: AI probability = 0.0\n",
      "   Sample 6: AI probability = 0.0\n",
      "   Sample 8: AI probability = 0.0\n",
      "\n",
      "4. Running detection metrics calculation for Original:\n",
      "   AI probabilities parsed: 352 out of 352\n",
      "   AI probability range: 0.0 to 48.4\n",
      "   Valid samples for analysis: 352\n",
      "   Using AI threshold: 50.0%\n",
      "   Detected as AI: 0 out of 352\n",
      "   Generated entries: 0 (detected as AI: 0)\n",
      "   Original entries: 352 (detected as AI: 0)\n",
      "   True Positive Rate: 0.0% (Generated correctly identified as AI)\n",
      "   False Positive Rate: 0.0% (Original incorrectly identified as AI)\n",
      "\n",
      "5. The issue explanation:\n",
      "   - For 'Original' analysis, we only have original (human-written) descriptions\n",
      "   - True Positive Rate = Generated content correctly identified as AI / Total Generated\n",
      "   - But we have 0 generated entries in the 'Original' dataset\n",
      "   - False Negative Rate = Original content incorrectly identified as AI / Total Original\n",
      "   - This should show the misclassification rate of human content\n"
     ]
    }
   ],
   "source": [
    "# Debug: Let's examine the Original descriptions detection logic step by step\n",
    "print(\"=== DEBUGGING ORIGINAL DETECTION METRICS ===\\n\")\n",
    "\n",
    "# Get original data again\n",
    "original_data = merged_df[merged_df['entry_type'] == 'original']\n",
    "print(f\"1. Original data shape: {original_data.shape}\")\n",
    "print(f\"   Entry types: {original_data['entry_type'].value_counts()}\")\n",
    "\n",
    "# Check zerogpt responses for original data\n",
    "print(f\"\\n2. ZeroGPT response availability for Original data:\")\n",
    "print(f\"   Non-null responses: {original_data['zerogpt_response'].notna().sum()}\")\n",
    "print(f\"   Null/empty responses: {original_data['zerogpt_response'].isna().sum()}\")\n",
    "\n",
    "# Try parsing a few responses manually\n",
    "print(f\"\\n3. Sample AI probabilities for Original descriptions:\")\n",
    "sample_original = original_data.head(5)\n",
    "for idx, row in sample_original.iterrows():\n",
    "    ai_prob = parse_zerogpt_response(row['zerogpt_response'])\n",
    "    print(f\"   Sample {idx}: AI probability = {ai_prob}\")\n",
    "\n",
    "# Now let's run the detection metrics function step by step\n",
    "print(f\"\\n4. Running detection metrics calculation for Original:\")\n",
    "\n",
    "# Copy the function logic but with debug prints\n",
    "original_debug = original_data.copy()\n",
    "original_debug['ai_probability'] = original_debug['zerogpt_response'].apply(parse_zerogpt_response)\n",
    "\n",
    "print(f\"   AI probabilities parsed: {original_debug['ai_probability'].notna().sum()} out of {len(original_debug)}\")\n",
    "print(f\"   AI probability range: {original_debug['ai_probability'].min():.1f} to {original_debug['ai_probability'].max():.1f}\")\n",
    "\n",
    "valid_df = original_debug[original_debug['ai_probability'].notna()].copy()\n",
    "print(f\"   Valid samples for analysis: {len(valid_df)}\")\n",
    "\n",
    "if len(valid_df) > 0:\n",
    "    # Check detection threshold\n",
    "    ai_threshold = 50.0\n",
    "    valid_df['detected_as_ai'] = valid_df['ai_probability'] > ai_threshold\n",
    "    print(f\"   Using AI threshold: {ai_threshold}%\")\n",
    "    print(f\"   Detected as AI: {valid_df['detected_as_ai'].sum()} out of {len(valid_df)}\")\n",
    "    \n",
    "    # Break down by entry type\n",
    "    generated_entries = valid_df[valid_df['entry_type'] == 'generated']\n",
    "    original_entries = valid_df[valid_df['entry_type'] == 'original']\n",
    "    \n",
    "    print(f\"   Generated entries: {len(generated_entries)} (detected as AI: {generated_entries['detected_as_ai'].sum() if len(generated_entries) > 0 else 0})\")\n",
    "    print(f\"   Original entries: {len(original_entries)} (detected as AI: {original_entries['detected_as_ai'].sum() if len(original_entries) > 0 else 0})\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    tp_rate = 0\n",
    "    if len(generated_entries) > 0:\n",
    "        tp_rate = (generated_entries['detected_as_ai'].sum() / len(generated_entries)) * 100\n",
    "    \n",
    "    fp_rate = 0  \n",
    "    if len(original_entries) > 0:\n",
    "        fp_rate = (original_entries['detected_as_ai'].sum() / len(original_entries)) * 100\n",
    "    \n",
    "    print(f\"   True Positive Rate: {tp_rate:.1f}% (Generated correctly identified as AI)\")\n",
    "    print(f\"   False Positive Rate: {fp_rate:.1f}% (Original incorrectly identified as AI)\")\n",
    "    \n",
    "else:\n",
    "    print(\"   No valid samples to analyze!\")\n",
    "\n",
    "print(f\"\\n5. The issue explanation:\")\n",
    "print(f\"   - For 'Original' analysis, we only have original (human-written) descriptions\")\n",
    "print(f\"   - True Positive Rate = Generated content correctly identified as AI / Total Generated\")\n",
    "print(f\"   - But we have 0 generated entries in the 'Original' dataset\")\n",
    "print(f\"   - False Negative Rate = Original content incorrectly identified as AI / Total Original\")\n",
    "print(f\"   - This should show the misclassification rate of human content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "503dc05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n=== EXPLANATION OF ORIGINAL DETECTION METRICS ===\\n\n",
      "Why Original shows 0.0 for both metrics:\n",
      "\\n1. **True Positive Rate (0.0%)**:\n",
      "   - This measures: Generated content correctly identified as AI\n",
      "   - For 'Original' row: We have 0 generated entries (only human-written content)\n",
      "   - So: 0 generated entries detected as AI / 0 total generated entries = 0/0 = 0%\n",
      "   - This is correct but not meaningful for the Original baseline\n",
      "\\n2. **False Negative Rate (0.0%)**:\n",
      "   - This measures: Original (human) content incorrectly identified as AI\n",
      "   - For 'Original' row: 0 out of 352 human descriptions were misclassified as AI\n",
      "   - So: 0 misclassified / 352 total human descriptions = 0%\n",
      "   - This means the detector correctly identified ALL original content as human-written!\n",
      "\\n=== CORRECTED INTERPRETATION ===\\n\n",
      "The Original row should be interpreted as:\n",
      "- True Positive Rate: N/A (no generated content to detect)\n",
      "- **Human Content Accuracy: 100%** (all 352 original descriptions correctly identified as human)\n",
      "- **AI Probability range: 0.0% to 48.4%** (all below 50% threshold)\n",
      "\\n=== ORIGINAL CONTENT DETECTION STATISTICS ===\n",
      "Total original descriptions analyzed: 352\n",
      "AI probability statistics:\n",
      "  Mean: 13.5%\n",
      "  Median: 9.9%\n",
      "  Std Dev: 14.7%\n",
      "  Max: 48.4%\n",
      "  Min: 0.0%\n",
      "\\nDistribution of AI probabilities:\n",
      "  0-10%: 176 descriptions\n",
      "  10-20%: 66 descriptions\n",
      "  20-30%: 55 descriptions\n",
      "  30-40%: 44 descriptions\n",
      "  40-50%: 11 descriptions\n",
      "  Above 50%: 0 descriptions (would be misclassified)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\\\n=== EXPLANATION OF ORIGINAL DETECTION METRICS ===\\\\n\")\n",
    "\n",
    "print(\"Why Original shows 0.0 for both metrics:\")\n",
    "print(\"\\\\n1. **True Positive Rate (0.0%)**:\")\n",
    "print(\"   - This measures: Generated content correctly identified as AI\")\n",
    "print(\"   - For 'Original' row: We have 0 generated entries (only human-written content)\")\n",
    "print(\"   - So: 0 generated entries detected as AI / 0 total generated entries = 0/0 = 0%\")\n",
    "print(\"   - This is correct but not meaningful for the Original baseline\")\n",
    "\n",
    "print(\"\\\\n2. **False Negative Rate (0.0%)**:\")  \n",
    "print(\"   - This measures: Original (human) content incorrectly identified as AI\")\n",
    "print(\"   - For 'Original' row: 0 out of 352 human descriptions were misclassified as AI\")\n",
    "print(\"   - So: 0 misclassified / 352 total human descriptions = 0%\")\n",
    "print(\"   - This means the detector correctly identified ALL original content as human-written!\")\n",
    "\n",
    "print(\"\\\\n=== CORRECTED INTERPRETATION ===\\\\n\")\n",
    "print(\"The Original row should be interpreted as:\")\n",
    "print(\"- True Positive Rate: N/A (no generated content to detect)\")\n",
    "print(\"- **Human Content Accuracy: 100%** (all 352 original descriptions correctly identified as human)\")\n",
    "print(\"- **AI Probability range: 0.0% to 48.4%** (all below 50% threshold)\")\n",
    "\n",
    "# Let's also check a few more statistics for Original\n",
    "original_debug = merged_df[merged_df['entry_type'] == 'original'].copy()\n",
    "original_debug['ai_probability'] = original_debug['zerogpt_response'].apply(parse_zerogpt_response)\n",
    "\n",
    "print(f\"\\\\n=== ORIGINAL CONTENT DETECTION STATISTICS ===\")\n",
    "print(f\"Total original descriptions analyzed: {len(original_debug)}\")\n",
    "print(f\"AI probability statistics:\")\n",
    "print(f\"  Mean: {original_debug['ai_probability'].mean():.1f}%\")\n",
    "print(f\"  Median: {original_debug['ai_probability'].median():.1f}%\")\n",
    "print(f\"  Std Dev: {original_debug['ai_probability'].std():.1f}%\")\n",
    "print(f\"  Max: {original_debug['ai_probability'].max():.1f}%\")\n",
    "print(f\"  Min: {original_debug['ai_probability'].min():.1f}%\")\n",
    "print(f\"\\\\nDistribution of AI probabilities:\")\n",
    "print(f\"  0-10%: {((original_debug['ai_probability'] >= 0) & (original_debug['ai_probability'] < 10)).sum()} descriptions\")\n",
    "print(f\"  10-20%: {((original_debug['ai_probability'] >= 10) & (original_debug['ai_probability'] < 20)).sum()} descriptions\")\n",
    "print(f\"  20-30%: {((original_debug['ai_probability'] >= 20) & (original_debug['ai_probability'] < 30)).sum()} descriptions\")\n",
    "print(f\"  30-40%: {((original_debug['ai_probability'] >= 30) & (original_debug['ai_probability'] < 40)).sum()} descriptions\")\n",
    "print(f\"  40-50%: {((original_debug['ai_probability'] >= 40) & (original_debug['ai_probability'] < 50)).sum()} descriptions\")\n",
    "print(f\"  Above 50%: {(original_debug['ai_probability'] >= 50).sum()} descriptions (would be misclassified)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3578682a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VERIFICATION: ZeroGPT RESPONSES FOR ORIGINAL ENTRIES ===\n",
      "\n",
      "1. Checking detection CSV structure:\n",
      "   Total entries in detection file: 704\n",
      "   Entry types: entry_type\n",
      "original     352\n",
      "generated    352\n",
      "Name: count, dtype: int64\n",
      "\n",
      "2. Original entries in detection file:\n",
      "   Count: 352\n",
      "   Non-null zerogpt_response: 352\n",
      "   Sample ZeroGPT responses for original entries:\n",
      "     Sample 1: AI probability = 17.46% | Response: {\"success\": true, \"code\": 200, \"message\": \"detection result passed to proxy\", \"data\": {\"sentences\": ...\n",
      "     Sample 2: AI probability = 26.09% | Response: {\"success\": true, \"code\": 200, \"message\": \"detection result passed to proxy\", \"data\": {\"sentences\": ...\n",
      "     Sample 3: AI probability = 29.56% | Response: {\"success\": true, \"code\": 200, \"message\": \"detection result passed to proxy\", \"data\": {\"sentences\": ...\n",
      "\n",
      "3. After merging with generation data:\n",
      "   Count: 352\n",
      "   AI probabilities calculated:\n",
      "     Mean: 13.5%\n",
      "     Max: 48.4%\n",
      "     Above 50% threshold: 0\n",
      "\n",
      "4. IMPORTANT INSIGHT:\n",
      "   âœ… The false_negative_pct is CORRECTLY 0.0% because:\n",
      "   âœ… ALL 352 original descriptions scored below 50% AI probability\n",
      "   âœ… This means ZeroGPT correctly identified ALL original content as human-written\n",
      "   âœ… No original content was misclassified as AI-generated\n"
     ]
    }
   ],
   "source": [
    "# VERIFY: Let's confirm the ZeroGPT responses for original entries are being used\n",
    "print(\"=== VERIFICATION: ZeroGPT RESPONSES FOR ORIGINAL ENTRIES ===\\n\")\n",
    "\n",
    "# Check the detection CSV file directly\n",
    "print(\"1. Checking detection CSV structure:\")\n",
    "print(f\"   Total entries in detection file: {len(detection_df)}\")\n",
    "print(f\"   Entry types: {detection_df['entry_type'].value_counts()}\")\n",
    "\n",
    "# Check original entries specifically\n",
    "original_detection_entries = detection_df[detection_df['entry_type'] == 'original']\n",
    "print(f\"\\n2. Original entries in detection file:\")\n",
    "print(f\"   Count: {len(original_detection_entries)}\")\n",
    "print(f\"   Non-null zerogpt_response: {original_detection_entries['zerogpt_response'].notna().sum()}\")\n",
    "print(f\"   Sample ZeroGPT responses for original entries:\")\n",
    "\n",
    "# Parse a few original entry responses\n",
    "for i, (idx, row) in enumerate(original_detection_entries.head(3).iterrows()):\n",
    "    response_sample = row['zerogpt_response'][:100] if isinstance(row['zerogpt_response'], str) else str(row['zerogpt_response'])\n",
    "    ai_prob = parse_zerogpt_response(row['zerogpt_response'])\n",
    "    print(f\"     Sample {i+1}: AI probability = {ai_prob}% | Response: {response_sample}...\")\n",
    "\n",
    "# Now let's check what happens when we merge with generation data\n",
    "print(f\"\\n3. After merging with generation data:\")\n",
    "original_merged = merged_df[merged_df['entry_type'] == 'original']\n",
    "print(f\"   Count: {len(original_merged)}\")\n",
    "print(f\"   AI probabilities calculated:\")\n",
    "original_merged_copy = original_merged.copy()\n",
    "original_merged_copy['ai_probability'] = original_merged_copy['zerogpt_response'].apply(parse_zerogpt_response)\n",
    "print(f\"     Mean: {original_merged_copy['ai_probability'].mean():.1f}%\")\n",
    "print(f\"     Max: {original_merged_copy['ai_probability'].max():.1f}%\")\n",
    "print(f\"     Above 50% threshold: {(original_merged_copy['ai_probability'] > 50).sum()}\")\n",
    "\n",
    "print(f\"\\n4. IMPORTANT INSIGHT:\")\n",
    "if (original_merged_copy['ai_probability'] > 50).sum() == 0:\n",
    "    print(f\"   âœ… The false_negative_pct is CORRECTLY 0.0% because:\")\n",
    "    print(f\"   âœ… ALL {len(original_merged_copy)} original descriptions scored below 50% AI probability\")\n",
    "    print(f\"   âœ… This means ZeroGPT correctly identified ALL original content as human-written\")\n",
    "    print(f\"   âœ… No original content was misclassified as AI-generated\")\n",
    "else:\n",
    "    misclassified = (original_merged_copy['ai_probability'] > 50).sum()\n",
    "    total = len(original_merged_copy)\n",
    "    print(f\"   âŒ Something's wrong: {misclassified} out of {total} original entries scored > 50%\")\n",
    "    print(f\"   âŒ False negative rate should be: {(misclassified/total)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2ce8b2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FINAL CONFIRMATION OF DETECTION CALCULATION ===\n",
      "\n",
      "Manual calculation verification:\n",
      "1. Total original entries: 352\n",
      "2. AI probabilities successfully parsed: 352\n",
      "3. AI threshold used: 50.0%\n",
      "4. Entries above threshold (detected as AI): 0\n",
      "5. False Negative Rate calculation:\n",
      "   = Entries incorrectly identified as AI / Total original entries\n",
      "   = 0 / 352\n",
      "   = 0.0%\n",
      "\n",
      "6. Distribution of AI probabilities for original entries:\n",
      "   Min: 0.0%\n",
      "   25th percentile: 0.0%\n",
      "   Median: 9.9%\n",
      "   75th percentile: 25.7%\n",
      "   Max: 48.4%\n",
      "\n",
      "âœ… CONFIRMATION: The ZeroGPT responses for original entries ARE being used!\n",
      "âœ… The 0.0% false negative rate is CORRECT - it means perfect accuracy!\n",
      "âœ… ZeroGPT successfully identified all original content as human-written.\n",
      "2. AI probabilities successfully parsed: 352\n",
      "3. AI threshold used: 50.0%\n",
      "4. Entries above threshold (detected as AI): 0\n",
      "5. False Negative Rate calculation:\n",
      "   = Entries incorrectly identified as AI / Total original entries\n",
      "   = 0 / 352\n",
      "   = 0.0%\n",
      "\n",
      "6. Distribution of AI probabilities for original entries:\n",
      "   Min: 0.0%\n",
      "   25th percentile: 0.0%\n",
      "   Median: 9.9%\n",
      "   75th percentile: 25.7%\n",
      "   Max: 48.4%\n",
      "\n",
      "âœ… CONFIRMATION: The ZeroGPT responses for original entries ARE being used!\n",
      "âœ… The 0.0% false negative rate is CORRECT - it means perfect accuracy!\n",
      "âœ… ZeroGPT successfully identified all original content as human-written.\n"
     ]
    }
   ],
   "source": [
    "print(\"=== FINAL CONFIRMATION OF DETECTION CALCULATION ===\\n\")\n",
    "\n",
    "# Let's manually reproduce the exact calculation from our function\n",
    "original_for_verification = merged_df[merged_df['entry_type'] == 'original'].copy()\n",
    "original_for_verification['ai_probability'] = original_for_verification['zerogpt_response'].apply(parse_zerogpt_response)\n",
    "\n",
    "print(\"Manual calculation verification:\")\n",
    "print(f\"1. Total original entries: {len(original_for_verification)}\")\n",
    "print(f\"2. AI probabilities successfully parsed: {original_for_verification['ai_probability'].notna().sum()}\")\n",
    "print(f\"3. AI threshold used: 50.0%\")\n",
    "print(f\"4. Entries above threshold (detected as AI): {(original_for_verification['ai_probability'] > 50.0).sum()}\")\n",
    "print(f\"5. False Negative Rate calculation:\")\n",
    "print(f\"   = Entries incorrectly identified as AI / Total original entries\")\n",
    "print(f\"   = {(original_for_verification['ai_probability'] > 50.0).sum()} / {len(original_for_verification)}\")\n",
    "print(f\"   = {((original_for_verification['ai_probability'] > 50.0).sum() / len(original_for_verification)) * 100:.1f}%\")\n",
    "\n",
    "print(f\"\\n6. Distribution of AI probabilities for original entries:\")\n",
    "ai_probs = original_for_verification['ai_probability']\n",
    "print(f\"   Min: {ai_probs.min():.1f}%\")\n",
    "print(f\"   25th percentile: {ai_probs.quantile(0.25):.1f}%\") \n",
    "print(f\"   Median: {ai_probs.median():.1f}%\")\n",
    "print(f\"   75th percentile: {ai_probs.quantile(0.75):.1f}%\")\n",
    "print(f\"   Max: {ai_probs.max():.1f}%\")\n",
    "\n",
    "print(f\"\\nâœ… CONFIRMATION: The ZeroGPT responses for original entries ARE being used!\")\n",
    "print(f\"âœ… The 0.0% false negative rate is CORRECT - it means perfect accuracy!\")\n",
    "print(f\"âœ… ZeroGPT successfully identified all original content as human-written.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
